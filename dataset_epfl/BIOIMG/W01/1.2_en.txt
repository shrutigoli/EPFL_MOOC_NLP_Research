Let's talk about optimization of contrast-to-noise ratio and signal-to-noise ratio.
But first, the question is,
"How can we optimize the signal-to-noise ratio of our measurement?"
So one way that I want to talk about here is-- by repeated measurements.
So let's have you repeat the measurements, and we'll call them <i>S_i</i>, where <i>I</i> is the index of the <i>N</i> repeated measurements.
And now we obtain an average measurement-- that's an average signal, which I call here <i>&lt;S&gt;</i>.
That's a sum of all the measurements divided by <i>N</i>-- that is the deviated definition of average.
Now, the signal-to-noise ratio that we obtained here depends on the square root of law.
That law--I will briefly show where this comes from
So what this means is, if we do four repeated measurements of the same thing we are averaging together, the precision of the measurement is improved by twofold.
Let's just do a quick demonstration.
For some of you this must be the standard theory from statistical courses that you may have taken, for others it might be new.
So we'll take here just a demonstration of the square root law.
So I'll compose the <i>I</i>-th measurement, one of the <i>N</i> measurements, as the true signal S plus a source of noise <i>Îµ_i</i>.
So for the 3rd measurement, for example, <i>S_3</i>, we have the true variable S plus a contribution of noise which we call <i>Îµ_3</i>, now indicated by the index <i>I</i>
The expectation value, the average of <i>Îµ_i</i> square, is by definition <i>Ïƒ</i> squared, and the other thing that we introduce is that the noise has an expectation value, the amplitude of zero, so <i>Îµ_i</i> will average to zero.
So it's not contributing to systematic values.
So now <i>S</i> is the true signal, which for a moment is unknown.
We have the average signal which is sum of <i>S_i</i> divided by <i>N</i>.
This is equal to <i>S</i>, because it is <i>N</i> times <i>S</i> divided by <i>N</i>, plus sum of the <i>Îµ_i</i> divided by <i>N</i>.
This is our average signal.
Now, we'll define here the deviation of our measured signal, <i>&lt;S&gt;</i> here minus the true signal which we suppose exists.
That is then equal to the sum of the <i>Îµ_i</i> divided by the number of measurements.
Now we will calculate the square of <i>Î” S</i>.
<i>Î” S</i> square, so that's the sum of <i>Îµ_i</i>-s squared divided N squared, and now we will dissect this sum here into different terms.
We have the terms where <i>Îµ_i</i> is multiplied by <i>Îµ_i</i>.
So it gives us the sum of <i>Îµ_i </i>-s squared, and then we have a sum where <i>I</i> and <i>j</i> are not the same, so the error of the second measurement is multiplied by the error of the third measurement etc., over all sample--that's this term here.
This term here is essentially the expectation value of <i>Îµ_i</i> times <i>Îµ_j</i>.
And that goes to zero because we have here that on average the error has an average value of zero.
So if you multiply the error in the 3rd measurement and sum it with multiplication of the errors of all the other measurements-- this is <i>Îµ_3</i> times <i>Îµ_j</i>, where <i>I</i> is not equal to <i>j</i>.
Then on average <i>Îµ_j</i> goes to zero, so this term here goes to zero.
And now we are coming to <i>Î” S</i> square, this equals to the sum of <i>Îµ_i</i> square, which is nothing but <i>N</i> time (<i>Ïƒ</i> square), this we had here, this term here, sum of <i>Îµ_i</i> square is <i>N (Ïƒ squared)</i> and so we now have the error, the deviation,
<i>(Î” S)</i> square is equal to
<i>Ïƒ</i> square divided by <i>N</i>.
Or, in other words, the average amplitude of the noise, that is, deviation of our measurement relative to the ground truth, the <i>S</i>, the average value is given by <i>Ïƒ</i> divided by square root <i>N</i>, where <i>N</i> is the number of repeated measurements that we have done.
So this gives us the square root law.
So this means--this is known very well from statistics--
I've really arrived here at fairly standard formula.
What this means is that if we want improve the signal-to-noise ratio by repeated measurements we have to increase the measurement time.
This increases the number of measurements, and, to give you an idea, if you want to reduce the dimensions for voxels by the factor of 2--
2 in everyone of the dimensions-- so it's your voxel size that reduces the size by <i>8</i>, and you want to increase, to compensate for this reduction in signal with the smaller voxel that you have by increasing the signal-to-noise by a factor of <i>8</i>, the square root of law tells you that you have to increase the measurement time by factor of <i>64</i>.
So we go from a 2 mm resolution image to a 1 mm resolution image,
3D means we go from a 1 min measurement, for example, to a 64 min measurement, or one hour.
Can you imagine that this is very uncomfortable?
So now, second question was,
"How can we optimize the contrast-to-noise ratio?"
And here for the purpose of this course
I will introduce a concept that is very important, and we will make repeated use of this approach that I will be demonstrating here.
So, how do we optimize contrast?
So we want optimize the ability to distinguish between two tissue types.
So we want to optimize here the experimental parameters of our imaging modality to maximize difference between, say, signals <i>S_1</i> and <i>S_2</i>.
This is typically a complex and empirical procedure, not very easy.
Sometimes we can actually deduce some useful information, if the signal behavior can be modeled.
Unfortunately we have many situations where we can't put a reasonable model of signal behavior to estimate the optimal contrast-to-noise ratio.
And here what we will use is error propagation calculation.
We will look at a procedure now, and this is very important procedure.
We will look at this procedure to optimize the contrast-to-noise ratio.
The approach here consists of the following.
Let's assume we have the signal, or the measurement variable <i>S</i>.
And we will assume here it's a function of two variables, <i>k</i> and <i>t</i>.
It does not matter what they are.
We can imagine certain things.
Let's say <i>k</i> is a tissue property.
That's what we impose:
<i>k</i> is a tissue property--that's important, and that's a decay rate-- it's something that is decaying.
And let's say <i>t</i> is an experimental parameter.
That is something that I as an operate, I can change
I can manipulate that parameter.
And here, for example, it could be time-- when we measure, for example.
So we have decay rate and measurement time.
Now, the approach of finding the best contrast-to-noise ratio consists first in calculating by how much does the signal change if the tissue property changes.
That is, we do nothing else than take the derivative of a signal with respect to the tissue property <i>k</i>.
That gives us a measure by how much the signal changes if we change the tissue property <i>k</i>.
We cannot of course change the tissue property <i>k</i>, but in the end, we are asking is if we have a range of different tissue properties, and where are the imaging techniques most sensitive to chain differences in tissue property, here, what we call <i>k</i>.
So we'll take this function that we defined here, and we calculate <i>dS / dk</i>.
That's a sensitivity of our measurement to changes in tissue properties.
And then we want to find out where is it maximal, because if you know that the signal change is maximal with respect to tissue property <i>k</i> then we know we have the best possible contrast-to-noise ratio.
And to to find out maximum, we take standard calculus here.
For the maximum we'll take the derivative of <i>dS / dk</i>, this function with respect to <i>t</i>, the experimental parameter.
So first I am asking how does the signal change maximum with respect to tissue parameter.
Then I have to find with respect to the experimental parameter, here <i>t</i>, where is that change maximal, and we just have to do by taking the derivative.
Then we have just to verify that we indeed have found the maximum.
So let's take an example here.
It's actually a fairly ubiquitous function, and we'll say our signal as function of tissue parameter <i>k</i> and experimental parameter <i>t</i> is given by S_0, a constant, times <i>e</i> to the <i>- k t</i>.
That's a very simple model.
So where is this signal maximally dependent, the changes in <i>k</i>, where do they produce the biggest changes in signal?
Look at this function and think about it.
If we take...
Let's just go ahead here.
I will take the derivative of a signal with respect to <i>k</i>.
So the constant is zero.
We have the internal derivative <i>t</i> and <i>(e to the - k t)</i>.
So, remember, we derive with respect to <i>k</i>.
If we look at this function, this function tells us at what point is this function most sensitive to what extent is it sensitive to changes in a tissue parameter <i>k</i>.
If we look at this parameter <i>t</i> where I want to find the maximum <i>t</i>, where this is maximum
I can already deduce some behavior.
If I let go <i>t</i> to infinity-- so I measure at very long times-- then <i>(e to the - k t)</i> goes to zero much faster than <i>t</i> goes to infinity.
So <i>t (e to the - k t)</i> goes to zero for <i>t</i> going to infinity.
So we have at that point: our signal is completely not sensitive to different <i>k</i>-s.
This is, in other words, saying here, at <i>k t</i> vs. infinity this is a zero, no matter what the <i>k</i> is.
On the other hand, if I choose <i>t = 0</i>, this function here is <i>0</i>.
<i>0</i> times <i>e</i> to <i>- 0</i> is <i>0</i>.
The function actually, it turns out, at <i>t = 0</i> equals to <i>S_0</i>.
So it's <i>S_0</i>, no matter what the <i>k</i> is, so I have no ability to distinguish the tissue parameter <i>k</i> at time <i>0</i>.
So neither at time <i>0</i> nor at time infinity do I have an ability to discriminate two tissues based on the parameter <i>k</i>.
So now we want to find the maximum, so we will take the derivative of this function here with respect to <i>t</i> and set in to <i>0</i>.
That's where we find an extremum, we'll do the derivative-- so that's standard derivation rules-- and then we can separate out
<i>S_0 e</i> to the (- k t), and we can see here we separate this out, we have this term here.
This is never <i>0</i>.
We want to set this equal to <i>0</i> and to have this <i>0</i>, we have to have the measurement at <i>t_0 = 1 / k</i>.
So our ability to discriminate the tissue parameter <i>k</i> based on the choice of <i>t</i> is maximal if we choose <i>t</i>-- the measurement variable here so that is equal to <i>1</i> over the decay rate of our signal in this particular model.
This is fairly ubiquitous, like I said, and so the conclusion is: for an exponentially decaying signal your optimal time of measurement is equal to <i>1</i> over the decay rate.
That is the best result in this case.
Now, let's say, we have two tissues, but two different <i>k</i>-s.
So we are getting two different optimal <i>t_0</i>-s, two different optimal measurements.
So we no longer can have the best contrast-to-noise ratio for both tissues, and we have to find a compromise.
How do we find a compromise?
We can calculate it, or we can also ask ourselves, are we willing to sacrifice a certain percentage of the optimal contrast-to-noise ratio, that is, the optimal sensitivity, and deviate a little bit?
So now we are going to look at this function here which is plotted here.
So this is just normalized, so it has the maximum at value <i>1</i>, and on the x-axis we plot the variable <i>k t</i>.
So we are plotting this function normalized to <i>1</i> at the maximum
<i>k t = 1</i>, <i>t = 1 / k</i>.
It's the function that we obtain here.
And luckily, if we are willing to sacrifice a certain percentage of a contrast-to-noise ratio, let's say, we are willing to sacrifice 20% here.
20% means we have repeated measurements.
We have to go by the square root law.
This means we have to scan 40% more-- we go from 1 minute to 1.5 - 2 minutes, something like that.
So this is not a huge penalty to pay.
And now look at this function here.
If we are saying we are going to stay within 80% of the maximum, then we go from essentially <i>k t = 0.5</i> to almost up to <i>k t = 2</i>.
Roughly estimating here, we are gaining actually a fourfold range in changes in the parameter <i>k</i>.
This parameter can change and be still very close to the optimal measurement conditions.
We should not be down here, we should not be up here but if we are somewhere in this range the penalty to be away from the optimal contrast-to-noise ratio is not enormous, and we can certainly live with that.
So the choice of the exact measurement variable, here <i>t</i>, is not critical.
It does not have to match exactly <i>1 / k</i>, and if we have two different tissues, it is not so critical if we are close to be optimum.
This practically means, if we have two different tissues with with two different <i>k</i>-s, then your <i>t_0</i> is somewhere between <i>1 / k_1</i> and <i>1 / k_2</i>, somewhere between those two values.
One can of course calculate that.
So here is the optimum in this [chart].
