Let's think about what kind of algorithm is adapted to 1971's computing power.
We first need to imagine what is 1970's computing power.
Take this. This is what we all, pretty much all of us, in one form or another have a smartphone or we have seen smartphones.
The smartphones are so ubiquitous that we no longer think on what they are capable of.
Imagine if you have a camera here.
The camera has face recognition.
It tells you when you take a picture where's the face of the person.
It can recognize faces.
I sometimes play with chess.
25 years ago, I used to play, beat almost any of the computers that play chess that were with reasonable pricing available.
This thing, I have to put it in a very low level, then I can still beat it.
It's very powerful.
By 1970 standard, this is a super computer.
We've had to deal with the situation in the 1970s that the computing power was much more limited; memory was limited.
I remember my first computer had a hard disk of 8 megabytes.
Think about that.
Now it's... today we would have it in terabytes.
So this is a super computer by 1970 standard.
We're looking for an algorithm that a computer at the time could reasonably handle.
The question here to you is, what mathematical operations or what are the operations that the computer does fast?
Think about this for a moment: which operation is the fastest in a computer?
Did you guess it?
Do you have some ideas?
Well, let's discuss this.
The simplest, fastest operation that you can do in a computer is writing to memory.
The second fastest operation is addition.
So if we can come up with an algorithm that writes to memory and adds numbers, then we're using the fastest operations that our computer can provide in 1970.
And this is actually the underlying principle of the back reconstruction principle.
What we are doing with backprojection, and we'll just take you an example.
Let's say we've collected a projection in this direction, I'd add one pixel and we've had recorded in a computer a beam intensity of 1, or a detection of 1.
Now we write into memory.
We'll write, along this direction into the memory, we'll write "1s."
Now we will record a next projection orientation and let's say it's here.
We'll again, in this direction that we'll take the intensity we've recorded here, we'll write 1s in this direction, and where we have non-zeros, we'll add.
So we'll start up with zeros, we'll write the 1s in here, and now we'll add the next intensity on here, this is one.
So here where they intersect, we'll have 2.
You can do this example of course also with 1 and 3, then you would have 4 here, and 3 is here.
It's the same principle.
So we sum the values in overlapping pixels and the rest we write in there, and that is for two orientations here.
We do this for multiple orientations we can test, take any orientation in <i>x'</i>, <i>y'</i> and this is just showing here the function--the projection here.
What we're looking at here is just a rotation of the coordinate system that we have with the angle fader,
That's what we had in the sinogram, so we're rotating the angles here.
We'll just rotate, we'll do this.
We'll rotate. We'll do that.
Now, instead of going through the mathematical elaborative way of working out the maths here,
I want to illustrate what's happening with gray shades for a point-like object.
Let's look at two projections.
We have two projections.
So we have taken the computer, we filled in the numbers, give you this projection; we filled in numbers with this projection, the rest is zero.
Here where we overlap, we have added, so this gets a bit darker.
Let's take four projections.
When we have four projections, one... two... three... four...
Again, we take the same intensity because it's a point-wise object and we're starting to add the numbers together here where they overlap so it gets progressively darker, the grayscale.
Now we'll look at eight projections.
This is what we're getting with eight projections.
The overlap here, it's getting darker and darker.
Here, we still have the same intensity as here.
And now with 16 projections, we're starting to see that we have an object here.
This is where the object, the original object was.
We can start to see that there was some intensity there.
This is with 16 projections.
If we look at the reconstruction in real time, this is an example here from a standard phantom.
The first 60 projections, the next 60 projections and so forth, and here, this is 180 projections here, the backreconstruction is complete.
We can see here this so-called 
<i>Shepp-Logan phantom</i> quite nicely.
One thing that we have a problem here is we said we have point-like object.
So if we have a point-like object, if we sum it together, we should in here have a point-like object reconstruction.
As we can see here, it gets very blurry with the number of projections.
This is a problem I want to address in the next few minutes.
Why does simple Backprojection, the algorithm that we've just discussed have poor spatial resolution?
We take with many backprojections, a point-wise object we are getting an intensity that looks like this.
And we can actually show that the reconstruction at this point-object falls of with <i>1 / r</i>.
So instead of a point-object, we're getting an object whose intensity falls of with <i>1 / r</i>.
And the question is "why?"
Why is this so?
We'll take the situation here and we'll look at this.
I'm bringing here a intuitive argument.
There's a mathematical proof of this principle.
We can show this mathematically.
We have this in the exercises.
Here, we're going to look at the analogy of the light intensity.
So we are, when we look at a candle, from very close, the candle is bright.
Overlooking it from a distance, the candle is weak, we can barely see the candle.
And this analogy I want to use here.
We're going to look at the coordinates-- cylindrical coordinates here.
We have <i>Ï�</i>, we have <i>dÏ�</i> here.
We have a <i>dÏ†</i>, an opening of the angle.
And so this dimension here is <i>Ï� dÏ†</i>.
This is giving us the area of our voxel here at this point.
The area of the voxel is <i>dx dy</i>; that's the infinite dismal area, <i>dA</i>.
That's <i>Ï� sin(dÏ†) dÏ�</i> or this is <i>Ï� dÏ† dÏ�</i> that's this times this.
That's our voxel science here.
Now we can take this, so we're taking this approach here and we'll see a reconstruction.
We have lights, we produce a projection.
We have the incident x-rays.
This gives us the projection here.
For the reconstruction, we do the inverse.
We do shine light here again.
We have the second projection here, so we have collected these two projections.
So, to recap: x-rays come in, we collect the projection with this orientation, x-rays converse direction, we do a projection-- collect the projection here with the intensity.
And now for the reconstruction what we are doing is the opposite.
We're writing the intensity that we saw here into the image.
So we are backprojecting the light, if you will.
We're doing the inverse operation in this direction here.
This is the other direction, and we're adding this together.
So for the image reconstruction or the measurement, x-rays come in, this gives us a projection, x-rays come in, this gives us a projection.
And then in a computer, we do the reverse.
We emit the x-rays in the computer, so to speak, in the direction of this orientation, we fill it in and we do it here for this angle here.
We're sort of reverting the process.
It's like we are shining back to our original object, the intensity with which we have seen the object.
So what we are having now is we have a certain angle <i>dÏ†</i>, we have the area here which is proportional to <i>Ï�</i>, to the distance that we are from the point-wise object.
Now we've said here in this process that reconstructing with backprojection is like we are inverting the process of emitting light, so we have instead of a candle, shining out light in distance, we're shining the light back onto our object.
So we're shining this light, the x-rays-- that's also light-- all according to the angle back in here.
If we have a certain number of photons counted here, a certain intensity, for a given <i>dÏ†</i>, this intensity is the same.
But the area is proportional to <i>Ï�</i>.
In the end we reconstruct the image with constant pixel size.
Our pixel size, the number of back projected intensities for a given pixel size is inversely constant with <i>dÏ†</i> but you want to keep the pixel size constant in <i>dA</i>, and since <i>dA</i> is proportional to <i>Ï�</i>, then the number of rays that go through it decrease with <i>1 / Ï�</i>.
And that's the reason behind the fact that a point-wise object is not reconstructed into a point-wise object with simple backprojection upon an object whose intensity falls off with <i>1 / r</i>.
Now the question comes, how can we maintain good image resolution?
With simple back projection, to recap, with simple back projection, our point object becomes something, that falls off relatively slowly.
The trick here is to use filtered backprojection, and the filtering that is done is a mathematical operation.
We'll take our object.
Here it's supposed to be a point-wise object, the projection that we've measured here.
We'll convolute it with a function that has negative wings on the side and we're getting this projection.
Then this is being backprojected to form the final image.
So what does this do?
We have said a point object produces in the reconstruction, produces wings that fall off with <i>1 / r</i>.
We want the point object to be represented by a point object.
What we're doing is we're convoluting the measured projection with the function that has negative wings on the sides, that's this convolution operation here, we're getting this kind of projection.
Now, let's think about here what are the consequences for the image.
Are there any consequences for sensitivity?
How about resolution?
Well, resolution, if with this we are getting back our point-wise object, the resolution improves.
Beginning from point object to a point object, so that's good.
What about sensitivity?
Remember backprojection is a process where we add numbers.
Now our projection here has negative tails on the sides.
This reduces the number of additions that we do in a computer, so it reduces the sensitivity.
We're back to the old game in my medical imaging.
On one side, we have sensitivity, on the other side, we have resolution, and the two things are fighting against each other.
So we are going to lose some sensitivity.
Here's an original sinogram of an object.
We just have a point-wise object.
Here's the sinogram.
Then we do the backprojections.
In white are the negative intensities.
This is one, two, eight, 32 and 64.
And here, we're getting, very nicely, from the backprojections, we're getting a nice definition of our original object.
Here's another filtered backprojection example.
For larger object, we have two projections, four projections, eight projections, and so on.
With 64 projects, we get a nice square phantom.
If we don't do filtered backprojection, this is this example here.
This is what we would be getting.
So we are getting a blurry image.
This is definitely not something that we would want.
To give you an example of a real-life image, this is a backprojected image of the abdomen, the thorax.
We can start to see some organs in there but this is really very blurry.
Now, think what you're going to see next.
It's the same image but now reconstructed with filtered backprojection.
And here it is. Same image.
We get a much clearer depiction of the internal structure, a much better resolution, even though we sacrificed sensitivity.
This image here has a lot of intensity.
We've sacrificed it to gain resolution and the anatomical detail.
In this video as it loops through, you're seeing the reconstruction of the image through the process of backprojection.
At the beginning we can hardly see what the object is, and as it reconstructs, we're seeing that it reconstructs into four different vials of different absorption that are depicted.
So we can see the process here as it loops through of the reconstruction, or filtered backprojection reconstruction.
I'll show it here in individual slices.
This is now an in vivo example from the heart as we start out with one projection, multiple projections, increase the number of projections, we can still see not much.
Now we're starting to guess there's something in the center but still we cannot detect much of a structure.
And as we come to the end, we are seeing here the reconstruction.
This is actually a specific scan of the heart.
Now we'll take these eight images and here in this looping animation we're seeing the same eight images in the process of reconstruction, and so we can visualize how this process in the computer as we are continuously backprojecting how this summation of intensities produces a picture here of an anatomical organ.
Here we have the looping presentation of the vials filled with different substances.
X-ray techniques. What we've seen is once we see a Radon transform we can reconstruct the image using backprojection
And this is essentially a technique that's being used for all the x-ray techniques.
It's the fundamental technique.
Yet, we are also going to talk about MRI, and their major technique that's being used is Fourier transformation, and there is indeed a link between the two, and this is what I want to discuss here briefly.
We have the reconstruction of an image.
We have the image space and the final image in CT.
This is the acquired data.
So this is the image space.
Then in terms of Fourier language we have the Fourier space, or k-space, or inverse space.
There are other terminologies.
In MRI, it uses k-space.
This is acquired data in MRI.
This is the Fourier transform of the object, the Fourier Space that has been acquired.
In CT, we're directly acquiring the image.
Now, let's take the <i>linear attenuation coefficient</i>.
We'll do a Fourier transformation.
And if we do a two-dimensional
Fourier transformation...
So we're doing the Fourier tranformation of an object that's represented in real space, that's what we know.
We get into the Fourier space, or k-space, and there the object is represented by the coordinates, <i>k_x</i> and <i>k_y</i> instead of <i>x</i> and <i>y</i>.
Here's the Fourier transformation.
We have just disregarded the normalization constants at the beginning to illustrate the principle here.
We can take the linear attenuation coefficient, fully transform it, and we get a representation of the object.
This at EPFL is dealt with, for the students with the course
<i>Signals and Systems</i>.
Now, if we do the Inverse 
Fourier Transformation of this data, then we go obviously, come back to the linear attenuation coefficient of the object.
Let's say we have our object in Fourier space, k-space, and now we set the k-space coordinate along x to zero.
Then we have this expression here, so <i>M 0</i> along <i>k_y</i> is this term here.
Can you recognize what this is?
If you look at this here very clearly, this is a projection.
This is the Radon transform of <i>Âµ</i>, so this is the projection of <i>Âµ</i> onto <i>y</i>.
This, what we're looking at here is the Fourier transformation.
If we do the Fourier transformation of the Randon transform, we're getting one line in k-space.
This is the <i>Central Slice Theorem</i>.
Central because <i>k_x</i> is <i>0</i>.
In principle, we can relate the data that we have obtained with Radon tranform to something that we have acquired in k-space, or Fourier space. and we could, in theory, do Fourier transformation.
And this in theory can be done.
This however, in practice, leads to artifacts.
This is the Shepp Logan Phantom in here with [the ringing ] artifacts, and the question is to reconstruct the specter using Fourier transformation, but backprojection is still a dominant method of imagery reconstruction for x-ray based techniques that are also iterative algorithms that are being used.
But the point here is we will be talking about reconstruction of images using MRI and that there is a link between CT and MRI reconstruction.
