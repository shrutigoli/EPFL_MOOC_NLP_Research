1
00:00:03,387 --> 00:00:09,989
On arrive à la fin de notre développement
de la théorie pour pouvoir démontrer

2
00:00:09,989 --> 00:00:14,647
cette décomposition d'une matrice 
quelconque en une matrice

3
00:00:14,647 --> 00:00:18,048
qui est un produit de 3 matrices : 
une matrice orthogonale,

4
00:00:18,048 --> 00:00:25,861
une matrice presque diagonale
et puis une matrice orthogonale,

5
00:00:25,861 --> 00:00:29,839
donc inversible, presque 
diagonale inversible.

6
00:00:29,839 --> 00:00:33,467
Donc voilà le théorème de décomposition
en valeurs singulières.

7
00:00:33,467 --> 00:00:37,898
Donc je me donne une matrice
<i>m</i> fois <i>n</i> de rang <i>R</i>.

8
00:00:37,898 --> 00:00:44,507
Alors il existe une matrice sigma tel que:
sigma est de cette forme-là,

9
00:00:44,507 --> 00:00:49,428
où <i>D</i> est une matrice diagonale 
et à part ça, on a des valeurs zéro.

10
00:00:49,428 --> 00:00:53,809
<i>D</i> est en fait la matrice diagonale:
<i>sigma 1</i> jusqu'à <i>sigma r</i>,

11
00:00:53,809 --> 00:00:57,629
où ce sont les <i>r</i> valeurs singulières
non nuls de la matrice <i>A</i>.

12
00:00:57,629 --> 00:01:04,212
Et puis il existe une matrice <i>U m x m</i>
et une matrice <i>V n x n</i>,

13
00:01:04,212 --> 00:01:08,877
orthogonale tel que A est le produit de
<i>U sigma V</i>.

14
00:01:08,877 --> 00:01:13,957
Donc ça, c'est la décomposition en
valeurs singulières. En anglais,

15
00:01:13,957 --> 00:01:18,451
<i>singular value decomposition</i>. Donc c'est 
presque le mieux qu'on puisse espérer.

16
00:01:18,451 --> 00:01:21,938
<i>A</i> n'est même pas forcément une 
matrice carrée et même si elle est carrée,

17
00:01:21,938 --> 00:01:25,491
on sait qu'on ne peut pas forcément 
la diagonaliser mais on peut toujours

18
00:01:25,491 --> 00:01:28,879
faire cette décomposition-là.
C'est ça qui à beaucoup d'applications.

19
00:01:28,879 --> 00:01:33,202
Maintenant, j'aimerais faire la preuve 
de ce théorème, et je commence ici.

20
00:01:33,202 --> 00:01:45,913
Donc soit <i>Phi</i>, l'application de <i>Rn en Rm</i>,
l'application linéaire dont la matrice

21
00:01:45,913 --> 00:01:53,531
est la matrice <i>A</i> tel que la matrice de <i>Phi</i>
par rapport aux bases canoniques <i>c1 c2</i>,

22
00:01:53,531 --> 00:02:08,233
est égal à la matrice <i>A</i> où <i>c1 c2</i> sont les
bases canoniques de <i>Rn Rm</i> respectivement.

23
00:02:08,233 --> 00:02:12,931
Alors par le théorème de l'existence
des bases compatibles,

24
00:02:12,931 --> 00:02:19,281
je sais qu'il existe une base orthonormée

25
00:02:25,731 --> 00:02:38,278
<i>B1</i> est égal à <i>v1</i> jusqu'à <i>v n de Rn</i>
tel que <i>phi de v1</i> jusqu'à <i>phi de v r</i>,

26
00:02:38,451 --> 00:02:43,118
soit une base orthogonale 
de l'image de <i>phi</i>.

27
00:02:43,289 --> 00:02:48,424
C'est ce que dit le théorème de 
l'existence des bases compatibles.

28
00:02:48,621 --> 00:02:51,354
Maintenant je reprends ça 
dans le prochain slide.

29
00:02:51,588 --> 00:02:55,206
Ça, c'est une base orthogonale, 
ce n'est pas une base orthonormée.

30
00:02:55,364 --> 00:03:02,616
Donc je vais faire ça.
On normalise cette base.

31
00:03:02,806 --> 00:03:13,384
Donc posons <i>u i = phi de v i</i>
divisé par sa norme qui est <i>sigma i</i>.

32
00:03:13,583 --> 00:03:17,623
On a déjà utilisé ça plusieurs fois.
C'est un vecteur de norme 1.

33
00:03:20,996 --> 00:03:32,596
Donc maintenant <i>u 1</i> jusqu'à <i>u r</i> est 
une base orthonormée de l'image.

34
00:03:32,776 --> 00:03:37,772
Ensuite, je vais étendre ça à 
une base orhtonormée de <i>Rm</i>.

35
00:03:37,953 --> 00:03:50,741
On complète cette base pour former 
une base orthonormée de <i>Rm</i>.

36
00:03:51,023 --> 00:04:03,038
J'appelle ça <i>B2</i>, qui est <i>u1</i> jusqu'à <i>u r</i>, 
<i>u r+1</i> jusqu'à <i>u m</i>.

37
00:04:03,218 --> 00:04:08,660
Maintenant, on a la base <i>B1</i>,
une base orthonormée de <i>Rn</i>.

38
00:04:08,818 --> 00:04:13,940
On a <i>u1</i> jusqu'à <i>u r</i>,
une base orthonormée de l'image

39
00:04:14,110 --> 00:04:17,445
et <i>u1</i> jusqu'à <i>u m</i>,
une base orthonormée de <i>Rm</i>.

40
00:04:17,657 --> 00:04:24,864
Comme j'ai cette égalité-là, 
j'ai <i>phi de v i</i> est égal à <i>sigma i u i</i>.

41
00:04:25,046 --> 00:04:30,007
Et ça, c'est pour tout
<i>i</i> plus petit ou égal à <i>r</i>.

42
00:04:30,195 --> 00:04:36,915
Et les <i>phi de v j</i> égaux à 0 
pour les <i>j</i> plus grands ou égaux à <i>r+1</i>,

43
00:04:37,083 --> 00:04:39,918
parce que ça correspondait 
à la valeur singulière 0.

44
00:04:40,094 --> 00:04:45,801
Donc maintenant, si j'écris <i>phi</i>
par rapport à ces deux bases-là, <i>B1 B2</i>,

45
00:04:46,028 --> 00:04:51,841
ça a une très jolie forme,
donc j'aurais <i>sigma 1</i> parce que

46
00:04:52,037 --> 00:04:56,035
le 1er vecteur est envoyé à <i>sigma 1</i>
fois le premier vecteur à droite, donc 0.

47
00:04:56,035 --> 00:05:00,824
Ensuite le deuxième vecteur est envoyé
à sigma 2 fois le deuxième vecteur,

48
00:05:01,011 --> 00:05:06,430
donc une colonne comme ça
et ainsi de suite jusqu'à <i>sigma r</i>,

49
00:05:06,610 --> 00:05:12,357
toujours le long de la diagonale
et après le reste de la matrice est 0.

50
00:05:12,533 --> 00:05:16,723
Voilà, c'est une matrice qui a justement
ici, dans ce coin-là,

51
00:05:16,885 --> 00:05:20,512
une matrice diagonale avec des valeurs 
singulières le long de la diagonale.

52
00:05:20,695 --> 00:05:27,347
La matrice <i>A</i> originale était la matrice 
de phi par rapport aux bases canoniques.

53
00:05:27,553 --> 00:05:33,453
Pour faire le lien entre cette matrice-là
et celle-là, je dois juste faire

54
00:05:33,644 --> 00:05:39,520
des changements de base.
Donc on a que A, qui est cette matrice,

55
00:05:39,714 --> 00:05:45,783
est égale à... Ici, je mets au milieu
la matrice <i>phi</i> par rapport à <i>B1</i> et <i>B2</i>

56
00:05:45,783 --> 00:05:50,543
Comme ici, je veux commencer avec 
quelque chose qui prend la base <i>C1</i>,

57
00:05:50,778 --> 00:05:56,711
je prends la base <i>C1</i> écrit en termes 
de <i>B1</i>, ensuite cette application

58
00:05:56,935 --> 00:06:01,611
me redonne un vecteur en termes de <i>B2</i>, 
donc je le prends en termes de <i>B2</i> et

59
00:06:01,837 --> 00:06:09,708
et je la transforme en termes de <i>C2</i>. 
Maintenant posons <i>u</i> égal à la matrice

60
00:06:09,708 --> 00:06:16,749
ici à gauche et <i>v</i> égal à la matrice ici. 
Ces deux matrices sont

61
00:06:16,916 --> 00:06:20,219
des matrices orthogonales parce que 
ce sont des matrices de passage

62
00:06:20,381 --> 00:06:28,291
entre bases orthonormées. 
<i>U</i> et <i>V</i> sont orthogonales car

63
00:06:28,418 --> 00:06:34,866
leurs colonnes forment 
des bases orthonormées.

64
00:06:35,130 --> 00:06:42,951
Ici, c'était dans <i>Rm</i>. et ici dans <i>Rn</i>, 
respectivement.

65
00:06:44,679 --> 00:06:58,386
Et posons, Sigma, la matrice de
cette forme-là : <i>sigma 1</i>, <i>sigma 2</i>, <i>sigma r</i>

66
00:06:58,572 --> 00:07:03,602
et puis on a que notre matrice <i>A</i>
est égale à <i>U sigma V</i>.

67
00:07:03,602 --> 00:07:07,041
Voilà la décomposition de 
cette matrice en un produit de

68
00:07:07,041 --> 00:07:10,041
orthogonale, donc inversible,

69
00:07:10,041 --> 00:07:14,904
une matrice presque diagonale
et orthogonale donc inversible.

70
00:07:15,160 --> 00:07:20,787
C'est la preuve de cette décomposition.
Maintenant dans le dernier paragraphe,

71
00:07:20,975 --> 00:07:23,612
nous allons appliquer ça à un exemple.
