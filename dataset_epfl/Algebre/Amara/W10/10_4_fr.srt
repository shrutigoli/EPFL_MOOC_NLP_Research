1
00:00:03,807 --> 00:00:08,111
Dans la vidéo précédente,
j'ai montré que si on a une matrice

2
00:00:08,111 --> 00:00:10,680
qui est orthogonalement diagonalisable,

3
00:00:10,680 --> 00:00:13,767
alors, cette matrice est
forcément symétrique.

4
00:00:13,767 --> 00:00:16,880
Et je vous ai dit
que la réciproque est aussi vraie.

5
00:00:16,880 --> 00:00:19,798
Maintenant, ça c'est
le premier théorème de ce cours

6
00:00:19,798 --> 00:00:23,406
que nous ne pouvons pas montrer,
ça s'appelle le théorème spectral

7
00:00:23,406 --> 00:00:25,385
donc c'est un des théorèmes spectraux.

8
00:00:25,385 --> 00:00:28,595
Mais je peux quand même faire
une première étape de la démonstration

9
00:00:28,595 --> 00:00:32,584
pour vous montrer pourquoi
c'est au moins un énoncé raisonnable.

10
00:00:32,584 --> 00:00:34,635
Je vais donc commencer
par vous montrer ça.

11
00:00:34,635 --> 00:00:38,919
Donc voilà, je me donne une matrice
symétrique, donc égale à sa transposée.

12
00:00:38,919 --> 00:00:43,602
Si cette matrice a deux vecteurs propres
de valeurs propres distinctes,

13
00:00:43,602 --> 00:00:47,057
alors, ces vecteurs sont
forcément orthogonaux.

14
00:00:47,057 --> 00:00:50,013
Ici, par rapport au produit scalaire
usuel de ℝn.

15
00:00:50,013 --> 00:00:53,975
On a l'espace vectoriel ℝn,
on a une matrice ;

16
00:00:53,975 --> 00:00:57,113
Elle a deux vecteurs propres
de valeurs propres distinctes,

17
00:00:57,113 --> 00:01:01,177
c'est ça qui est important,
alors ces deux vecteurs sont orthogonaux.

18
00:01:01,177 --> 00:01:03,320
Donc je donne la preuve.

19
00:01:05,480 --> 00:01:09,005
Alors, je prends deux vecteurs dans ℝn

20
00:01:11,035 --> 00:01:12,985
qui sont justement des vecteurs propres

21
00:01:12,985 --> 00:01:15,554
de valeurs propres distinctes
pour la matrice A.

22
00:01:24,054 --> 00:01:30,230
Disons deux valeurs propres
λ et μ, respectivement,

23
00:01:33,160 --> 00:01:36,160
avec λ différent de μ

24
00:01:36,160 --> 00:01:40,420
Donc ça veut dire que si je fixe,
disons la base canonique

25
00:01:40,420 --> 00:01:45,378
Donc fixons C la base canonique de ℝn

26
00:01:48,942 --> 00:01:53,872
et posons X égale à la matrice de u

27
00:01:54,338 --> 00:01:58,145
où c'est le vecteur colonne qui représente
u par rapport à cette base

28
00:01:58,885 --> 00:02:04,814
et Y la colonne qui représente
v par rapport à cette base

29
00:02:04,814 --> 00:02:11,295
alors on a que AX est égale à λX

30
00:02:13,163 --> 00:02:17,203
et AY est égale à μY

31
00:02:20,285 --> 00:02:24,513
Bon maintenant, j'ai envie
de montrer que X est orthogonale à Y

32
00:02:24,513 --> 00:02:31,731
Donc je fais
le produit scalaire de AX avec Y.

33
00:02:31,731 --> 00:02:39,559
Maintenant, ceci est égal à λ fois X
produit scalaire avec Y

34
00:02:39,559 --> 00:02:42,559
Et comme c'est un produit scalaire,
je peux sortir le λ

35
00:02:42,559 --> 00:02:46,611
et c'est X produit scalaire avec Y.
Donc, ça c'est un côté.

36
00:02:46,611 --> 00:02:50,143
Mais de l'autre côté je sais
que je peux faire ce calcul-là,

37
00:02:50,143 --> 00:02:52,534
comme une multiplication de matrice

38
00:02:52,534 --> 00:02:57,423
Donc je prends le vecteur colonne AX,
je fais la transposée,

39
00:02:57,423 --> 00:03:00,826
et je fais le produit matriciel avec Y.

40
00:03:00,826 --> 00:03:06,216
Et puis, ceci est la même chose
que X transposée A transposée Y

41
00:03:06,216 --> 00:03:10,028
et il faut bien que j'utilise quelque part
que la matrice A est symétrique

42
00:03:10,028 --> 00:03:16,089
donc cette égalité-là, ça utilise
l'hypothèse ici A égale à A transposée

43
00:03:16,089 --> 00:03:19,672
donc c'est important que je l'utilise
parce que ce n'est pas du tout vrai sinon.

44
00:03:19,672 --> 00:03:25,154
Donc ici j'ai X transposée AY
et donc, de nouveau, j'ai la transposée

45
00:03:25,154 --> 00:03:29,999
de un vecteur, donc produit avec un autre,

46
00:03:29,999 --> 00:03:34,466
donc ça c'est la même chose
que le produit scalaire entre X et AY

47
00:03:34,466 --> 00:03:38,148
Maintenant <i>A x Y = μ x Y</i>

48
00:03:38,148 --> 00:03:41,994
Donc ici j'ai le produit scalaire
de X avec <i>μ x Y</i>

49
00:03:41,994 --> 00:03:45,644
Le produit scalaire est linéaire
dans les deux coordonnées

50
00:03:45,644 --> 00:03:49,631
donc j'ai μ fois le produit scalaire
de X avec Y

51
00:03:49,631 --> 00:03:53,566
Donc, du coup, j'ai λ fois
le produit scalaire de X avec Y égale à μ

52
00:03:53,566 --> 00:03:55,034
fois ce produit scalaire

53
00:03:55,034 --> 00:04:01,411
donc on a λ - μ fois le produit scalaire
de X avec Y est égale à zéro

54
00:04:01,411 --> 00:04:04,346
Mais on a supposé que
λ est différent de μ

55
00:04:04,346 --> 00:04:11,127
Donc comme λ est différent de μ,
<i>λ - μ</i> n'est pas égale à zéro

56
00:04:11,127 --> 00:04:14,594
et donc, dans ce produit-là,

57
00:04:14,594 --> 00:04:18,552
c'est le X produit scalaire avec Y
qui est égal à zéro.

58
00:04:18,552 --> 00:04:20,900
Et c'est exactement
ce qu'on voulait montrer

59
00:04:20,900 --> 00:04:23,755
Donc on voulait montrer
que les vecteurs propres,

60
00:04:23,755 --> 00:04:26,597
pour les valeurs propres distinctes,
donc le X et le Y,

61
00:04:26,597 --> 00:04:31,858
ça, ça représente le U et V,
sont des vecteurs qui sont orthogonaux.

62
00:04:31,858 --> 00:04:38,237
Donc u est orthogonal à v.

63
00:04:38,867 --> 00:04:41,027
C'est la preuve.

64
00:04:41,027 --> 00:04:46,047
Puis ça c'est la première étape
de la démonstration du théorème suivant.

65
00:04:46,047 --> 00:04:50,675
Donc ici, on avait, on a déjà vu

66
00:04:52,415 --> 00:04:58,765
que A, une matrice
orthogonalement diagonalisable

67
00:05:08,005 --> 00:05:10,580
et nécessairement symétrique ;

68
00:05:15,510 --> 00:05:19,772
et ici, on a un théorème
qui est un des théorèmes spectraux

69
00:05:19,772 --> 00:05:21,846
donc, théorème spectral,

70
00:05:23,206 --> 00:05:26,999
Il y a aussi un théorème spectral
dans le cas des espaces hermitiens

71
00:05:26,999 --> 00:05:29,741
ici on travaille
avec les espaces euclidiens,

72
00:05:29,746 --> 00:05:36,071
donc on se donne une matrice symétrique,
alors il existe une matrice B orthogonale

73
00:05:36,071 --> 00:05:38,443
tel que ceci soit diagonale,
c'est-à-dire que

74
00:05:38,443 --> 00:05:41,361
cette matrice symétrique est
orthogonalement diagonalisable

75
00:05:41,839 --> 00:05:44,318
Donc comme j'ai dit,
c'est le premier théorème de secours

76
00:05:44,318 --> 00:05:46,761
que nous ne pouvons pas démontrer
parce qu'il nous manque

77
00:05:46,761 --> 00:05:50,793
un tout petit peu une sortie
des espaces définis sur ℝ

78
00:05:50,793 --> 00:05:54,146
mais on doit étudier
les espaces vectoriels

79
00:05:54,146 --> 00:05:57,081
où les scalaires sont repris
dans le cours sur les nombres complexes.

80
00:05:57,081 --> 00:05:59,673
On en a déjà parlé
un tout petit peu dans le chapitre 8.

81
00:05:59,673 --> 00:06:02,719
Si on refait la théorie, il n'y a pas
beaucoup de choses qui changent,

82
00:06:02,719 --> 00:06:04,574
mais ce théorème-là,
on ne peut le montrer

83
00:06:04,574 --> 00:06:09,185
que dans le contexte des espaces
vectoriels sur les complexes.

84
00:06:09,185 --> 00:06:11,246
Mais c'est quand même
un théorème très étonnant

85
00:06:11,246 --> 00:06:13,273
parce que c'est un critère très simple.

86
00:06:13,273 --> 00:06:15,222
On regarde une matrice,
elle est symétrique,

87
00:06:15,222 --> 00:06:18,143
alors on sait qu'on peut
la diagonaliser orthogonalement.

88
00:06:18,143 --> 00:06:21,536
Donc sans preuve, malheureusement,
donc sans preuve.

89
00:06:23,806 --> 00:06:25,780
Je vais faire quelques remarques.

90
00:06:29,630 --> 00:06:33,573
Bon, je vais juste d'abord
faire la remarque de redire le théorème.

91
00:06:33,573 --> 00:06:40,253
Donc A une matrice M fois N est
orthogonalement diagonalisable

92
00:06:47,243 --> 00:06:50,650
si et seulement si elle est symétrique.

93
00:06:53,470 --> 00:06:55,774
Donc ça c'est vraiment
le contenu du théorème.

94
00:06:55,774 --> 00:06:57,766
Il y a une direction
que nous avons pu montrer

95
00:06:57,766 --> 00:06:59,412
qui n'était pas difficile

96
00:06:59,412 --> 00:07:01,765
et l'autre direction
que nous n'avons pas pu montrer,

97
00:07:02,175 --> 00:07:03,718
mais on va l'utiliser.

98
00:07:03,728 --> 00:07:06,160
Donc une matrice est
orthogonalement diagonalisable

99
00:07:06,160 --> 00:07:08,699
si et seulement si
cette matrice est symétrique.

100
00:07:08,699 --> 00:07:10,903
Donc il est très facile de déterminer

101
00:07:10,903 --> 00:07:14,251
si une matrice est orthogonalement
diagonalisable ou non,

102
00:07:14,251 --> 00:07:17,722
ce qui n'était pas du tout le cas
avec la diagonalisation simple.

103
00:07:18,892 --> 00:07:20,358
Maintenant deuxième :

104
00:07:20,358 --> 00:07:23,379
Ça veut dire quoi qu'on peut
la diagonaliser orthogonalement ?

105
00:07:23,379 --> 00:07:26,171
Ça veut dire qu'il existe
une base de vecteurs propres.

106
00:07:26,171 --> 00:07:30,248
Donc ça veut dire qu'on peut
factoriser le polynôme carctéristique

107
00:07:30,248 --> 00:07:32,236
en facteurs linéaires.

108
00:07:32,866 --> 00:07:34,499
Donc si A est symétrique,

109
00:07:38,269 --> 00:07:44,809
on peut factoriser
le polynôme caractéristique de A

110
00:07:47,749 --> 00:07:52,909
en un produit de facteurs linéaires.

111
00:07:57,189 --> 00:08:00,276
Maintenant, il n'est peut être pas
facile de faire la factorisation

112
00:08:00,276 --> 00:08:03,174
parce qu'on sait qu'il est parfois
difficile de factoriser des polynômes

113
00:08:03,174 --> 00:08:04,464
mais on est sûr

114
00:08:04,474 --> 00:08:06,984
que toutes les racines sont
des valeurs réelles,

115
00:08:07,464 --> 00:08:08,758
donc, c'est-à-dire ici,

116
00:08:10,698 --> 00:08:13,698
toutes les racines

117
00:08:16,896 --> 00:08:23,346
de l'équation ca(t) = 0

118
00:08:24,488 --> 00:08:26,524
sont des nombres réels.

119
00:08:28,754 --> 00:08:30,838
Donc il n'y a aucune racine complexe.

120
00:08:35,418 --> 00:08:37,515
Ça c'est déjà surprenant aussi.

121
00:08:37,515 --> 00:08:42,047
Une matrice symétrique, on sait
forcément qu'il y a pas de racine complexe

122
00:08:42,047 --> 00:08:43,858
ou de polynômes caractéristiques.

123
00:08:43,858 --> 00:08:45,127
Et puis troisième :

124
00:08:45,127 --> 00:08:48,277
Donc on se rappelle de nouveau
ce que veut dire être diagonalisable :

125
00:08:48,277 --> 00:08:50,200
ça veut dire que,
pour chaque valeur propre,

126
00:08:50,200 --> 00:08:53,970
la dimension de l'espace propre est égale
à la multiplicité algébrique

127
00:08:53,970 --> 00:08:55,875
de cette valeur propre.

128
00:08:55,875 --> 00:08:57,970
Donc, si A est symétrique,

129
00:09:02,040 --> 00:09:04,300
pour chaque valeur propre λ de A,

130
00:09:10,740 --> 00:09:16,903
la dimension de l'espace propre Eλ

131
00:09:19,363 --> 00:09:25,063
est égale à la multiplicité algébrique

132
00:09:30,703 --> 00:09:34,545
de λ comme racine
du polynôme caractéristique.

133
00:09:40,515 --> 00:09:42,077
Donc ça aussi c'est surprenant.

134
00:09:42,077 --> 00:09:45,613
Il suffit que la matrice soit
symétrique, ensuite on est sûr

135
00:09:45,613 --> 00:09:49,912
qu'on peut factoriser complètement sur ℝ
et puis que pour chaque racine

136
00:09:49,912 --> 00:09:52,765
du polynôme caractéristique,
donc pour chaque valeur propre,

137
00:09:52,765 --> 00:09:56,035
la multiplicité géométrique sera égale
à la multiplicité algébrique

138
00:09:56,035 --> 00:09:57,465
de cette racine.

139
00:09:57,465 --> 00:10:03,220
Et maintenant, dans la prochaine vidéo,
enfin, nous allons calculer.
