I am Sean Hill, co-director of the Blue Brain Project and the head of the division of the neuroinformatics.
I'll be talking today about some of the neuro informatics' issues and challenges in organizing all of the data needed for the data driven modeling process.
So, I'll start off taking you through the importance of meta data and provenance and how we use that to build up the knowledge graph
I'll introduce ontologies
I'll give some examples of text mining and data integration from the literature and I'll also just give a quick overview of the brain Atlases and search and interface into our knowledge base.
So, first of all, the challenge in neuroscience is really that neuroscience data is really heterogenous.
Nearly every piece of data is gathered in a different lab with different protocols and different experimental techniques and the challenge is of how do we take all this data which is really ranging from the sub-cellular level from gene expression and proteomics to the cellular level with individual cell electro physiology and morphology up to the tissue level and whole brain scale with the new techniques coming in to measure the whole brain activity
EEG behavioral measurements.
How do we actually integrate that data?
How do we organize it so that we can find the relationships between those different levels and ultimately build models that helps us go from gene to behavior and this is the challenge in neuroscience.
And the way that we approach it in Blue Brain and this is the challenge in neuroscience.
And the way that we approach it in Blue Brain is really to establish an iterative cycle of data integration by building models but building digital preconstructions that help us understand how do the pieces of data actually fit together and then if we take this step by step by starting from data available data from the literature, existing models existing data repositories, we have to organize that data into Brain Atlases so we can start using to build models run simulations, and then perform a validation step and that validation is really essential to the process because it's only by validating the model by showing where does it work and where does it not work where is it consistent with biological data and where does it fail to recreate the biological phenomenon that we know occur.
And it's through that process and iteratively performing that process that we improve the models that we identify new experiments that should be performed missing data that would be critical that would be filling in key gaps between the levels, and overall starting to identify the principals by which this whole brain operates across different scales.
And the neuroinformatics platform is really the key to supporting that process, that center of the process by enabling the data to be organized data that maybe distributed around the world
And here, there we got different components of the neuroinformatics platform.
One is the data space, which is really where is the data situated, distributed around the world.
We've got data registration a process by which we annotate the data with the meta data necessary to discover it and to anchor it in Atlases.
We've god the knowledge graph and search which I'll talk about, which allows us to actually find and discover data and the related data so any type of data and its properties as well as Atlases, which help us to navigate the data discover data related to particular brain structures and also anchor data into a standard space.
And finally, we have the knowledge space which the online community encyclopedia which really provides a place where the current state of knowledge can be maintained in terms of encyclopedic article but also links to data and data that is either in the knowledge graph that we're talking about or in other curated data bases from around the world.
So where is the data?
This is actually a massive challenge for neuroscience because most of, what's the so called the Long tail data is actually in printed documents or it's unpublished.
There are about a Hundred thousand publications or it's unpublished.
There are about a Hundred thousand publications each year in neuroscience, but unfortunately a print publication or a .PDF is not a machine accessible way to get data or even to get knowledge.
It really requires reading the paper understanding what is talked about and seeing if you can extract data from that paper manually.
And that's a massive challenge it's not an efficient way for the future of sharing data.
What we see on this graph is that there is organized big data in the Allen Institute for Brain Science is probably the best example of generating high quality large scale data, but as I was saying most of the data is actually unpublished or dark data or it's referred to in papers.
And there's a big challenge of how do we discover it how do we find it, how do we organize it how do we make it accessible?
And the way that you do that you've got to be able to make it possible to combine data, so in order to find the small data and to make it possible to combine it to make big data, is that you've got to make that small data discoverable you've got to make it machine accessible you've got to use the standardized meta data data types, data structures, and identify which data has actually been produced using a compatible experimental method or from a compatible experimental context from which data can be compared and that's a massive challenge.
Now, there are different ways of organizing data.
One of those is to put them into folders on a hard drive and this is quite common when you ask somebody to share their data, very often it comes in folders in sets of different files, and you navigate the folders to find the pieces of data that you're looking for.
Now, a somewhat more sophisticated approach is to actually have searchable meta data that tags this data with properties that are then searchable and this gives you a lot of flexibility and how you can discover and access the data.
So, we really think that having metadata annotations for the data files and making that searchable is really key to having a much more flexible way to discovering and accessing the data.
So, there are three main classes of meta data.
There's descriptive meta data and that's really the data that describes the key properties of the data for discovery and identification.
That includes things like the title, the abstract the author, maybe keywords that help identify the data set.
Structural meta data is information about the structure of the different pieces of data how are they put together, for example in a document how are the pages ordered to make chapters.
Administrative meta data is actually that meta data that provides information on how that resource can be accessed, by who what file type is it, it really helps provide information about who can, who should and what ways in which that data set can be accessed.
And that's important for things like protecting privacy or knowing when the data set actually hasn't been accessed for a long time and can be archived to tape.
But coming out of the laboratory that's really the point where you want to have the richest meta data annotations, in fact one of the greatest challenges in current neuroscience is that when you get a data set it usually it doesn't have the meta data that you would when you get a data set it usually it doesn't have the meta data that you would need to actually understand the contents and it would be a lot of work for the experimenter to go back and add that meta data.
So, we're really advocating that at the point of data capture in the laboratory that you capture all the meta data needed and some of the best practices that we would advocate is using the electronic note books use of the laboratory information management systems electronically capture the details of the specimen.
What species is it?
What experimental protocol are you using?
What is the data captured from specific experiments?
Document the experimental design that includes having the hypothesis clearly documented the objectives the protocols are really critical and the provenance in terms of which data samples came from which specimens, and so on.
Use standardized vocabularies' when recoding meta data values.
This is really essential because in the end when you want to analyze what you discovered you want to be as consistent and as rigorous as possible in the way that you annotate the data.
And also, finally, curate the data when you're selecting the key set of data that is used for the publication or is used to be shared with the community select the high-quality data, make sure that you flag and remove the data with questionable quality.
You as the experimenter really are the best person to judge and the best time to make that decision is when you're very familiar with the data that has been captured from the experiment.
One of the additional challenges is what meta data is really needed.
How do you find neuroscience data set if you're talking about any possible type of neuroscience data set?
And this is a big challenge, and it still remains to be addressed by the community in terms of setting the common standards od what is the minimal meta data needed to describe a neuroscience data set.
Here we've developed an initial proposal we want to see this developed further in collaboration with the community but also adopted widely.
So, what our proposal is what we call
Ã¬The Minimal Information for Neuroscience DatasetsÃ®
M.I.N.D.S. And that means what that includes is actually capturing details of the subject of the experiment, the species, the strain, the sex the methods used, the protocols the equipment, and the parameters used to perform the experiment the classification, what is the data category the data format, the cell type the brain location where in the brain in a standard Atlas either in terms of coordinates or in terms of just standardized names for that brain region was this data captured, who other contributors and what are their affiliations and ideally identify those contributors by something like an Orchid, a standard identifier for authors so they can get credit for making their data available?
And then, importantly, and URL or a persistent identifier, and this is something like a DOI that allows you to actually go online and directly access the raw data.
So that URL is the key part of the Minimal Information for
Neuroscience Datasets.
So, in this session we talked about how neuroscience data is very heterogenous and some of the challenges in organizing it described to you what meta data is and how we can use meta data to organize data sets and I've also described to you M.I.N.D.S. it's a proposed initial standard for a minimal meta data set for neuroscience data and we thing that's really essential to starting to get the broader community to consistently annotate and organize their data.
