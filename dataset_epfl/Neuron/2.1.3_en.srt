1
00:00:05,431 --> 00:00:07,319
Hello, my name is Samuel Kerrien

2
00:00:07,518 --> 00:00:10,518
I'm the manager of
the Blue Brain Nexus Project

3
00:00:10,753 --> 00:00:15,428
which is currently being
developed by the Blue Brain Project.

4
00:00:15,652 --> 00:00:18,527
Today we're going to look
at the challenges faced

5
00:00:18,736 --> 00:00:21,298
by neuroscience in data integration,

6
00:00:21,522 --> 00:00:24,809
we're going to look at
various approaches to do

7
00:00:25,049 --> 00:00:26,349
data integration.

8
00:00:26,576 --> 00:00:29,051
Then we're going to have a look
at the Blue Brain Nexus :

9
00:00:29,250 --> 00:00:32,250
a data platform,
and in that topic

10
00:00:32,627 --> 00:00:35,102
we're going to look
at it's architecture

11
00:00:35,329 --> 00:00:39,654
at how to do the main design,
at the management of ontologies

12
00:00:39,883 --> 00:00:44,208
and how to re-use existing
and well defined domains.

13
00:00:44,405 --> 00:00:46,480
Finally, we're going
to look at MINDS

14
00:00:46,674 --> 00:00:50,411
and what it enables in
the context of data integration.

15
00:00:50,753 --> 00:00:56,466
So first of all, I'll come back
to the issue with data integration

16
00:00:56,788 --> 00:01:02,525
which is that in neuroscience there's
a really broad variety of data.

17
00:01:03,044 --> 00:01:08,144
Not only have we got multiple
scales we want to capture

18
00:01:08,386 --> 00:01:12,848
from sub cellular resolution,
to cellular, to tissue resolution,

19
00:01:14,080 --> 00:01:18,568
and the whole brain, but also
within each of these scales

20
00:01:18,856 --> 00:01:21,068
we have multiple
data modalities.

21
00:01:21,315 --> 00:01:25,803
And so this leads to serious
challenges in integrating data

22
00:01:26,074 --> 00:01:29,074
as you have to cope with
them all in a seamless manner.

23
00:01:29,487 --> 00:01:32,487
So here we're going to discuss
various challenges

24
00:01:32,754 --> 00:01:35,542
of integrating data
in neuroscience.

25
00:01:35,775 --> 00:01:39,875
So like we said,
data is extremely varied.

26
00:01:40,118 --> 00:01:44,805
Then we have data ranging
from very small data to data set

27
00:01:45,061 --> 00:01:49,124
around Petabytes, which is
just by the shear size of it

28
00:01:49,320 --> 00:01:50,807
difficult to handle.

29
00:01:51,029 --> 00:01:54,029
The data, by it's nature
can be distributed in silos

30
00:01:54,375 --> 00:01:56,913
whether it is within
the same organizations,

31
00:01:57,147 --> 00:02:01,385
people having their own
packets of data,

32
00:02:01,636 --> 00:02:03,661
all the way to data being

33
00:02:03,891 --> 00:02:06,891
distributed through
different organizations,

34
00:02:07,154 --> 00:02:10,742
and connecting them
together can be difficult.

35
00:02:11,886 --> 00:02:14,274
There's an obvious
need for data provenance

36
00:02:14,505 --> 00:02:16,455
and I'm not going to
elaborate too much

37
00:02:16,648 --> 00:02:19,161
on these aspects right now,
we're going to look at it

38
00:02:19,352 --> 00:02:20,990
in more detail later;

39
00:02:21,213 --> 00:02:23,350
but essentially you
want to know where

40
00:02:23,575 --> 00:02:26,575
data came from if
the data set was generated

41
00:02:26,833 --> 00:02:28,533
from another specific one

42
00:02:28,711 --> 00:02:30,207
that gives you a lot of useful
information to know why

43
00:02:30,207 --> 00:02:31,086
this data set is and
that gives you a lot of useful
information to know why

44
00:02:31,086 --> 00:02:31,302
this data set is and

45
00:02:31,302 --> 00:02:33,207
assess it's quality.
this data set is and

46
00:02:33,207 --> 00:02:35,352
assess it's quality.

47
00:02:35,772 --> 00:02:39,347
Then you have the need to
discover similar related data

48
00:02:39,565 --> 00:02:44,390
so obviously you bring your
own data into a platform

49
00:02:44,852 --> 00:02:49,915
you are running your own science
and it is very useful to be able

50
00:02:50,132 --> 00:02:53,370
to tap into what the rest of
the community has actually

51
00:02:53,600 --> 00:02:55,875
brought into the platform in order

52
00:02:56,078 --> 00:02:59,141
to enrich whatever data sets
you had at your disposal

53
00:02:59,335 --> 00:03:03,335
and improve the science
you are currently running.

54
00:03:03,740 --> 00:03:08,365
Finally, neuroscience has
very ambitious goals

55
00:03:08,690 --> 00:03:12,402
and that does require a state of
the art data platform to support it.

56
00:03:12,586 --> 00:03:18,124
So, just to give you a few examples,
the automatic reconstructions

57
00:03:18,339 --> 00:03:23,639
or the reconstruction of neurons,
so here, on the right-hand side

58
00:03:23,868 --> 00:03:28,305
we can see a slice of a mouse brain
that has been generated by

59
00:03:28,517 --> 00:03:33,692
Wuhan University in China, and they
have reconstructed by hand

60
00:03:33,912 --> 00:03:37,437
the morphology of neurons
which you see highlighted:

61
00:03:37,649 --> 00:03:39,599
you have one here,
you have another yellow one,

62
00:03:39,829 --> 00:03:42,829
you have the red one here,
and the green one, so four neurons

63
00:03:43,060 --> 00:03:46,497
spanning the whole width
of the brain, and this task

64
00:03:46,714 --> 00:03:50,964
is extremely time consuming,
thus very expensive.

65
00:03:51,177 --> 00:03:56,052
And wielding tools that will allow
to automate this reconstruction

66
00:03:56,234 --> 00:03:58,309
is extremely valuable.

67
00:03:58,512 --> 00:04:00,887
Now that's a very
difficult task as well

68
00:04:01,088 --> 00:04:04,088
and thus it is
a significant challenge.

69
00:04:04,365 --> 00:04:07,365
And we have a few other examples,
such as electro-physiology

70
00:04:07,556 --> 00:04:12,006
analysis, whole brain imaging,
and finally large scale simulations

71
00:04:12,204 --> 00:04:15,642
validation, which is the heat of
what Blue Brain does.

72
00:04:16,477 --> 00:04:20,627
So next we're going to have a brief
look at the various components

73
00:04:20,877 --> 00:04:23,602
of what makes
a neuro-informatics platform,

74
00:04:23,865 --> 00:04:26,240
and here I'm not going to
give you all the details that

75
00:04:26,463 --> 00:04:29,613
Sean has already explained
in previous videos,

76
00:04:29,854 --> 00:04:33,267
but I want ask to really focus
on the Knowledge Graph

77
00:04:33,472 --> 00:04:37,910
and the search facilities that
the knowledge graph does enable.

78
00:04:38,305 --> 00:04:41,867
So in the next few slides
we're really going to focus on

79
00:04:42,105 --> 00:04:43,255
that part.

80
00:04:43,589 --> 00:04:46,589
So what is the purpose
of the Knowledge Graph?

81
00:04:47,242 --> 00:04:51,054
It is to represent and validate
and store scientific knowledge

82
00:04:51,294 --> 00:04:53,832
and when I say validate,
it means when you bring

83
00:04:54,028 --> 00:04:57,028
the information into
this knowledge graph

84
00:04:57,304 --> 00:05:02,054
It is able to tell you if you have
indeed connected your elements

85
00:05:02,261 --> 00:05:03,186
in the right order,

86
00:05:03,368 --> 00:05:06,368
if you have provided
the right amount of meta data,

87
00:05:06,632 --> 00:05:12,257
if mandatory information
has been indeed provided.

88
00:05:12,487 --> 00:05:15,449
So I'm listing two things here:

89
00:05:15,652 --> 00:05:20,814
This encompasses the entities of
knowledge you're trying to capture

90
00:05:21,758 --> 00:05:24,445
with it's meta data,
but also with

91
00:05:24,645 --> 00:05:26,845
raw data that is attached
to these entities.

92
00:05:27,028 --> 00:05:29,615
You can think of raw data
as your data sets.

93
00:05:30,508 --> 00:05:33,658
Also there's the concern of
provenance that links these

94
00:05:33,872 --> 00:05:36,872
entities together as they are
derived from one and other.

95
00:05:39,320 --> 00:05:43,283
Next, it allows you to search data
based on user defined criteria

96
00:05:43,532 --> 00:05:46,957
and that can be either on
the entity's meta data

97
00:05:47,169 --> 00:05:50,031
whether they are literal values
or ontology terms

98
00:05:50,245 --> 00:05:52,620
and I'll be giving
an example of that later

99
00:05:52,839 --> 00:05:55,464
but at the same time,
it can also be based on

100
00:05:55,684 --> 00:05:58,684
provenance information,
so a user should be able to build

101
00:05:58,906 --> 00:06:03,318
and formulate scientific questions,

102
00:06:03,503 --> 00:06:05,228
submit a query to
the Knowledge Graph,

103
00:06:05,414 --> 00:06:07,952
that both encompasses
meta data

104
00:06:08,174 --> 00:06:09,811
and provenance information,

105
00:06:10,018 --> 00:06:13,456
that gets an answer
to its question.

106
00:06:14,176 --> 00:06:17,176
Finally you want to be able
to discover new data through

107
00:06:17,573 --> 00:06:21,398
the main specific commonalities,
so by leveraging:

108
00:06:22,721 --> 00:06:26,734
meta data is shared by
a number of these data sets

109
00:06:26,941 --> 00:06:28,741
you want to be able to actually find

110
00:06:28,930 --> 00:06:32,405
new things that you weren't
aware of originally

111
00:06:32,609 --> 00:06:34,847
and again, we'll cover
more on that later

112
00:06:35,051 --> 00:06:37,401
and give very concrete examples.

113
00:06:37,919 --> 00:06:41,531
So I have been mentioning
provenance in the past few slides

114
00:06:41,738 --> 00:06:44,675
and I think I would like to take
the time now to actually

115
00:06:44,893 --> 00:06:48,368
tell you again what is provenance.

116
00:06:49,064 --> 00:06:53,877
At a very simple level,
it is a way to tell where your

117
00:06:54,100 --> 00:06:57,512
entities are coming from,
and one way to do that

118
00:06:57,730 --> 00:07:02,055
for instance, is to use
the concept of activity.

119
00:07:02,279 --> 00:07:06,991
So activity may consume or use
a number of entities,

120
00:07:07,197 --> 00:07:15,260
an activity would generate a number,
one or many other entities.

121
00:07:16,400 --> 00:07:22,050
And this activity is associated with
agents and these agents can be

122
00:07:22,229 --> 00:07:25,379
people, you can imagine them like
people doing an experiment in

123
00:07:25,595 --> 00:07:28,558
the lab and generating
a specific data set

124
00:07:28,916 --> 00:07:32,054
it could be software, it has been
used in the process of running this

125
00:07:32,244 --> 00:07:35,219
activity to generate
these entities

126
00:07:35,434 --> 00:07:40,372
so on a very simple level
this is what provenance is

127
00:07:40,617 --> 00:07:43,930
and it really enables you to
track where things came from

128
00:07:44,136 --> 00:07:47,136
and how they have been
deriving into other things.

129
00:07:47,705 --> 00:07:50,018
Now, why provenance?

130
00:07:50,238 --> 00:07:52,563
It really does enable
you to do a lot of things.

131
00:07:52,772 --> 00:07:55,997
Like I said earlier, it enables you
to track the original data

132
00:07:56,234 --> 00:08:00,421
track how the data is being used
and we call that derivation.

133
00:08:01,748 --> 00:08:05,948
Having this information at hand
allows you to asses the quality

134
00:08:06,159 --> 00:08:09,522
of the data and
the trust in the data

135
00:08:09,730 --> 00:08:14,018
you might be re-using for
your own use cases.

136
00:08:14,722 --> 00:08:16,909
Reproducability of workflows
can be ensured as

137
00:08:17,103 --> 00:08:20,703
if you capture all the necessary
information that allows

138
00:08:20,932 --> 00:08:22,457
the generation of
the given data sets

139
00:08:22,663 --> 00:08:25,638
you could just re-run the same
workflow with the same parameters

140
00:08:25,830 --> 00:08:28,830
and reproduce these
exact same data sets,

141
00:08:29,495 --> 00:08:30,845
quite easily.

142
00:08:31,068 --> 00:08:36,068
Another use for provenance
is attribution, whether it is about

143
00:08:36,255 --> 00:08:38,955
the data you're generating,
or algorithms you're creating

144
00:08:39,147 --> 00:08:42,022
you can really find out who
has done the work

145
00:08:42,245 --> 00:08:45,245
and which affiliations they
are associated with.

