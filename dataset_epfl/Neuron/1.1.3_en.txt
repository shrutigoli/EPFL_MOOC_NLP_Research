Okay, so let's look now at some of the principles.
What are the principles on which simulation neuroscience rests?
So, the first important principle is that we have to do dense reconstructions dense, very detailed reconstructions and we have to do it from sparse data.
Ok, so, I just gave you an idea, a glimpse at how much data there really is to measure.
And if anybody has to do a calculation you will realize that it would take the entire human race, thousands of years if you wanted to map out all of those things in all of those different connotations.
So, we will forever have sparse data.
No matter how much data we collect, it will forever be sparse in terms of experimental.
So, the problem we have to solve is how do we get a complete picture from sparse data.
So, the first principle of simulation neuroscience is we have to establish the strategy from going to a little bit of data to using whatever knowledge we have of how the pieces fit together to build algorithms and algorithmically build the circuit.
Now, this first principle is not a new principle.
It is extremely old, it is the basis, actually, of our entire society today of information and communication technologies.
Let me try and explain this in another way.
Imagine that you have a TV screen
Okay?
And the channel is unplugged.
So, it is a white noise.
So, basically every pixel is independent of every pixel.
Okay?
And now I want to transmit this image to you.
I want to tell you what is this image.
I need to give you a complete picture of what this image is.
Based on Claude Shannon's work from the 40s we know the theory here
It is very clear.
I have to measure if this is a random system. 
If this is a random image or a random system
I have to measure every single pixel.
That is I have no way out, but to measure every pixel if I wanna tell you about this complete story here
Okay?
But it is only if I have a random system.
If I now have an organized system where there is a very clear structure the theory shows you absolutely clearly that I don't need to transmit every pixel to here, I don't need to transmit every single pixel,
I don't have to tell you what every pixel is here because there are relationships between all these pixels and it is the basis of image compression.
So, this is order.
So, in this case if I have a random system
I don't need any intelligence,
I just have to copy these pixels across.
So, I don't need an algorithm here.
Okay?
I don't need an algorithm, I just need to transfer the data.
But if I have the order in the system
I need a rule how to rebuild it if I am going to send a few of these pixels across and I want to rebuild this picture.
I need an algorithm.
Okay?
I need some rule that will rebuild the relationships that are in this picture.
Okay, so in a way with a random system I need to measure everything and I do it blindly, I don't understand anything,
I just have to copy it from here to here.
In an ordered system I have to understand order.
And I have to package it into an algorithm.
Then I can send it to you and I can reconstruct it.
Basically, the important message is that dense reconstructing from sparse data is not just because we have to, because we forever have sparse data it is because it is science.
It is actually not scientific to say that we need to go and measure everything in the brain.
Okay? Only if it is a random system do we need to measure every pixel.
You don't need any principles, you don't have to understand very much and you can close your eyes and blindly map one thing to the next.
So, when there is an objection that says:
"Oh, you can't reconstruct or simulate a cell because we don't know everything about the cell", that is not a scientific statement.
The challenge is to find the minimum data you need.
The smallest data set, not the largest data set.
The real science is to say what is the least amount of data I need to be able to reconstruct it.
That is a compression algorithm.
It is a simile, it is the inverse, what is the least amount of data that I need to transmit to you so, you can rebuild the image.
Okay.
In that case you do not need to measure everything, you need principles and that is what drives simulation neuroscience.
And then you algorithmically reconstruct it.
So a simulation neuroscience is fundamentally about finding those principles, packaging them into algorithms, and then reconstructing.
That is the first principle of simulation neuroscience.
The second principle is, actually also quite similar to computer and software engineering.
It is a bottom-up reconstruction process.
And it actually has very strict rules.
And the rules are, first of all, you have got to follow the biological principles.
You don't try and invent how you think the brain should look. or the neuron should look.
You measure and you use as much of the data that is available and where it is not available you have a hypothesis of what is available.
And you can then test that hypothesis.
You build the smallest components first you really start atomic, if you wish, you start at the eye channel to build a neuron, you don't start at the neuron and then try make a neuron behave and then say:
"Oh, let me go back and build eye channels".
You start at the very bottom and you build upwards
Okay?
And a key thing which is the same as what you do in computer and software engineering, you build modules and cells, and functions and there you freeze your variables and you freeze them.
And then you combine them.
We go to pi cell, it is so beautiful, it is combinatorial because I just built these and I can consume them later.
But in order to do that I freeze the components and then I can combine the components.
And the other important thing and very different from computation neuroscience, for example is you validate upwards.
In other words, you validate what emerges when you combine things.
You don't try to fit and your data, you never fit the data looking up at an emerging property.
So, for example, you wouldn't try to say: "look, my brain is not speaking how can I adjust my eye channels until I speak?"
You will be adjusting eye channels forever. 
Okay, that is a massively, it is an intractable problem.
So, you have got to come down to the emerging property as close to that parameter as possible.
For example, an eye channel, you want to build an eye channel you look at the behavior of an eye channel.
You don't look at the behavior of the neuron to build an eye channel.
So, you always do it, at the bottom you freeze it, and then when you say: "ok, I have got my eye channels,
I want to know what they do,
I put them in a neuron and then I look at the neuronal behavior and that way I can see how they play out when they are being used.
If you find that you can never replicate the neurons behavior then you can say: "maybe
I got my eye channels wrong.
Maybe I don't have the right eye channels."
And so on.
So that is the way we begin the cycle, which we are going to learn a lot about in the course of this and other lectures.
The guiding principle here is that if you build it right it will automatically behave right.
OK, so you build bottom up.
You build the components you freeze them.
If this is valid in terms of the behavior even if you don't know all the pieces, but if a neuron behaves right, and you put in a circuit you can talk about what neurons do.
If the circuit behaves right and you put it into a brain you can talk about what the circuit does and not the brain.
OK, so you don't try and tweak the model to behave right.
If the neuron, if you built it right it automatically behaves right.
The last, the third, and there are of course many but the third principle is that it is iterative reconstruction and testing.
So you get your experimental data, you get your literature, you go and find all the papers you can you mind all the facts you possibly can. and you try and build the unifying model of a neuron.
You find the eye channels, you find the morphologies, the structures the physiology, any data that you have about that neuron you collect.
And then the challenge is how do you package that into the simplest algorithm that will then reconstruct it that is consistent with all the data that you gathered?
You obtain your model and then you interrogate it anatomically, you look at it and you see: "OK, I got it right, it looks like a real neuron,
I do the statistics between my reconstructed neuron and the real neuron and I see that they match".
I look at where the eye channels are, they match and so on.
And then you simulate it to see if it behaves the way that it does in the real tissue.
And then you do an analysis, you do the systematic comparison and you get your errors.
Well, it is not going to work the first time.
In fact, the goal is to do it very fast, fail very fast, identify where you may have got the wrong data or the wrong assumptions and then you build it again, and you keep going through the cycle as many times as you possibly can.
In fact, it is an endless loop, you keep going and integrating and perfecting it, but with each cycle, this model replicates the tissue more and more accurately.
You follow this process, you can't go left, you can't go right you can't go backwards, you can only go forward.
So, the model can only improve each and every time that you integrate it.
So, to illustrate this let's just imagine that you step inside a forest of neurons and you look up at the neurons, there are thousands of neurons millions of synapses, and you look around.
And you say:
"I wonder how these neurons connect?
I mean, how are they connecting with each other?
Who is connected with each other?
Where are all these...?"
So, how do you approach this problem?
Of course you can try and let's take an electro microscope and measure everything that is, of course, one approach, but it you actually look at it you will be measuring for a very very long time
But there is another approach and the way that we do it in simulation neuroscience, we say:
"OK, let's just look at what we know.
Let's start off and let's imagine that we have a whole series of cells, neurons".
OK.
And, of course, there are different kinds of neurons.
You have many different kinds of neurons.
Each one looks different, behaves different and it can, of course, connect differently to the others.
So, how do you approach this problem ?
Well, you can say: "What do we know?"
And maybe there's only one experiment that tells you how this one cell, this one type of cell connects with others.
There are potentially thousands of different types of connections here, but you only have one experimental data point.
And this experimental data point you go and you see that the axon that goes to connect all these neurons does this.
And you look at this and you analyze the data here and you say: 
"Oh, I can see the principle". 
First principle:
The axon somehow touches every neuron.
Okay?
So, before I look at who is connected
I just see that the way this axon goes, it is like what we call a tabular riser.
It goes and it touches everything on you.
And then I look at this one neuron and I say: Oh, I see that when it touches, when it touches onto another cell, it actually puts lots of little synapses on it
It never puts just one synapse".
OK?
It never puts just one synapse it puts lots of them.
So, I see principle number 2: it is a multi synapse rule.
Okay?
And I think: "oh that's interesting, but what else can I learn by looking at this one experiment?"
I see principle number 3:
I see, wow, if I look at the axon of the cell, and I look at it very carefully
I see that it has got places called blue tones which is where it can form synapses.
So, I think: "how many are there?
And what is the space between them?"
And then I say: "okay, that will limit how many neurons this one can contact", right, because if I have an axon only as two then it is, of course, going to have a very different kind of activity than I have axon that has many boutons.
Then now I have a bouton principle.
It is a physical limitation.
If I have lots of boutons, if I have a neuron with an axon with lots of boutons, then
I can contact lots of cells.
If I have one, I can contact one cell.
So, these are 3 principles.
Now there are more principles, but let's just assume, just say: "OK
I like these 3 principles,
I am going to see if I can rebuild the tissue just with these three principles".
So, we package them into an algorithm.
Okay?
And digitally we reconstruct this tissue.
Now we have 2 possible scenarios: it succeeds or it fails.
Succeeds means that you have a look at it and then somebody tells you:
"Hey, you know what?
There are 10 other experiments out there that looked at how this cell connects, and this cell connects and you have other data that you can check it with".
And then when you look at that you see that your reconstructed tissue now reproduces all of that experiment.
So, you think: "wow, I mean I looked at 1 cell, applied 3 principles of how they should connect with each other and they reproduce what other people have done in other laboratories all over the world".
That is a success.
But what is important to understand with this success is that you haven't really gained any new knowledge, but you have gained a new understanding.
You understand how your principles interact.
You understand that this principle for this neuron applies to all of them.
Two very concrete new understandings.
It is not new knowledge because this knowledge existed, people had measured this, we saw this, it was just fragmented knowledge but you didn't even had that sort of integrated, unified view.
So, when it succeeds, you understand better.
I am an experimental neuroscientist and I have looked under microscopes for 20 years.
And when I put this together, in a single reconstruction
I understand what I was doing for all those 20 years.
Because you can see this as an integrated, an integrated answer or perspective on what the pieces are that you have been collecting for so long.
Now the other scenario is it fails.
I do this, I check it again, somebody comes into the door and they say: "Look, I have an experiment and it shows me that this cell, it really doesn't follow those principles and it connects in a completely different way".
And then I have new knowledge.
I know that these principles are not enough.
There have to be new principles.
There have to be some other rules that will apply whether they are exceptions, or whether they are that these principles have to be modified.
But there has to be something else that has to be understood.
And then one can start investigating and that triggers, when you fail it triggers new experiments which you have to test now.
How would an algorithm reconcile these differences?
Another way to describe this is imagine that this algorithm works in one part of the brain.
And it explains all the way the neurons connect.
Now I go to another part of the brain and it doesn't work there.
You think: "that is terrible"!
No, that is great, because it tells you that in that other part of the brain there is something else going on.
And that something else nobody knew about before.
Because you had too much fragmented knowledge.
That is when failing really leads to an advancement of knowledge.
New knowledge.
Because it triggers a completely new perspective.
It actually shows you the boundaries of a current knowledge.
You get to the end and you say :
"I've taken all the data, all the principles and I can't explain how this happens".
Then you are at the frontier.
And at that frontier you can either do theory, you can do experiment you can iteratively test, but there are many ways to advance it.
And this is a very exciting approach to advancing knowledge.
Okay, so we looked at these
3 principles on which simulation neuroscience is based.
