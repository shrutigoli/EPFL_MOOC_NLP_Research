1
00:00:05,657 --> 00:00:07,957
So, in the next couple of slides,

2
00:00:08,157 --> 00:00:10,475
we are going to have a look
at the various approaches

3
00:00:10,675 --> 00:00:14,835
we can use to do data integration
into a KnowledgeGraph.

4
00:00:16,234 --> 00:00:18,736
Here, I am going to present
a first approach

5
00:00:18,936 --> 00:00:21,094
which I called 
"Fixed Data Model".

6
00:00:22,286 --> 00:00:26,053
In this approach,
we have designed a model

7
00:00:26,416 --> 00:00:28,292
with entities and provenance

8
00:00:28,492 --> 00:00:32,789
that is aimed at encompassing
any used cases we meet 

9
00:00:32,989 --> 00:00:35,153
in the field of neuroscience.

10
00:00:36,037 --> 00:00:39,151
I'm going to illustrate a little bit
its structure now

11
00:00:39,810 --> 00:00:43,338
First, we have a specimen,

12
00:00:43,538 --> 00:00:47,513
and the specimen is really
the subject of which we study the brain,

13
00:00:48,246 --> 00:00:54,013
and so that specimen can be derived
into a number of samples,

14
00:00:54,622 --> 00:00:57,622
which can derive from each other.

15
00:00:58,461 --> 00:01:04,424
So, for instance, a sample can be
a specific slice of that brain 

16
00:01:05,114 --> 00:01:08,302
into which we are going to study
a number of neurons.

17
00:01:08,656 --> 00:01:11,994
So, if I were to do this use-case,

18
00:01:12,503 --> 00:01:15,793
I would have a specimen,
then I would have

19
00:01:16,590 --> 00:01:17,979
a slice,

20
00:01:21,449 --> 00:01:23,179
and I would have
right next to it

21
00:01:24,719 --> 00:01:25,585
a neuron.

22
00:01:26,224 --> 00:01:29,482
Both of which, slice and neuron
would be of type sample,

23
00:01:29,805 --> 00:01:31,619
I would just have to have a property

24
00:01:31,819 --> 00:01:34,956
in the sample that describes
that this is actually a slice,

25
00:01:35,156 --> 00:01:38,026
and this other one
is a neuron.

26
00:01:39,057 --> 00:01:41,150
Now, let's say I'm in a lab,

27
00:01:41,350 --> 00:01:43,890
and I want to do
and electrophysiology recording

28
00:01:44,090 --> 00:01:46,186
of this specific neuron,

29
00:01:46,837 --> 00:01:49,329
I can then

30
00:01:50,376 --> 00:01:52,728
design an activity
with some protocol

31
00:01:52,928 --> 00:01:56,918
that is going to describe
how this experiment is being run.

32
00:01:57,701 --> 00:02:00,686
And that experiment is going to generate
a dataset.

33
00:02:01,349 --> 00:02:04,390
Itself, it will have a specific category
that can be for instance 

34
00:02:04,590 --> 00:02:06,106
"electrophysiology recording".

35
00:02:07,520 --> 00:02:10,789
And this is really the entity 
and the metadata describing the dataset,

36
00:02:11,108 --> 00:02:14,676
and we have also
the raw data for this dataset,

37
00:02:14,944 --> 00:02:18,390
as in the files that have been generated
on your local hard drive

38
00:02:18,590 --> 00:02:20,026
when you did your recording.

39
00:02:20,832 --> 00:02:23,610
And that is really
what came out of it.

40
00:02:25,036 --> 00:02:28,852
Another example here could have been
a morphology reconstruction.

41
00:02:30,213 --> 00:02:33,100
So, the scientist can,
for instance, 

42
00:02:33,480 --> 00:02:35,432
study the data
that came out

43
00:02:36,108 --> 00:02:37,300
of this experiment,

44
00:02:37,593 --> 00:02:40,617
and do further annotation,
or classification 

45
00:02:40,816 --> 00:02:44,240
of this neuron here.

46
00:02:45,111 --> 00:02:49,223
And for instance, define that
this is a fast spiking cell,

47
00:02:49,423 --> 00:02:53,320
by studying the directory response
of this specific neuron

48
00:02:54,426 --> 00:02:57,127
so the scientist can make claims,

49
00:02:57,331 --> 00:02:59,378
and something I haven't mentioned
here,

50
00:02:59,578 --> 00:03:02,211
the activity obviously has a number
of agents,

51
00:03:02,411 --> 00:03:06,025
whether it is a scientist
who run the experiment 

52
00:03:06,225 --> 00:03:12,937
or the software that was used to acquire
the directory physiology recording.

53
00:03:15,393 --> 00:03:18,864
So, while this approach can work,

54
00:03:19,413 --> 00:03:23,545
if you are fine dealing
with a very generic data model,

55
00:03:24,080 --> 00:03:28,172
and do not want to do very deep
data integration,

56
00:03:28,643 --> 00:03:29,868
that method can work.

57
00:03:30,284 --> 00:03:34,264
But, you have to bear in mind
that you have to fit a very specific domain

58
00:03:34,464 --> 00:03:36,958
into a very generic formalism,

59
00:03:37,746 --> 00:03:40,251
and that has limitation.

60
00:03:40,539 --> 00:03:43,629
On top of that, 
you have very strong operational constraint,

61
00:03:43,892 --> 00:03:47,228
with HBP-PROV, which is the format
I have shown earlier.

62
00:03:47,681 --> 00:03:50,520
Like I said, you can have
loss of details,

63
00:03:50,917 --> 00:03:52,556
so just to give you an example:

64
00:03:53,837 --> 00:03:58,249
if you wanted to record both
electric activity of neurons

65
00:03:58,522 --> 00:04:03,642
but also morphological reconstruction
of that same neuron,

66
00:04:04,874 --> 00:04:09,395
these two kinds
of dataset have very

67
00:04:10,931 --> 00:04:14,457
specific attributes
that differentiates them.

68
00:04:14,657 --> 00:04:19,031
For instance,
an electrophysiology recording

69
00:04:19,232 --> 00:04:21,295
uses what we call
a stimuli

71
00:04:21,495 --> 00:04:24,202
which is the kind of stimulation
we apply to the neuron

72
00:04:24,402 --> 00:04:26,659
in order to observe its response.

72
00:04:27,737 --> 00:04:31,584
Which is something
a morphology reconstruction does not have.

73
00:04:32,090 --> 00:04:36,408
And so, you can not have these attributes
on the dataset per sur,

74
00:04:37,178 --> 00:04:40,660
because it only fits
some of these subclasses of dataset.

75
00:04:40,860 --> 00:04:45,504
So you will not store
this information there.

76
00:04:46,357 --> 00:04:50,355
Again, you have to compromise
on some of these details,

77
00:04:51,190 --> 00:04:55,379
and so this approach in that sense 
is not always the best.

78
00:04:57,046 --> 00:05:02,004
As the format is defined, you have to submit
the complete data model in one go,

79
00:05:02,204 --> 00:05:05,658
and so this can be also
a constraint

80
00:05:05,858 --> 00:05:09,333
for the person actually submitting
data to the data platform

81
00:05:09,533 --> 00:05:11,827
in a sense that 
today, 

82
00:05:12,027 --> 00:05:15,394
you're taking care of the specimen
you're going to work on,

83
00:05:15,594 --> 00:05:21,037
and you could already potentially
enter this information in the platform.

84
00:05:21,632 --> 00:05:24,727
And when you start preparing
the slice

85
00:05:24,927 --> 00:05:26,844
of the brain of your subject,

86
00:05:27,461 --> 00:05:31,206
you could then, at this stage,
enter the information into a platform.

87
00:05:32,557 --> 00:05:34,759
With this model,
you cannot do that

88
00:05:35,035 --> 00:05:38,387
you have to capture
the full provenance of everything you have done,

89
00:05:38,603 --> 00:05:42,174
and submit it in one go,
which has again limitations.

90
00:05:43,773 --> 00:05:49,559
Lastly, the domain as it is represented
in this data model 

91
00:05:50,767 --> 00:05:53,077
is a set in stone and

92
00:05:53,405 --> 00:06:01,119
the platform that handles it 
has a very good understanding

93
00:06:01,465 --> 00:06:02,491
of this model.

94
00:06:02,691 --> 00:06:06,677
And so if you wanted to evolve this model,
you would have to change a lot of code

95
00:06:06,877 --> 00:06:07,661
in the platform.

96
00:06:07,861 --> 00:06:11,706
And here I'm talking more about
the "behind the scene" of the data platform.

97
00:06:11,983 --> 00:06:14,424
You have people developing it,
maintaining it,

98
00:06:14,764 --> 00:06:18,592
and any evolution of this format
is very time-consuming,

99
00:06:19,489 --> 00:06:22,787
and so the scientist
is becoming very dependent 

100
00:06:23,359 --> 00:06:25,791
of the technical people
working on the platform.

101
00:06:26,357 --> 00:06:29,879
And that can slow down
progress of science.

