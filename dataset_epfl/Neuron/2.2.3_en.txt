So, in the next couple of slides, we are going to have a look at the various approaches we can use to do data integration into a KnowledgeGraph.
Here, I am going to present a first approach which I called 
"Fixed Data Model".
In this approach, we have designed a model with entities and provenance that is aimed at encompassing any used cases we meet in the field of neuroscience.
I'm going to illustrate a little bit its structure now
First, we have a specimen, and the specimen is really the subject of which we study the brain, and so that specimen can be derived into a number of samples, which can derive from each other.
So, for instance, a sample can be a specific slice of that brain into which we are going to study a number of neurons.
So, if I were to do this use-case,
I would have a specimen, then I would have a slice, and I would have right next to it a neuron.
Both of which, slice and neuron would be of type sample,
I would just have to have a property in the sample that describes that this is actually a slice, and this other one is a neuron.
Now, let's say I'm in a lab, and I want to do and electrophysiology recording of this specific neuron,
I can then design an activity with some protocol that is going to describe how this experiment is being run.
And that experiment is going to generate a dataset.
Itself, it will have a specific category that can be for instance 
"electrophysiology recording".
And this is really the entity and the metadata describing the dataset, and we have also the raw data for this dataset, as in the files that have been generated on your local hard drive when you did your recording.
And that is really what came out of it.
Another example here could have been a morphology reconstruction.
So, the scientist can, for instance, study the data that came out of this experiment, and do further annotation, or classification of this neuron here.
And for instance, define that this is a fast spiking cell, by studying the directory response of this specific neuron so the scientist can make claims, and something I haven't mentioned here, the activity obviously has a number of agents, whether it is a scientist who run the experiment or the software that was used to acquire the directory physiology recording.
So, while this approach can work, if you are fine dealing with a very generic data model, and do not want to do very deep data integration, that method can work.
But, you have to bear in mind that you have to fit a very specific domain into a very generic formalism, and that has limitation.
On top of that, you have very strong operational constraint, with HBP-PROV, which is the format
I have shown earlier.
Like I said, you can have loss of details, so just to give you an example: if you wanted to record both electric activity of neurons but also morphological reconstruction of that same neuron, these two kinds of dataset have very specific attributes that differentiates them.
For instance, an electrophysiology recording uses what we call a stimuli which is the kind of stimulation we apply to the neuron in order to observe its response.
Which is something a morphology reconstruction does not have.
And so, you can not have these attributes on the dataset per sur, because it only fits some of these subclasses of dataset.
So you will not store this information there.
Again, you have to compromise on some of these details, and so this approach in that sense is not always the best.
As the format is defined, you have to submit the complete data model in one go, and so this can be also a constraint for the person actually submitting data to the data platform in a sense that today, you're taking care of the specimen you're going to work on, and you could already potentially enter this information in the platform.
And when you start preparing the slice of the brain of your subject, you could then, at this stage, enter the information into a platform.
With this model, you cannot do that you have to capture the full provenance of everything you have done, and submit it in one go, which has again limitations.
Lastly, the domain as it is represented in this data model is a set in stone and the platform that handles it has a very good understanding of this model.
And so if you wanted to evolve this model, you would have to change a lot of code in the platform.
And here I'm talking more about the "behind the scene" of the data platform.
You have people developing it, maintaining it, and any evolution of this format is very time-consuming, and so the scientist is becoming very dependent of the technical people working on the platform.
And that can slow down progress of science.
