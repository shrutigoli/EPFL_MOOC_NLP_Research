Hello and welcome to the last lesson of this MOOC.
It is devoted to augmented reality, a computer technology that allows to superimpose a virtual model in 2 or 3 dimensions, to the perception that we have of reality and this, in real time.
The combination of augmented reality with geographic information systems offers interesting prospects and it is this technological coupling that we will present to you.
The goals of this lesson are to explain the technology of augmented reality and then to present some examples of integration of augmented reality with geographic information system applications.
After this lesson, you will be able to restore the principle of the augmented reality functioning, to explain what can the advantages of coupling be between augmented reality and geographic information system and to give a few examples of applications for which this coupling was carried out.
We are going first to explain the functioning of the augmented reality and then evoke the combination between this technology and the GIS before presenting 2 examples.
So what is the augmented reality?
This is the question we asked Jens Ingensand, professor of geoinformatics at the G2C institute of the "Haute Ecole d'Informatique et de Gestion" of the canton of Vaud located in Yverdon-les-Bains.
The augmented reality is a technology which is about superimposing virtual information to the reality.
So very often, we use devices like this one, so a tablet or also a smartphone, and these devices, they have a camera and then they have accelerometers and a GPS.
This allows to determine the exact position of the person who uses the device and then it also allows to determine what is the visible area through the camera.
As a result, with these information it is possible to superimpose spatial information through that interface.
Then we can make things visible which are otherwise not visible in the environment.
In fact, it was already in 1901 that the american writer Lyman Frank Baum, the author of the novel for children
"The magician of Oz", imagines some glasses that would allow their wearers to see appearing on the forehead of the people that they meet a letter that would correspond to their characters.
In 1960, Morton Heilig invented an immersive cinema camera called the sensorama.
The viewer is immersed in a visual, olfactory and sonorous environment associated to vibrations.
The increase in reality is made by superimposition of sensory effects that are synchronized with the image.
A little later in 1968 Intel releases the first integrated microprocessor.
Ivan Sutherland with two
Harvard students develop an image processing software which constitutes the first application of augmented reality.
Then in 1972, Myron Krueger invents the videoplace which makes interactions with virtual objects possible.
And in 1978, Steve Mann creates an electronic eye of augmented reality called
Digital Eye Glass.
And from this date, it is the technological evolution of various components that has made the augmented reality progress, the augmented reality being very demanding in terms of calculation power.
There is a clear turning point from 1980 with the release of the
Motorola 68000 32-bit processor.
The principle of the functioning was developed in 1972 by Krueger and it is schematized in this figure.
We capture an image of the reality, the images are collected in real time by an augmented reality software.
The software detects predefined catching points, here the hand, but it can be also GPS coordinates for example.
And the software selects the information associated to the catchment point, here the ball.
Next, the real image is augmented by this information then projected on a screen or on glasses or on a tablet.
And the algorithm runs in a loop in real time and in interactivity.
So the augmented reality allows to place precisely virtual objects in shots of reality.
Boeing, for example, had the idea to use this technology in its assembly lines by superimposing by augmented reality the assembly instructions directly on the pieces.
So if we consider one of the trades of geoinformatics, what interest can we find in combining augmented reality with GIS?
So we can say that GIS and augmented reality are relatively complementary, but we can also say that the augmented reality also brings another vision to the geographic information.
First of all, we can see that in a GIS, most of the time, we use the data in 2 dimensions so we create for example a two-dimensional map.
And what happens is that for the user, the user who creates the map or the person who sees the map, he has to do a recoding of the two-dimensional information to reality.
For example here we have contour lines and they are very close.
It means that we have to recode this information and then understand that as these contour lines are close, there is a steep slope, so you have to imagine that at that place, there is a certain slope.
Whilst with the augmented reality this recoding does not take place because we directly see the reality then we directly see the virtual information superimposed to the reality.
The second thing is that there is a certain democratization at the level of geographical information through augmented reality.
You have to know that GIS are very often restricted to a very targeted audience so there are few users who know how to use a GIS.
And with augmented reality, as now almost everyone has a tablet or a smartphone, we can make these geographic data accessible when they are otherwise just restricted to very targeted audience.
We are going now to present
2 examples of application which incorporate augmented reality and geographic information.
The first example is a project called biosentiers and whose aim is to raise awareness to urban biodiversity whilst encouraging the users to carry out actions in favor of the latter, one of which being the capture of new observations.
The augmented reality facilitates the collaborative aspects as we will see.
Jens Ingensand explains the details of this project.
So the biosentiers project is a project we have at the HEIG in Yverdon-les-Bains.
It is a project that aims to develop an augmented reality application.
This application will be used by students who will go to the station of Yverdon-les-Bains, they will go with their teachers from this station to the Champ-Pittet center.
This center is a nature reserve.
So these students will go to the station and then they will be able to use this augmented reality application and see species along the way, so birds, plants, trees, or butterflies for example.
Then they will be able to click on these objects and learn several things in relation to these species.
What is interesting also, they will learn which species live in the city, which species live outside the city and then they will be able to see, from the station to Champ-Pittet, an explosion in the number of species.
Close to the station, they will maybe see just a few birds, and towards the center there are hundreds and thousands of different species that will be visible.
The other example we will present is part of a didactic approach and falls into the category of what is called the tangible interfaces.
The Augmented Reality Sandbox lets students learn the basics concept in topography and generate contour lines with the aim to create topographic maps by hand.
This installation also makes it easier to learn
Earth sciences, one of which major challenges is the visualization of processes that occur on large spatial and temporal scales.
The Sandbox consists of a sandbox of 100 x 75 x 20 cm filled with 50 dm2 of fine white sand.
A Kinect for Xbox camera and a beamer are on top of the tray by 1 meter.
The size of the sandbox is limited by the minimum and maximum detection distances of the Kinect and by the desired resolution.
The ratio of 4 to 3 between the length and the width of the Sandbox corresponds to the field of vision of the Kinect and the beamer.
A Kinect is made up of a color camera and an infrared depth camera.
The depth camera uses an approach of structured light according to which a luminous pattern is sent to the sand.
Then the camera retrieves the reflected signal and can, from the deformation of the pattern sent, reconstruct the geometry of the surface.
The computer to which the kinect and the beamer are connected is powerful enough and is equipped with a powerful graphic map.
The Augmented Reality Sandbox software developed by the geology institute of the
University of Davis in California is open source and runs under Linux Mint.
The augmented reality computer device sets a closed loop consisting of the following steps.
First, the kinect receives a 480 x 640 matrix of raw distance with a frequency of 30 frames per second.
These data go into a statistical filter who has 3 goals. The first is to identify moving objects like here the hands of the user.
Next, it is to reduce the signal disturbances or the noise and ultimately it is to compensate for the missing data in the data flow of the depth of the Kinect.
The resulting topographic surface is then projected by the beamer so that it corresponds exactly to that formed in the sand.
The software uses a spectral colorimetric palette, blue for lower altitude areas to red through green and white for the highest points.
The Sandbox is a teaching aid for a variety of subjects related to the Earth sciences in general: the geology, the geomorphology, the topography, map reading on the basis of contour lines but also the watersheds and the runoff problems in hydrology.
Three typical experiments can be performed with the Sandbox. On one hand, the reconstruction of an existing relief which is based on the capacity of the device to project contour lines on the sand, secondly, the modeling of historical evolution of a landscape that exploits a relief comparison algorithm, and thirdly, watershed and runoff analysis based on a hydrological model and simulations of fluid flows.
The Sandbox also enables to develop research activities such as the integration of different modes of interaction and various levels of interactivity or the aspects of graphical semiology in the perspective of adopting, depending on the phenomena represented, color palettes that optimize the visualization and thus the understanding of the processes studied.
There you go ! We are at the end of this lesson, the last of this introduction 
MOOC to the GIS.
By way of perspectives on the modes of exploitation and representation of geographic information, we have presented you how it is possible to combine the augmented reality technology with spatial information.
This area is evolving a lot on one hand thanks to extraordinary progresses of image technologies and, on the other hand, thanks to the enormous increase in calculation capacity of computer equipments.
Geographic coordinates are used in algorithms which run in a loop to coordinate data flows between the equipments which allow to augment the reality with virtual objects.
These can appear at the right time and in the right place thanks to the location of the users who are equipped with tablets or smartphones and who move in the landscape.
We have illustrated this coupling with an application dedicated to the inventory of biodiversity and an educational aid scheme but there are many possible fields of application, mainly in the fields of air, marine, road and pedestrian navigation.
So thank you for following this online course until the end and good luck for the rest of your studies or for the implementation of this new knowledge.
