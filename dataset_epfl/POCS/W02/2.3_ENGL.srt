1
00:00:05,084 --> 00:00:07,835
Hello again, this is [Topaks]
still talking about layering,

2
00:00:07,835 --> 00:00:10,413
this time, how two layers
talk to each other.

3
00:00:10,413 --> 00:00:15,313
We're no longer looking at the broader,
bigger picture of how you compose a stack

4
00:00:15,313 --> 00:00:16,633
made up of multiple layers,

5
00:00:16,633 --> 00:00:20,007
but the very basic question of how
two layers can communicate

6
00:00:20,007 --> 00:00:21,245
with each other.

7
00:00:21,245 --> 00:00:24,034
And the general answer is,
they communicate through their API,

8
00:00:24,034 --> 00:00:24,827
their interface.

9
00:00:24,827 --> 00:00:28,161
There are actually three broad ways
in which this can be done.

10
00:00:28,641 --> 00:00:32,633
First, the two layers can share
the same address space

11
00:00:32,633 --> 00:00:34,070
and the same protection domain.

12
00:00:34,070 --> 00:00:37,478
That's the case, for example, if a layer
is a C++ class and the other layer

13
00:00:37,478 --> 00:00:40,324
is another C++ class
that depends on it.

14
00:00:40,324 --> 00:00:44,399
And in that case, the interface is merely
that of a local API.

15
00:00:45,195 --> 00:00:48,218
But sometimes the two layers
are protected from each other.

16
00:00:48,218 --> 00:00:52,197
That's specifically the case
of the <i>system call</i> interface.

17
00:00:52,197 --> 00:00:56,026
The client, which is the application,
makes system calls that execute

18
00:00:56,026 --> 00:01:00,265
in a blocking, synchronous matter
by the operating system.

19
00:01:00,265 --> 00:01:02,682
And there's a last case,
which is an interesting case,

20
00:01:02,682 --> 00:01:06,819
where the two layers communicate with
each other over some kind of a channel.

21
00:01:07,569 --> 00:01:11,189
And we refer to this
as the client/server programming model.

22
00:01:11,681 --> 00:01:15,970
And we'll see it's generally based
on Remote Procedure Calls, or RPCs.

23
00:01:15,970 --> 00:01:19,206
Now there are multiple ways in which
this channel could be implemented.

24
00:01:19,206 --> 00:01:21,661
This could be done in hardware,
it could be an I/O bus,

25
00:01:21,661 --> 00:01:24,903
it could be done over the network,
it could also simply be two different

26
00:01:24,903 --> 00:01:27,252
processes that are running
on the same computer

27
00:01:27,252 --> 00:01:31,361
but are communicating with each other
over a local socket.

28
00:01:33,700 --> 00:01:36,653
Now the client/server model
is one that enables layering,

29
00:01:36,653 --> 00:01:39,616
but also enforces modularity.

30
00:01:39,616 --> 00:01:43,376
It actually enforces differently
than when the two layers

31
00:01:43,376 --> 00:01:47,113
are part of the same address space
and communicate using a local API

32
00:01:47,113 --> 00:01:53,054
because now the modularity is enforced
and is no longer discretionary.

33
00:01:53,054 --> 00:01:56,088
Now this, in turn,
has a number of advantages.

34
00:01:56,088 --> 00:01:58,861
And specifically, you can always reduce
the client/server model

35
00:01:58,861 --> 00:02:03,886
as being that of two systems running
on two different computers

36
00:02:03,886 --> 00:02:07,952
using only messages to communicate
even though the two entities may actually

37
00:02:07,952 --> 00:02:10,548
be co-located on the same computer itself.

38
00:02:11,214 --> 00:02:15,763
Now, in this mental model it actually
enables a sweeping simplification

39
00:02:15,763 --> 00:02:17,119
in the design.

40
00:02:17,119 --> 00:02:20,775
And that is because all forms
of communication and errors

41
00:02:20,775 --> 00:02:23,824
are actually modelled as messages

42
00:02:23,824 --> 00:02:25,846
and the modules are
isolated from each other

43
00:02:25,846 --> 00:02:28,879
and so they fail independently
from each other.

44
00:02:28,879 --> 00:02:32,296
And so the use of the client/server model
even within a local computer

45
00:02:32,296 --> 00:02:36,083
actually is one that generally
enforces a good design

46
00:02:36,083 --> 00:02:39,432
because it makes it impossible,
in practice,

47
00:02:39,432 --> 00:02:41,308
to bypass any particular abstraction.

48
00:02:41,308 --> 00:02:45,448
Also makes it impossible for one module
to corrupt, inadvertantly,

49
00:02:45,448 --> 00:02:47,988
the data structures of another module.

50
00:02:48,759 --> 00:02:53,771
There's actually a trade-off in the cost
associated with this enforced modularity,

51
00:02:53,771 --> 00:02:57,941
and that is typically that the granularity
of a layer is typically much coarser

52
00:02:57,941 --> 00:03:04,475
when the modularity is enforced
than if the modularity is discretionary.

53
00:03:04,475 --> 00:03:07,861
And that is because of the overheads
of message space communication.

54
00:03:07,861 --> 00:03:12,555
And those overheads must be amortized
over a more complex computation.

55
00:03:12,555 --> 00:03:16,868
As long as you run within a single address
space, layers of abstractions are cheap,

56
00:03:16,868 --> 00:03:18,721
and they're a good programming practice.

57
00:03:18,721 --> 00:03:22,743
But if you actually need to linearize
any single communication,

58
00:03:22,743 --> 00:03:26,530
any single call over a communication
channel, then this could become

59
00:03:26,530 --> 00:03:27,894
quite expensive.

60
00:03:28,338 --> 00:03:33,446
But if the modularity is actually in force,
there are key design trade-offs.

61
00:03:33,446 --> 00:03:37,513
For example, how much client state,
or session state should be kept

62
00:03:37,513 --> 00:03:39,345
on the servers.

63
00:03:39,345 --> 00:03:42,270
This is sometimes required for performance
or for correctness,

64
00:03:42,270 --> 00:03:45,382
but it increases the complexity
of the design.

65
00:03:45,382 --> 00:03:47,499
So one of the key attributes
of the client/server model,

66
00:03:47,499 --> 00:03:49,829
unlike the shared address space model,

67
00:03:49,829 --> 00:03:53,777
is that the components may fail
independently of each other.

68
00:03:53,777 --> 00:03:55,937
In other words,
they don't share the same fate,

69
00:03:55,937 --> 00:03:59,506
and each side must determine
how to respond to the failure

70
00:03:59,506 --> 00:04:01,443
of the other side.

71
00:04:03,661 --> 00:04:08,105
Now the client/server model is typically
implemented using Remote Procedure Calls,

72
00:04:08,105 --> 00:04:12,755
and RPCs were introduced in a 1984 paper
by Birrell and Nelson.

73
00:04:13,196 --> 00:04:17,192
And the idea is to wrap both the REQUEST
and the RESPONSE messages

74
00:04:17,192 --> 00:04:22,113
to look as much as possible like the call
and return of a normal procedure call.

75
00:04:22,660 --> 00:04:25,943
And the picture here shows how
the sequence is deployed.

76
00:04:25,943 --> 00:04:29,386
First, the clients <i>marshalls</i>,
which is a technical term

77
00:04:29,386 --> 00:04:32,650
to copy all the arguments
into the message,

78
00:04:32,650 --> 00:04:36,233
and then the message itself is sent
by some underlying communication

79
00:04:36,233 --> 00:04:38,545
mechanism on the channel.

80
00:04:39,395 --> 00:04:42,481
Now there on the right, the server
processes messages that have been

81
00:04:42,481 --> 00:04:46,395
fully received, and does not process
partially received messages.

82
00:04:46,395 --> 00:04:49,814
But when a message is fully received,
it then unpacks all the arguments --

83
00:04:49,814 --> 00:04:53,837
this is the <i>unmarshalling</i> step --
and then each message comes with

84
00:04:53,837 --> 00:04:56,447
an identifier for the [ro-it]
function to call.

85
00:04:56,447 --> 00:04:59,799
And so at that point the server calls
the corresponding function

86
00:04:59,799 --> 00:05:02,404
and that is known as the <i>compute</i> step.

87
00:05:03,157 --> 00:05:06,746
And then once the compute step is done,
then it returns by sending the response

88
00:05:06,746 --> 00:05:07,763
back to the client.

89
00:05:07,763 --> 00:05:10,335
And that is similar to the request,
the arguments of the response

90
00:05:10,335 --> 00:05:13,232
are marshalled to a message,
which is sent back to the client

91
00:05:13,232 --> 00:05:14,852
over the communication channel.

92
00:05:15,397 --> 00:05:17,736
And generally,
until it receives a response,

93
00:05:17,736 --> 00:05:21,402
the client typically is simply waiting
for that response to come.

94
00:05:21,402 --> 00:05:24,596
And that would be known as the
<i>synchronous RPC</i> model.

95
00:05:24,596 --> 00:05:27,572
There's also an asynchronous variant
which is becoming more and more

96
00:05:27,572 --> 00:05:31,447
popular these days, especially for people
programming in JAVA Script.

97
00:05:32,859 --> 00:05:35,712
If you think about this model,
it's very easy to use

98
00:05:35,712 --> 00:05:39,406
because there are tools that simplify
the programming model.

99
00:05:39,406 --> 00:05:43,338
In the older days,
two or three decades ago,

100
00:05:43,338 --> 00:05:47,551
the simplification was done by stubs
that were automatically generating

101
00:05:47,551 --> 00:05:52,312
by programs, and represented
in languages such as XDR.

102
00:05:52,312 --> 00:05:57,562
So you typically had a way of expressing
RPCs using XDR that would layer

103
00:05:57,562 --> 00:06:02,576
directly on top of either UDP or TCP/IP
communication protocol.

104
00:06:03,014 --> 00:06:08,022
In the more current era, we've moved
to web services, RESTful APIs or AJAX,

105
00:06:08,022 --> 00:06:13,452
all of which are RPCs implemented
on top of HTP base protocol.

106
00:06:14,776 --> 00:06:19,178
The success of RPC is because of its
relative simplicity.

107
00:06:19,178 --> 00:06:23,380
It is designed to look as much as possible
like local procedure calls.

108
00:06:23,380 --> 00:06:26,107
However, there are obvious differences
between the two,

109
00:06:26,757 --> 00:06:30,078
and one of them is that there is no global
shared address space

110
00:06:30,078 --> 00:06:31,693
or ability to pass pointers.

111
00:06:31,693 --> 00:06:34,786
So each request must be self contained,

112
00:06:34,786 --> 00:06:38,201
the data structure must be fully
marshalled, or linearized,

113
00:06:38,201 --> 00:06:42,362
since the server has no opportunity
to call back the client

114
00:06:42,362 --> 00:06:43,717
for missing information.

115
00:06:44,029 --> 00:06:47,584
So that is one important difference.
There's a second important difference,

116
00:06:47,584 --> 00:06:50,763
which is the lack of shared fate
that I mentioned before.

117
00:06:50,763 --> 00:06:53,408
And by that I mean that either the client
or the server may fail

118
00:06:53,408 --> 00:06:57,533
at any point in time
when the other side does not fail,

119
00:06:57,533 --> 00:07:00,390
and as we will see,
this creates complications.

120
00:07:01,609 --> 00:07:04,886
Now because there's no shared fate,
any RPC may fail.

121
00:07:04,886 --> 00:07:08,421
And because of that, either the client
or the server must somehow

122
00:07:08,421 --> 00:07:12,724
decide the semantics of an RPC that fails.

123
00:07:12,724 --> 00:07:15,637
Now it turns out there's no single,
universal answer.

124
00:07:15,637 --> 00:07:19,774
Actually this is a case, or an instance of
the application of the <i>enter in</i> principle

125
00:07:19,774 --> 00:07:24,105
which you will learn about soon, in which
the application generally knows best,

126
00:07:24,105 --> 00:07:28,498
and when in doubt, leave the problem
of deciding the semantic of a failure

127
00:07:28,498 --> 00:07:32,909
to the end-to-end applications
rather than the RPC,

128
00:07:32,909 --> 00:07:35,243
which is merely the communication
subsystem.

129
00:07:35,243 --> 00:07:37,818
And because of this end-to-end principle,

130
00:07:37,818 --> 00:07:39,975
or the application of that
end-to-end principle,

131
00:07:39,975 --> 00:07:44,952
there actually are three, generally used
semantics to deal with RPC failures.

132
00:07:45,768 --> 00:07:50,069
The first one is known as
the <i>at least once</i> semantic.

133
00:07:50,069 --> 00:07:53,175
And the semantic, in this case,
is that the client will actually,

134
00:07:53,175 --> 00:07:58,365
in the case of a failure by the server,
will keep re-trying until the RPC

135
00:07:58,365 --> 00:08:00,623
is proven to be successful.

136
00:08:00,623 --> 00:08:02,839
And so this actually has consequences.

137
00:08:02,839 --> 00:08:06,563
First of all, an RPC may hang
for an indefinite amount of time.

138
00:08:06,563 --> 00:08:09,578
And also that the operations themselves
may be idempotent

139
00:08:09,578 --> 00:08:12,456
because they are going to be applied
at least once,

140
00:08:12,456 --> 00:08:14,511
but possibly multiple times.

141
00:08:15,196 --> 00:08:20,591
But yet, this is a very popular way of
expressing the semantics of RPCs.

142
00:08:20,591 --> 00:08:23,954
There's another mechanism which is
the <i>at-most once</i> semantic,

143
00:08:23,954 --> 00:08:25,803
and this is actually doing the opposite.

144
00:08:25,803 --> 00:08:29,546
The client will actually attempt at most
once to issue the RPC,

145
00:08:29,546 --> 00:08:33,106
and if for any reason something fails,
they'll simply notify the application

146
00:08:33,106 --> 00:08:34,885
of the failure of that RPC.

147
00:08:34,885 --> 00:08:38,361
That is actually the common approach
to the web today.

148
00:08:38,361 --> 00:08:42,471
An HTTP request will either succeed
at most once, or fail.

149
00:08:42,471 --> 00:08:44,895
And if it fails, then you get either
an error code

150
00:08:44,895 --> 00:08:48,926
or you get some kind of a time-out message
that you were not able to send the message

151
00:08:48,926 --> 00:08:50,109
through.

152
00:08:50,489 --> 00:08:54,000
There's also the ability to have
<i>exactly-one</i> semantics,

153
00:08:54,000 --> 00:08:56,789
but this actually cannot be done
in a universal way

154
00:08:56,789 --> 00:09:01,389
by the communication subsystem itself,
rather, it actually requires that the clients

155
00:09:01,389 --> 00:09:04,522
and the servers keep some kind
of a persistent state

156
00:09:04,522 --> 00:09:08,311
that is coordinated between the two so
that they know how far along they've gone

157
00:09:08,311 --> 00:09:14,272
in their exchanges and so it can guarantee
that every RPC is delivered exactly once

158
00:09:14,272 --> 00:09:15,502
to the client.

159
00:09:16,615 --> 00:09:19,754
So let's wrap this module with an example
that puts it all together.

160
00:09:19,754 --> 00:09:25,809
And the example that I chose is NFS.
And the NFS paper is simply brilliant

161
00:09:25,809 --> 00:09:31,254
because it has the elegance of the earlier
era of papers in computer systems.

162
00:09:31,889 --> 00:09:34,904
Now what's important to [ ] put into
perspective is at the time

163
00:09:34,904 --> 00:09:39,021
NFS was by far not the only proposal
for distributed filesystems.

164
00:09:39,021 --> 00:09:41,977
As a matter of fact, there were many
other proposals in flight,

165
00:09:41,977 --> 00:09:46,045
many of which were more complex,
had more features than NFS itself.

166
00:09:46,045 --> 00:09:49,108
But one of the reasons why NFS won
is because it was the simplest

167
00:09:49,108 --> 00:09:52,952
possible way in which you can design
a network filesystem protocol.

168
00:09:52,952 --> 00:09:54,582
And it was simple in multiple ways.

169
00:09:54,582 --> 00:09:56,885
First of all, it was a very
clean RPC design.

170
00:09:56,885 --> 00:09:59,058
You had requests and responses.

171
00:09:59,058 --> 00:10:02,283
They developed XTR as part of this
in order to simplify the description

172
00:10:02,283 --> 00:10:04,307
of the messages themselves.

173
00:10:04,307 --> 00:10:07,443
And then the protocol allowed for a
<i>stateless</i> server design

174
00:10:07,443 --> 00:10:11,403
which means there was absolutely no
per client-specific state that was kept

175
00:10:11,403 --> 00:10:13,023
on an NFS server.

176
00:10:13,363 --> 00:10:16,429
They also simplified the communication
relationship so the clients

177
00:10:16,429 --> 00:10:17,926
never had to talk to each other.

178
00:10:17,926 --> 00:10:21,563
And servers managing different filesystems
also never had to talk to each other.

179
00:10:21,563 --> 00:10:24,645
Different clients only talked to their
respective NFS servers.

180
00:10:24,645 --> 00:10:26,575
And that also eliminated
many of the issues

181
00:10:26,575 --> 00:10:30,288
associated with coherency that were
present in other filesystems

182
00:10:30,288 --> 00:10:33,953
simply because NFS didn't provide
as strong coherency semantics,

183
00:10:33,953 --> 00:10:37,834
but did not attempt to provide as strong
coherency semantics as other filesystems.

184
00:10:38,444 --> 00:10:41,046
And then finally it was brilliantly
simple in its use

185
00:10:41,046 --> 00:10:43,929
of the <i>at least once</i> semantics
for RPC failures.

186
00:10:43,929 --> 00:10:46,925
The server was down;
the client would just keep trying.

187
00:10:46,925 --> 00:10:49,107
And so at the time you had these
<i>server down</i> messages

188
00:10:49,107 --> 00:10:53,346
that would pop up because some of these
NFS servers were not that reliable.

189
00:10:53,346 --> 00:10:56,994
But it made for a very simple design
because it actually maintained,

190
00:10:56,994 --> 00:11:02,602
allowed the operating systems running
on the clients to actually thing of an NFS

191
00:11:02,602 --> 00:11:05,319
filesystem just like they would think
of a local filesystem.

192
00:11:05,319 --> 00:11:08,643
And that's the last point that was
indicative of the success of NFS

193
00:11:08,643 --> 00:11:12,019
which is it was well integrated into
an existing filesystem,

194
00:11:12,019 --> 00:11:15,873
an existing operating system
using the VFS layer

195
00:11:15,873 --> 00:11:19,231
that is internal to the operating systems
themselves which we covered

196
00:11:19,231 --> 00:11:20,888
in the previous module.

197
00:11:20,888 --> 00:11:23,176
So in many ways, NFS
is a brilliant example

198
00:11:23,176 --> 00:11:25,605
of a very simple,
minimalistic design.

199
00:11:25,605 --> 00:11:29,245
HTTP is another example
of a minimalistic design

200
00:11:29,245 --> 00:11:31,860
that implements RPCs today.

201
00:11:33,946 --> 00:11:38,153
So if you actually want to look at what
NFS looks like in greater detail,

202
00:11:38,153 --> 00:11:41,193
this is a picture that I took
from the original paper.

203
00:11:41,193 --> 00:11:44,613
Note that I actually did not
tilt the picture myself.

204
00:11:44,613 --> 00:11:49,437
But instead this paper sold that the only
copies available are some scans

205
00:11:49,437 --> 00:11:54,565
of a not-so-aligned version of the paper
that was printed a few decades ago.

206
00:11:54,565 --> 00:11:57,200
But this picture actually allows us
to review many of the concepts

207
00:11:57,200 --> 00:11:58,984
that we've seen so far.

208
00:11:58,984 --> 00:12:03,700
We see the distinct layering of <i>system
call</i> layer and the VFS layer within UNIX.

209
00:12:03,700 --> 00:12:09,406
This is a form of <i>soft</i> modularity
and layering within the operating system.

210
00:12:09,406 --> 00:12:12,913
We see the VFS layers' ability to
virtualize different filesystem drivers

211
00:12:12,913 --> 00:12:17,866
below a common interface, and to create
a unified namespace

212
00:12:17,866 --> 00:12:22,364
that captures multiple files coming from
different filesystems of different types.

213
00:12:23,394 --> 00:12:27,528
Third, we see the layering of the NFS
on top of RPC and XDR.

214
00:12:27,528 --> 00:12:31,055
That's the ENT N layer of the networking
stack that provides the communication

215
00:12:31,055 --> 00:12:32,219
mechanism.

216
00:12:32,219 --> 00:12:34,808
And fourth, we see the use
of the network layer below

217
00:12:34,808 --> 00:12:36,627
to connect the client and the server.

218
00:12:36,627 --> 00:12:40,737
The link is not shown but that is expected
since the design only relies

219
00:12:40,737 --> 00:12:44,034
on the Network layer
as its lowest layer.

220
00:12:44,034 --> 00:12:45,410
And that wraps up this module.

221
00:12:45,410 --> 00:12:49,742
Next, we'll actually go into the next
level of detail in the networking stack

222
00:12:49,742 --> 00:12:54,189
and talk about the different layers
in networking itself.
