1
00:00:04,052 --> 00:00:08,460
So, to explain the distinction
between compilation and interpretation,

2
00:00:08,840 --> 00:00:13,944
this is a bit difficult and artificial
because the distinction has been changing,

3
00:00:13,944 --> 00:00:16,025
and, things have been converging
with time.

4
00:00:16,025 --> 00:00:17,342
But from a certain viewpoint,

5
00:00:17,342 --> 00:00:20,877
I could say compilation
is bulk translation between languages

6
00:00:21,157 --> 00:00:22,556
and execution at the end.

7
00:00:22,556 --> 00:00:25,689
So, that means I statically combine once
and then I might, as a whole,

8
00:00:25,689 --> 00:00:28,067
execute in that translated form already.

9
00:00:28,432 --> 00:00:30,335
A compiler from A to B,

10
00:00:30,335 --> 00:00:32,453
will take an input in language A,

11
00:00:32,453 --> 00:00:34,048
produce an output in language B

12
00:00:34,048 --> 00:00:37,295
and then we can execute that program
in language B whenever we want,

13
00:00:37,295 --> 00:00:38,627
as many times as we like.

14
00:00:38,627 --> 00:00:39,751
And interpretation,

15
00:00:39,751 --> 00:00:43,687
that's the kind of translation on the fly,
we do the translation transaction

16
00:00:43,687 --> 00:00:45,159
operation by operation,

17
00:00:45,159 --> 00:00:46,808
or statement by statement
of the language

18
00:00:46,808 --> 00:00:48,626
then immediately execute it,
so to say.

19
00:00:48,626 --> 00:00:51,744
It's one viewpoint, of course,
you can look at it from a different angle.

20
00:00:53,906 --> 00:00:57,364
In the past, there was a great distinction
between compilers and interpreters,

21
00:00:57,364 --> 00:01:00,348
but recently we have seen these kind
of staged ecosystems,

22
00:01:00,348 --> 00:01:02,774
where there are all kinds
of hybrids happening

23
00:01:02,774 --> 00:01:05,796
and multiple stages of compilation
and interpretation happen.

24
00:01:05,796 --> 00:01:07,858
So, this, for example, has become
a lot more subtle.

25
00:01:07,858 --> 00:01:11,243
Also, because there might be all kinds
of levels of virtualization

26
00:01:11,243 --> 00:01:12,779
on all kinds of levels,

27
00:01:12,779 --> 00:01:16,229
that might be akin 
to interpretation or not.

28
00:01:17,542 --> 00:01:20,781
The staging that is just-in-time
compiles special occasions

29
00:01:20,781 --> 00:01:22,758
on (inaudible) distinction.

30
00:01:22,758 --> 00:01:24,907
So, there are a number of terms
that are related.

31
00:01:24,907 --> 00:01:27,920
Simulation, interpretation, emulation
and virtualization.

32
00:01:28,200 --> 00:01:32,105
We've heard discussion of emulation
and virtualization, right?

33
00:01:32,230 --> 00:01:34,644
Emulation is one of the tasks
maybe also virtualization,

34
00:01:34,644 --> 00:01:37,116
emulation is of existing real hardware.

35
00:01:37,116 --> 00:01:39,679
While you could say interpretation
is for programs.

36
00:01:40,169 --> 00:01:42,405
Simulation essentially captures all of it.

37
00:01:42,405 --> 00:01:45,823
Simulation is typically used, for example,
simulating the physical world

38
00:01:45,823 --> 00:01:47,800
a biological system
or things like that.

39
00:01:48,500 --> 00:01:52,300
So, all these terms
it makes sense to talk about.

40
00:01:52,507 --> 00:01:57,535
So, we've seen in the previous module
about and heard about virtual machines

41
00:01:57,535 --> 00:01:58,540
and virtualization,

42
00:01:58,540 --> 00:02:00,760
there are, of course,
language virtual machines

43
00:02:00,760 --> 00:02:03,163
specifically like the famous
Java virtual machine.

44
00:02:03,163 --> 00:02:06,468
And they have a somewhat different
purpose from the typical purposes

45
00:02:06,468 --> 00:02:09,128
of virtualization infrastructure.

46
00:02:10,901 --> 00:02:12,400
They're primarily in place
to support things

47
00:02:12,400 --> 00:02:15,981
like late binding and late compilation:
adaptive computation.

48
00:02:16,828 --> 00:02:18,900
And to bring forth portability of code.

49
00:02:19,237 --> 00:02:22,133
For example Java bytecode
is a language

50
00:02:22,133 --> 00:02:25,600
for which there are virtual machines
that can execute this Java bytecode

51
00:02:25,600 --> 00:02:27,368
on many different architectures.

52
00:02:27,368 --> 00:02:30,312
So, we can use the same kind
binary codes, or pre-compiled codes,

53
00:02:30,792 --> 00:02:33,010
loaded code on many
different architectures,

54
00:02:33,010 --> 00:02:34,484
so it's very portable.

55
00:02:34,484 --> 00:02:36,224
And that's the goal
of this virtual machine.

56
00:02:36,224 --> 00:02:39,135
And it's less to virtualize resources.

57
00:02:39,421 --> 00:02:41,560
It's less the task of a language 
virtual machine.

58
00:02:41,560 --> 00:02:45,594
So, language centric VMs also sometimes
don't really feel like machines.

59
00:02:45,798 --> 00:02:48,534
Well, you could argue that 
a virtual machine for Java bytecode

60
00:02:48,534 --> 00:02:50,760
does feel like such a virtual machine,

61
00:02:50,760 --> 00:02:52,947
but there's also, for example, 
relational database managers

62
00:02:52,947 --> 00:02:54,091
and career engines,

63
00:02:54,091 --> 00:02:57,275
that you can think of as relational
algebra virtual machines, right?

64
00:02:57,275 --> 00:02:59,387
Or that (inaudible)

65
00:02:59,387 --> 00:03:02,658
some of them are very language-centric,
and these languages

66
00:03:02,658 --> 00:03:07,359
are distinct from what you would think of
as something like a CPU,

67
00:03:07,359 --> 00:03:11,770
so that it's reasonable to think of
it in different ways.

68
00:03:15,307 --> 00:03:17,367
Let's talk about a few principles
of the context of interpretation

69
00:03:17,367 --> 00:03:19,038
as a goal itself.

70
00:03:19,353 --> 00:03:21,459
And one is, you know,
we want to interpret

71
00:03:21,459 --> 00:03:23,983
to realize additional levels
of indirection.

72
00:03:24,505 --> 00:03:28,690
So, we can use a virtual machine
or interpret a high-level language

73
00:03:28,690 --> 00:03:31,379
to be machine-independent,
as we said, portability.

74
00:03:32,060 --> 00:03:34,224
The system becomes portable
and can be separated

75
00:03:34,224 --> 00:03:36,169
from legacy system components.

76
00:03:37,319 --> 00:03:39,756
And Java Virtual Machine is an example.

77
00:03:40,013 --> 00:03:43,801
When we are inlining a compilation result
is too big or too costly

78
00:03:43,801 --> 00:03:45,434
to build in hardware.

79
00:03:46,244 --> 00:03:48,402
An example would be microcode,

80
00:03:48,402 --> 00:03:50,156
to delay a system's becoming legacy.

81
00:03:50,156 --> 00:03:52,062
So, we want to support scripting, right?

82
00:03:52,062 --> 00:03:54,705
On one hand you could write
a fixed, monolithic system,

83
00:03:55,345 --> 00:03:57,002
adjust that a certain drop.

84
00:03:57,432 --> 00:03:58,773
Or we could make it scriptable,

85
00:03:58,773 --> 00:04:00,954
so that it accepts some kind
of permit authorization.

86
00:04:01,564 --> 00:04:03,885
Permit authorization could be 
flexible enough that you can

87
00:04:03,885 --> 00:04:06,512
think of it as programmable,
first the parameter file,

88
00:04:06,512 --> 00:04:10,216
but there is some kind of scripting way
to extend the modifier

89
00:04:10,216 --> 00:04:11,629
to keep the system fresh.

90
00:04:12,350 --> 00:04:14,215
So, that's a way of really fighting legacy

91
00:04:14,215 --> 00:04:16,011
in the sense that you could argue
that any system

92
00:04:16,011 --> 00:04:18,570
is not scripting at all,
is already a legacy system.

93
00:04:21,401 --> 00:04:23,265
Of course there are
many examples of scripting.

94
00:04:23,265 --> 00:04:24,293
We've seen many cases

95
00:04:24,293 --> 00:04:26,339
and, of course, you can think
of scripting languages

96
00:04:26,339 --> 00:04:28,445
as domain-specific languages, right,

97
00:04:28,445 --> 00:04:30,652
and often enough they are easier to use.

98
00:04:30,652 --> 00:04:33,774
That's why, for example, many first year
programming courses

99
00:04:33,774 --> 00:04:35,452
nowadays in the US
use Python,

100
00:04:36,192 --> 00:04:38,026
because you don't have to worry
about typing any more,

101
00:04:38,026 --> 00:04:41,124
it's relatively easy to use and so on,
it's quite robust and so on.

102
00:04:43,424 --> 00:04:46,662
Of course there are things
like the UNIX shell and command line tool,

103
00:04:46,662 --> 00:04:49,250
which is really getting 
scripting capabilities

104
00:04:49,250 --> 00:04:51,828
in a sense to an operating system.

105
00:04:52,598 --> 00:04:55,718
Or in database query processing,
is a set sequel engine.

106
00:04:56,822 --> 00:04:58,801
Or that's maybe contestable,

107
00:04:58,801 --> 00:05:01,710
but I could argue that maybe open floor
and software-defined networking

108
00:05:01,710 --> 00:05:03,354
are a bit like this.

109
00:05:04,049 --> 00:05:06,490
Particularly if it's flexible enough
that you can really script them,

110
00:05:06,490 --> 00:05:08,090
it's not just some in parameters.

111
00:05:09,630 --> 00:05:13,014
Another principle I already mentioned
in some ways, is staged compilation.

112
00:05:13,014 --> 00:05:16,660
Once you have the ability to interpret
and make your way

113
00:05:16,697 --> 00:05:20,407
from a program in a high-level language,
let's say, which you have programmed

114
00:05:20,407 --> 00:05:22,951
to the machine code that is executing,

115
00:05:22,951 --> 00:05:26,115
if you can make it multiple stages,
you have additional potential,

116
00:05:26,115 --> 00:05:28,809
for example, for being adaptive
and being more efficient.

117
00:05:28,881 --> 00:05:30,614
Classically, we had the choice maybe

118
00:05:30,614 --> 00:05:33,202
to take either this kind of high-level
host language program

119
00:05:33,202 --> 00:05:35,690
and run the interpreter
instruction by instruction,

120
00:05:36,140 --> 00:05:38,798
apply its kind of template translation
and execute that.

121
00:05:39,225 --> 00:05:41,117
Or a compiler could statically

122
00:05:41,387 --> 00:05:43,400
take the input code
and produce machine code.

123
00:05:44,200 --> 00:05:46,633
And there were things like Java,
where you go write Java bytecode,

124
00:05:46,633 --> 00:05:50,899
you compile, statically, offline
to Java bytecode,

125
00:05:50,899 --> 00:05:52,496
and that's what you ship
and that's what people see

126
00:05:52,496 --> 00:05:56,118
and there's a lower virtual machine
that is rather efficient already.

127
00:05:56,118 --> 00:05:58,693
But that machine might have behaved
like an interpreter

128
00:05:58,693 --> 00:06:00,884
and more recently there was actually
a just-in-time compilation,

129
00:06:00,884 --> 00:06:02,122
JITing happening.

130
00:06:02,122 --> 00:06:05,262
Rather than taking the bytecode
and executing it instruction by instruction

131
00:06:05,262 --> 00:06:07,032
like a machine,

132
00:06:07,032 --> 00:06:09,075
we would still have
a low-level compilation

133
00:06:09,075 --> 00:06:10,491
to real machine code

134
00:06:10,640 --> 00:06:14,033
and it's done once and for example
you compile the loop once

135
00:06:14,033 --> 00:06:16,931
rather than translate it every time 
you execute for its instructions.

136
00:06:17,792 --> 00:06:19,680
And there's of course potential there,

137
00:06:19,680 --> 00:06:22,940
particularly since at that point in time
you might actually know quite a lot

138
00:06:22,940 --> 00:06:24,305
about your data,

139
00:06:24,305 --> 00:06:26,956
and might use this to further
do some loop unrolling,

140
00:06:26,956 --> 00:06:30,862
or fusions and so on, inlining,
to actually make things more efficient.

141
00:06:31,408 --> 00:06:33,666
On the other hand, concretely
for Java bytecode,

142
00:06:33,949 --> 00:06:35,196
this actually doesn't work so well,

143
00:06:35,196 --> 00:06:37,653
because Java bytecode is missing
type information

144
00:06:37,653 --> 00:06:40,681
that would actually be very important
to do useful optimization

145
00:06:40,681 --> 00:06:44,440
actually right now they're working 
on changing the standards here,

146
00:06:44,440 --> 00:06:48,050
to actually make more optimization happen
by having a typed version

147
00:06:48,050 --> 00:06:49,710
of intermediate language.

148
00:06:50,154 --> 00:06:54,852
So, of course, these kind of managed codes
that are run in a virtual machine,

149
00:06:55,502 --> 00:06:58,732
can buffer for many kinds of protections,
memory protections,

150
00:06:59,222 --> 00:07:01,179
garbage-collected memory 
so that you don't have to worry

151
00:07:01,179 --> 00:07:02,776
about memory leaks in a sense.

152
00:07:02,776 --> 00:07:05,143
Memory protections, in a sense,
that you can't do overflows

153
00:07:05,143 --> 00:07:07,576
or writing of protected memory
and so on.

154
00:07:08,056 --> 00:07:10,939
Catch alloc-dealloc bugs and so on.

155
00:07:10,939 --> 00:07:13,444
Array bound checking and so on.
Things like that.

156
00:07:13,444 --> 00:07:16,200
So, you can get additional protection
and avoid certain bugs,

157
00:07:16,200 --> 00:07:22,396
or at least damages resulting 
from the bugs by such a system.

158
00:07:22,638 --> 00:07:25,473
It does some debugging for you
so to say.

159
00:07:26,603 --> 00:07:29,984
Now, some examples of such staging.

160
00:07:29,984 --> 00:07:32,752
As we've already said, Java VM,
particularly Java Hotspot.

161
00:07:32,752 --> 00:07:36,360
It's a really now supporting a compilation
a static compilation

162
00:07:36,360 --> 00:07:39,883
and a virtual machine that does JITing
and adaptive optimizations in there.

163
00:07:40,185 --> 00:07:41,116
It's hard.

164
00:07:41,116 --> 00:07:45,135
LLVM is not a very famous framework
that is based on the C language,

165
00:07:45,135 --> 00:07:47,214
and creating a new C compiler,

166
00:07:47,214 --> 00:07:50,951
that actually basically compiles C code
in an intermediate, low-level presentation

167
00:07:51,461 --> 00:07:53,262
in static single-assignment form.

168
00:07:53,262 --> 00:07:55,858
Something, again, like a bytecode
representation,

169
00:07:55,858 --> 00:07:58,182
in which there are actually tools

170
00:07:58,182 --> 00:08:00,416
for doing all kind of analysis
and optimizations,

171
00:08:00,416 --> 00:08:04,092
and also just-in-time compilation
to run the thing.

172
00:08:06,174 --> 00:08:07,876
LMS, lightweight modular staging,

173
00:08:07,876 --> 00:08:09,801
is something that has been developed
(inaudible)

174
00:08:09,801 --> 00:08:11,984
and lives in the Scala ecosystem,
the Java ecosystem.

175
00:08:11,984 --> 00:08:13,736
It's again staged compilation,

176
00:08:13,736 --> 00:08:15,786
where you can basically lift
any Scala code,

177
00:08:15,786 --> 00:08:17,237
you can easily write DSLs,

178
00:08:17,237 --> 00:08:19,422
but basically making a superset
of the subset of Scala,

179
00:08:19,422 --> 00:08:22,632
and adding new operations
or new library functions etc.

180
00:08:22,682 --> 00:08:24,390
And you can stage compilation.

181
00:08:25,310 --> 00:08:27,492
Every piece of code can be lifted
during presentation

182
00:08:27,492 --> 00:08:28,558
and then you can have,

183
00:08:28,558 --> 00:08:31,299
multiple stages of translation,
optimization, transformation

184
00:08:31,299 --> 00:08:33,529
that might go from a lifted version 
to a lift first,

185
00:08:33,529 --> 00:08:35,531
so you can use
as many steps as you like

186
00:08:35,531 --> 00:08:38,795
and at some point you emit code,
or you actually execute it.

187
00:08:38,795 --> 00:08:41,502
So, you can build all kinds of multi-level
just-in-time compilers

188
00:08:41,502 --> 00:08:43,416
and things like that
if you like.

189
00:08:45,516 --> 00:08:47,356
So, there is this myth

190
00:08:47,356 --> 00:08:49,410
that high-level languages
are necessarily less efficient.

191
00:08:49,410 --> 00:08:50,758
If you write something in C,

192
00:08:50,758 --> 00:08:52,594
it will be always faster
if you do a good job

193
00:08:52,594 --> 00:08:54,208
and if you are competent,

194
00:08:54,208 --> 00:08:56,960
than writing it, let's say,
in Java or Scala.

195
00:08:57,243 --> 00:09:00,739
Although to some extent this is true
if you do a perfectly competent job,

196
00:09:00,739 --> 00:09:03,022
but we have already seen,
for example, in register allocation

197
00:09:03,022 --> 00:09:06,670
that a C compiler nowadays
outperforms even the best humans.

198
00:09:06,792 --> 00:09:09,597
So, we won't even try to write
assembly code any more,

199
00:09:09,597 --> 00:09:13,480
because in the time of your multi-core
super Scala, superpipeline processors,

200
00:09:13,480 --> 00:09:16,264
it's actually, too hard for a human
to do this in practice.

201
00:09:16,492 --> 00:09:19,371
It's impractical; C compilers 
beat the humans nowadays.

202
00:09:19,760 --> 00:09:23,102
And we're getting to do this somehow, slowly,
all the formal high-level language

203
00:09:23,102 --> 00:09:26,020
and high-level aspects of compilation.

204
00:09:26,020 --> 00:09:28,340
So, it's not necessary that we're slow.

205
00:09:28,340 --> 00:09:32,236
For example, if you have heavy
allocation/deallocation of small objects,

206
00:09:32,236 --> 00:09:34,511
continuously changing that,

207
00:09:34,511 --> 00:09:37,132
and every time, for example, in C
you go through malloc,

208
00:09:37,132 --> 00:09:40,290
which has huge overheads as a system code
to the writing system and so on,

209
00:09:40,290 --> 00:09:41,723
lots of things happen there.

210
00:09:41,723 --> 00:09:44,818
This can actually be much slower
than if you actually in bulk

211
00:09:45,998 --> 00:09:49,573
allocate a big region of space
and then have your own management.

212
00:09:49,573 --> 00:09:52,713
As, for example, the Java
virtual machine does it for you.

213
00:09:52,713 --> 00:09:56,875
If you, for example, write heavily dynamic
data structure transformations

214
00:09:56,875 --> 00:09:58,680
and so on in C++,

215
00:09:58,680 --> 00:10:01,260
you won't actually be beaten
by Java off the shelf.

216
00:10:02,947 --> 00:10:04,925
Another thing is that
the more high-level you are,

217
00:10:04,925 --> 00:10:08,480
the more time and cycles you have left,
because you spend less time coding,

218
00:10:08,480 --> 00:10:09,673
to actually improve

219
00:10:09,673 --> 00:10:12,087
and add more intelligence
into your code, so to say

220
00:10:12,087 --> 00:10:13,989
which then your system
can profit from.

221
00:10:13,989 --> 00:10:15,407
A classical example here,

222
00:10:15,407 --> 00:10:17,840
is the Singularity operating system,
from Microsoft,

223
00:10:17,840 --> 00:10:20,321
which is an operating system,
written completely

224
00:10:20,321 --> 00:10:22,540
in a high-level language,
actually a fragment--

225
00:10:22,540 --> 00:10:24,594
a dialect of C#,

226
00:10:25,444 --> 00:10:27,872
and that runs in a virtual machine
in the CLI,

227
00:10:27,872 --> 00:10:29,699
the Common Language Interface.

228
00:10:32,579 --> 00:10:34,732
And they have done all kinds
of interesting optimizations

229
00:10:34,732 --> 00:10:38,007
and, actually, this operating system
beats classical operating systems,

230
00:10:38,007 --> 00:10:41,749
even developed, implemented 
like Linux, very substantially.

231
00:10:41,749 --> 00:10:43,689
For example, process overheads.

232
00:10:44,441 --> 00:10:47,575
Once you start implementing
your applications

233
00:10:47,575 --> 00:10:50,675
and even in the operating system itself,
it is high-level language,

234
00:10:50,675 --> 00:10:52,823
which is done in Singularity,

235
00:10:52,823 --> 00:10:56,162
you've got the potential
for static analysis on this high-level,

236
00:10:56,162 --> 00:10:58,553
for example, for merging processes
that don't need to be separate.

237
00:10:58,553 --> 00:11:00,433
That you don't put into the process 
switching all the time

238
00:11:00,433 --> 00:11:01,867
in the operating system.

239
00:11:01,867 --> 00:11:04,245
One thing it does
is called software isolated processes,

240
00:11:04,245 --> 00:11:06,931
which are not isolated processes
at (inaudible) any more,

241
00:11:06,931 --> 00:11:09,742
but it's compiled and you verify
that nothing bad can happen,

242
00:11:09,742 --> 00:11:12,039
and then you don't have to do
so much process switching between them,

243
00:11:12,039 --> 00:11:13,949
which is actually very expensive.

244
00:11:13,949 --> 00:11:17,565
Switching the context in and out
and so on, for an operating system.

245
00:11:17,565 --> 00:11:21,392
You can actually get better performance
in a high-level language

246
00:11:21,392 --> 00:11:24,708
because of modern technology,
modern compilers and language technology

247
00:11:24,708 --> 00:11:27,871
and because we've got more cycles
for doing intelligent optimizations,

248
00:11:27,871 --> 00:11:30,074
because it's less code to write.
