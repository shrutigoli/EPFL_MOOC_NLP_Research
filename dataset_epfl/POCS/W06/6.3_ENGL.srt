1
00:00:04,028 --> 00:00:06,471
Still talking about redundancy 
and fault tolerance,

2
00:00:06,471 --> 00:00:10,380
but this time moving away from the 
principle discussion to that of metrics.

3
00:00:10,599 --> 00:00:13,465
It's important to first have a 
good handle on the principles

4
00:00:13,677 --> 00:00:16,713
so that you have a mental map 
of the concepts that are in play,

5
00:00:17,148 --> 00:00:20,265
but then you need to have an equally 
strong handle on the metrics

6
00:00:20,265 --> 00:00:23,774
in order to quantitatively 
evaluate the system at hand.

7
00:00:24,411 --> 00:00:26,806
And in the case of redundancy 
and fault tolerance,

8
00:00:26,806 --> 00:00:30,254
the principle metric that we talk about 
is availability.

9
00:00:30,651 --> 00:00:33,533
Now since fault tolerance is about 
building reliable systems

10
00:00:33,533 --> 00:00:35,331
from unreliable components,

11
00:00:36,371 --> 00:00:39,034
we need to have those 
metrics of unreliability.

12
00:00:39,965 --> 00:00:42,282
And those metrics 
apply at all layers,

13
00:00:42,286 --> 00:00:46,444
the components or the faults 
may be frequent and the availability poor,

14
00:00:46,444 --> 00:00:48,492
and the fault tolerance systems 
themselves

15
00:00:48,492 --> 00:00:50,518
which hopefully show better metric.

16
00:00:52,168 --> 00:00:55,270
Now this is important first a metric 
is availability itself.

17
00:00:55,270 --> 00:00:59,802
The second key concept is that 
of availabilities complement,

18
00:00:59,802 --> 00:01:01,660
and that is called the downtime.

19
00:01:02,466 --> 00:01:06,466
Now the downtime is the ratio between 
two other metrics of a system.

20
00:01:06,466 --> 00:01:09,624
The denominator is the mean time 
in between failures

21
00:01:10,627 --> 00:01:13,537
and for the purpose of the fault time 
and design methodology,

22
00:01:13,537 --> 00:01:16,515
you first want to improve the mean time 
between failures.

23
00:01:17,346 --> 00:01:19,480
For example, 
redundant power supplies

24
00:01:19,480 --> 00:01:23,574
allow you to take the MTBF 
of the power supply module

25
00:01:23,574 --> 00:01:28,299
from being a significant source 
of faults in a computer system,

26
00:01:28,299 --> 00:01:32,433
to a completely insignificant 
source of faults in a computer system.

27
00:01:33,318 --> 00:01:37,251
But improving the MTBF alone 
is not sufficient to ensure availability.

28
00:01:37,749 --> 00:01:40,406
Rather, one must also 
consider the time it takes

29
00:01:40,406 --> 00:01:43,516
to recover from an unrecoverable fault.

30
00:01:44,133 --> 00:01:47,814
And that is where the 
mean time to repair or MTTR

31
00:01:47,814 --> 00:01:50,397
comes in as the nominator 
in this equation.

32
00:01:51,252 --> 00:01:53,402
Now the MTTR varies widely

33
00:01:53,402 --> 00:01:56,498
and is generally not only 
a function of technology.

34
00:01:57,253 --> 00:02:01,316
If your cellphone goes under a truck, 
the MTTR is the time it takes you

35
00:02:01,316 --> 00:02:05,427
to go to Swisscom or Orange at the 
store and get another cellphone.

36
00:02:06,119 --> 00:02:09,740
Now if your cellphone unfortunately 
goes under a truck on a Saturday night,

37
00:02:09,740 --> 00:02:15,185
your MTTR is 36 hours, since the stores 
are closed on Sundays in Switzerland.

38
00:02:15,557 --> 00:02:18,128
Now if your SIM card somehow 
survives the catastrophe,

39
00:02:18,128 --> 00:02:21,434
and you have an older phone at home, 
then your MTTR might be shorter.

40
00:02:21,921 --> 00:02:23,480
And here in this case,

41
00:02:23,480 --> 00:02:27,359
by now the old phone 
plays the role of the spare,

42
00:02:27,359 --> 00:02:30,080
and as we'll see in systems 
a notion of having a spare

43
00:02:30,080 --> 00:02:34,896
and even a hot spare is critical 
to reducing the MTTR.

44
00:02:36,202 --> 00:02:39,266
Now of course when you're talking about 
metrics of availability,

45
00:02:39,266 --> 00:02:40,877
you have to talk about the nines.

46
00:02:41,124 --> 00:02:43,207
3 nines, 4 nines, 
5 nines, 6 nines,

47
00:02:43,207 --> 00:02:45,057
I'm sure you've heard about the nines.

48
00:02:45,383 --> 00:02:48,770
So let's first define what this means 
and later what this implies.

49
00:02:49,010 --> 00:02:53,409
Take 5 nines, often desired as the 
target operation level

50
00:02:53,409 --> 00:02:54,743
for critical services.

51
00:02:55,506 --> 00:02:57,910
Now what 5 nines of availability means,

52
00:02:58,380 --> 00:03:00,484
it means that on average,

53
00:03:00,484 --> 00:03:04,909
you can allow for either 
five minutes of downtime a year

54
00:03:05,159 --> 00:03:08,514
or one hour of downtime per decade.

55
00:03:08,718 --> 00:03:12,517
Well, that's not a lot of room when you 
put all the assumptions on the table,

56
00:03:13,142 --> 00:03:16,329
and of course all assumptions 
must be put on the table,

57
00:03:16,477 --> 00:03:18,915
dealing with the failures in 
hardware and software,

58
00:03:18,915 --> 00:03:20,992
but also failures in the environment,

59
00:03:20,992 --> 00:03:23,406
even things such as software upgrades

60
00:03:23,406 --> 00:03:25,989
that are expected 
during the lifetime of the product.

61
00:03:27,247 --> 00:03:29,554
So as a matter of fact, 
for server applications,

62
00:03:29,554 --> 00:03:33,067
5 nines of availability generally 
requires that you have a solution

63
00:03:33,067 --> 00:03:35,933
that straddles multiple, 
physical data centers

64
00:03:35,933 --> 00:03:38,327
separated by substantial distance.

65
00:03:40,444 --> 00:03:43,588
On top of that, you need to monitor 
the per of those systems

66
00:03:43,588 --> 00:03:46,374
through strict procedures 
and human processes.

67
00:03:47,123 --> 00:03:50,025
And the traditional assumption, 
was that such a solution

68
00:03:50,025 --> 00:03:53,289
had to to be built exclusively 
out of enterprise grade components,

69
00:03:53,596 --> 00:03:55,644
although this assumption 
is being revisited

70
00:03:55,644 --> 00:03:58,390
with the advent of cloud based 
application paradigms.

71
00:03:59,637 --> 00:04:02,587
It's also important to think about 
the time dimension here.

72
00:04:02,587 --> 00:04:06,653
A decade is a reasonable length of time 
to bound the useful life

73
00:04:06,653 --> 00:04:08,964
of a mission critical application.

74
00:04:09,707 --> 00:04:13,131
And so it makes sense to think about 
the overall downtime budget

75
00:04:13,701 --> 00:04:15,525
at the timescale of a decade,

76
00:04:16,315 --> 00:04:18,389
and many, many, many things will occur

77
00:04:18,389 --> 00:04:20,463
to the service during 
that time period,

78
00:04:20,463 --> 00:04:24,304
including many component failures 
and many software upgrades, like...

79
00:04:26,011 --> 00:04:28,712
Now let's expand on the 
metric just a bit.

80
00:04:28,712 --> 00:04:31,240
We've already introduced a 
notion of the MTTR,

81
00:04:31,547 --> 00:04:33,367
that is the mean time to repair.

82
00:04:33,367 --> 00:04:37,704
Now this is something that is also known 
as the recovery time objective,

83
00:04:37,704 --> 00:04:39,614
when applied to data protection.

84
00:04:40,342 --> 00:04:43,294
Now depending on the type of 
fault in the type of module,

85
00:04:43,294 --> 00:04:45,660
the MTTR can actually be quite long.

86
00:04:46,719 --> 00:04:51,070
Now the MTBF is actually the 
sum of the time to repair

87
00:04:51,070 --> 00:04:52,593
and the time between failures.

88
00:04:52,593 --> 00:04:57,081
So in other words, the time to repair 
actually shows up both in the nominator

89
00:04:57,081 --> 00:04:59,948
and the denominator 
of the downtime equation.

90
00:05:00,294 --> 00:05:03,037
I can't stress enough how 
important this ends up being,

91
00:05:03,625 --> 00:05:05,939
having an impact on 
systems design in the end.

92
00:05:06,154 --> 00:05:08,523
Unfortunately, it's often overlooked.

93
00:05:09,754 --> 00:05:12,751
Now the MTBF itself is a funny metric.

94
00:05:13,347 --> 00:05:17,035
First, it's the mean of the 
median of a distribution

95
00:05:17,035 --> 00:05:21,018
that is nearly always non-uniform, 
and that is when you're lucky,

96
00:05:21,586 --> 00:05:26,962
and sometimes correlated with other 
failures when you're an unlucky person.

97
00:05:27,611 --> 00:05:30,473
Now the best example for this are 
hard disk drives.

98
00:05:30,473 --> 00:05:35,411
High quality disk drives are sometime 
quoted with MTTF of 200 years.

99
00:05:36,248 --> 00:05:40,107
Wow, that's impressive, none of us 
would live long enough to deal with that.

100
00:05:40,264 --> 00:05:43,314
Now although this sounds impressive, 
it's a lot less impressive

101
00:05:43,314 --> 00:05:45,839
when you have 10,000 disk drives 
in your data centers

102
00:05:46,203 --> 00:05:48,983
and one fails, on average, 
every week.

103
00:05:50,244 --> 00:05:53,081
Now in addition to the failure rates 
of many components,

104
00:05:53,081 --> 00:05:55,835
disk drives, flash drives, 
power supplies, batteries,

105
00:05:56,260 --> 00:05:59,347
are subject to what is known as 
the bathtub curve.

106
00:06:00,406 --> 00:06:04,015
Now there are three critical portions 
in the bathtub curve.

107
00:06:04,173 --> 00:06:09,384
First, on the length, there's a period 
in time that has a significant amount

108
00:06:09,384 --> 00:06:11,704
of what is called infant mortality,

109
00:06:12,472 --> 00:06:16,111
and that is the early phase of the 
system being in operation.

110
00:06:16,886 --> 00:06:20,881
It is often indicated by having a 
higher than usual failure rate.

111
00:06:21,847 --> 00:06:24,909
After that, there is a somewhat 
constant failure rate

112
00:06:25,335 --> 00:06:27,340
during the normal life of the component.

113
00:06:28,241 --> 00:06:32,683
And at the other end of the bathtub, 
there is often the observation

114
00:06:32,683 --> 00:06:37,415
of having an increased failure rate 
towards the end of the useful life

115
00:06:37,415 --> 00:06:40,285
of the component, and by useful life,

116
00:06:40,285 --> 00:06:42,541
this is something that is 
measured in years,

117
00:06:42,541 --> 00:06:46,420
rarely in decades and 
absolutely never in centuries,

118
00:06:46,420 --> 00:06:51,176
even for systems that have an 
MTBF quoted as being 200 years.

119
00:06:51,176 --> 00:06:54,080
And to build available systems, 
you have to take into account

120
00:06:54,080 --> 00:06:57,437
the fact that at some point, 
the system will fail.

121
00:06:59,225 --> 00:07:02,975
Well how important is the bathtub curve 
in practice in systems design?

122
00:07:02,975 --> 00:07:06,134
Well the answer is it's fundamental 
and I want to share with you

123
00:07:06,134 --> 00:07:08,813
a real world data point from a 
company called Backblaze

124
00:07:09,297 --> 00:07:12,058
that offers backup as a 
service to consumers.

125
00:07:12,441 --> 00:07:16,187
Now the data comes from 2012, 
but I doubt it's changed since.

126
00:07:16,916 --> 00:07:21,130
And they had, at the time, a pretty large 
installation with roughly 40PB

127
00:07:21,130 --> 00:07:24,256
of capacity under management, 
and that translates to

128
00:07:24,256 --> 00:07:28,296
about 20,000 drives which are all 
packed in these custom built,

129
00:07:28,296 --> 00:07:30,490
red storage boxes you see on the picture.

130
00:07:30,688 --> 00:07:33,101
And you can put about 
45 such drives per box.

131
00:07:33,948 --> 00:07:36,526
Now the system is obviously 
designed to handle faults,

132
00:07:36,526 --> 00:07:39,235
as matter of fact, durability of the data 
is fundamental

133
00:07:39,235 --> 00:07:42,520
since this is a backup service, 
and to ensure that,

134
00:07:42,520 --> 00:07:46,007
they're leveraging the coding algorithms 
of RAID 6 to deal with failures.

135
00:07:46,007 --> 00:07:51,259
And specifically it's organized as 
RAID 6 sets of 15 drives each

136
00:07:51,259 --> 00:07:53,914
so that you can lose two drives out of 15

137
00:07:54,647 --> 00:07:56,902
and still have the ability 
to recover data.

138
00:07:58,134 --> 00:08:02,713
So I want to talk about two examples 
from this data point.

139
00:08:02,713 --> 00:08:05,694
First is the MTTR and then 
the bathtub curve itself.

140
00:08:06,675 --> 00:08:09,465
Now this may come out as a shock, 
but the MTTR,

141
00:08:09,465 --> 00:08:12,166
which is measured as the 
time it takes to recreate

142
00:08:12,166 --> 00:08:15,201
the redundancy of RAID 6, 
so full protection,

143
00:08:15,201 --> 00:08:16,625
is about three days.

144
00:08:17,784 --> 00:08:20,028
Now during that time, 
the service is available,

145
00:08:20,028 --> 00:08:23,616
the data is available, but the durability 
is no longer as guaranteed.

146
00:08:24,139 --> 00:08:26,312
It's not three seconds, 
it's not three hours,

147
00:08:26,312 --> 00:08:27,474
but three days.

148
00:08:27,690 --> 00:08:31,751
Why? Well RAID 6 does coding 
over the entire disk set.

149
00:08:32,535 --> 00:08:38,606
So 15 x 3 TB, which is 45 TB of I/Os 
required to recreate the redundancy.

150
00:08:38,606 --> 00:08:39,957
And when you do the math,

151
00:08:39,957 --> 00:08:44,594
three days works out to about 200 MB 
of sustained sequential I/O per second.

152
00:08:45,539 --> 00:08:47,110
Now onto the bathtub curve.

153
00:08:47,899 --> 00:08:51,841
Not only do the engineers at Backblaze 
notice the bathtub curve,

154
00:08:51,841 --> 00:08:55,689
but they spontaneously use the term 
when discussing failure rates,

155
00:08:55,689 --> 00:08:58,847
even though I'm not sure whether 
they read about it

156
00:08:58,847 --> 00:09:00,508
during they're academic training.

157
00:09:01,339 --> 00:09:05,024
Now the good news in their case, 
is that infant mortality is short.

158
00:09:05,024 --> 00:09:06,586
It's just a few days.

159
00:09:06,886 --> 00:09:10,008
And after a few days of 
sustained I/O burn-in tasks,

160
00:09:10,008 --> 00:09:13,882
you can effectively eliminate 
all of the disk drives

161
00:09:13,882 --> 00:09:16,993
out the of system that are 
a subject to infant mortality.

162
00:09:18,034 --> 00:09:19,478
Well then comes the bad news.

163
00:09:19,478 --> 00:09:20,553
The bad news is that

164
00:09:20,553 --> 00:09:22,988
the best drives fail
at 2% per anumm.

165
00:09:22,988 --> 00:09:25,874
So forget that 200 year MTTF.

166
00:09:25,874 --> 00:09:29,170
A 50 year MTTF is what 
you can actually expect

167
00:09:29,170 --> 00:09:31,648
in practice for sustained operation.

168
00:09:32,664 --> 00:09:34,535
The worst is about 7% by the way,

169
00:09:34,535 --> 00:09:36,943
so there's a big difference 
between manufacturers.

170
00:09:37,560 --> 00:09:40,402
With that failure rate, 
the operational team must replace

171
00:09:40,402 --> 00:09:41,864
about two drives a day,

172
00:09:42,304 --> 00:09:44,536
and since the MTTR is about three days,

173
00:09:44,536 --> 00:09:46,898
that's why you understand why 
RAID 6 is necessary

174
00:09:46,898 --> 00:09:48,963
and RAID 5 would not be adequate.

175
00:09:49,425 --> 00:09:51,761
And then the last data point to 
the bathtub curve

176
00:09:51,761 --> 00:09:56,019
is that the company started in 2008, 
so it got to be about four years old,

177
00:09:56,534 --> 00:10:00,020
and they start to see an increase in the 
failure rate of the older drives

178
00:10:00,020 --> 00:10:02,338
which are nearing the end 
of their useful life.

179
00:10:02,665 --> 00:10:05,819
So even though you may have an 
MTTF measured in decades,

180
00:10:06,266 --> 00:10:09,860
four years might be all that you can 
actually expect out of the useful life

181
00:10:09,860 --> 00:10:11,975
of the components such as a 
mechanical drive

182
00:10:11,975 --> 00:10:14,343
that is being used on a sustained basis.

183
00:10:14,622 --> 00:10:15,914
So here you have it.

184
00:10:15,914 --> 00:10:18,277
Our real world data point 
on the bathtub curve,

185
00:10:18,277 --> 00:10:20,284
and the moral of the story 
is to make sure

186
00:10:20,284 --> 00:10:22,707
that you regularly backup your own data.

187
00:10:23,894 --> 00:10:26,324
Time to wrap up this 
quick module on metrics,

188
00:10:26,324 --> 00:10:27,131
and to be fair,

189
00:10:27,131 --> 00:10:29,538
we didn't really go in-depth 
at all, into metrics.

190
00:10:29,538 --> 00:10:32,925
I only brought them up in the context 
of redundancy and fault tolerance

191
00:10:33,286 --> 00:10:36,037
so that you have a basic sense 
of what they mean

192
00:10:36,037 --> 00:10:37,854
while evaluating the principles.

193
00:10:38,394 --> 00:10:39,753
Now what did we cover?

194
00:10:39,753 --> 00:10:41,299
Well metrics are important,

195
00:10:41,299 --> 00:10:44,785
the mean is even--
is both the central data point

196
00:10:44,785 --> 00:10:47,302
that you use to compare, quantitatively,

197
00:10:47,302 --> 00:10:49,968
components, as well as 
the most meaningless metric

198
00:10:49,968 --> 00:10:52,615
when you're actually studying 
the components in detail.

199
00:10:53,100 --> 00:10:56,343
And then the last point to keep in mind, 
is the importance of repair

200
00:10:56,963 --> 00:10:59,590
in availability and in 
fault tolerance systems.
