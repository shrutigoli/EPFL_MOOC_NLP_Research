Welcome to this topic on data center systems or, more specifically, on a few selected areas in the area of data centers and data center systems.
Now, data centers are not new, they emerged with the first enterprise mainframes and have been housing data ever since.
But there was a significant change of scale about a decade ago with the emergence of Mega Data Centers, as pioneered by Google.
The Data Center was no longer a machine room within an existing building, but was a building of its own and with this change of scale came a change of mindset.
The Data Center was no longer the place in which you had multiple independent computer systems, but the Data
Center itself became a system of its own.
So let's try to see what that actually can mean.
What it can actually, specifically mean to run a Mega Data Center today.
Now this slide is from a 2012 presentation by James Hamilton, and Hamilton was a very well-respected researcher who is currently responsible for networking architecture at Amazon.
So let me read it to you: 
Each day Amazon Web Services adds enough new capacity to support all of Amazon.com's global infrastructure through the company's first five years when it was a $2.7 billion dollar annual revenue enterprise.
So - Amazon itself was started in 1994, so Hamilton is referring to the period between 1994 and 1999 and the aggregate compute capacity deployed during those five years.
And he's comparing it with the incremental daily capacity of Amazon today.
So whenever I use this slide in a public talk, I usually read the quote twice, to make sure the significance of the statement actually sinks in.
Hamilton is comparing five years of aggregate capacity with what is being deployed on an incremental daily basis every single day at Amazon today.
So, since this is not a live lecture, but on tape, I'll have you rewind until the significance of the statement sinks in.
So how do we explain such a phenomenal disruption?
Whenever something appears too large to be true, it sometimes helps to step back and try to think about it from a 'first principles,' or axiomatic perspective.
Here I selected three axioms, three 'first principles' that can explain the trend.
My selection is somewhat arbitrary, and you'll notice that they're not principles in the sense of the
Principles of Computer Systems that we've been studying in class so far.
Instead, one of them is about semiconductor density and the other two are fundamental principles of economics.
So first, Moore's Law, which is about semiconductor density, implies that new applications are possible with every new generation of silicon.
Things that were not possible a few years ago are now possible because of Moore's Law - our cell phones, our tablets, are possible today because we are operating with ever-increasing silicon densities.
Now the second law simply says that innovation is expensive and that ongoing innovation can only be justified when there is high volume.
Said differently, it is relatively easy to justify innovating once and much harder to justify innovating on a constant basis, unless there is a real high volume application.
The third law states, generically, that economies of scale can be achieved once a delivery process is automated and the product becomes a commodity, or utility.
This is true, for example, when energy became ubiquitous as part of the Industrial Revolution.
So we can think about these three principles as axioms that are going to be true in space and in time.
What's interesting here is to study how these axiom apply to the Data Center transformation; and let's first study Moore's law.
Now we've learned to scale horizontally across more and more cores, because the cores themselves are not getting any faster.
Now this has been possible, in part, because Moore's law also applies to networking, and we now have ASICs with high port count to switch between servers, so we can put together a larger number of servers in a machine room and build effective applications because these servers can effectively and efficiently communicate with each other.
Actually in this area, Moore's Law is sufficiently effective.
The switching logic of the networking switches is no longer the bottleneck, and instead, complications in transmission and coding are actually limiting the increase in speed in bandwidth and Data Centers today.
But Moore's law applies to much more than simply processing - it applies to memory, and memory is now sufficiently dense that everything that matters, in effect, is in memory.
Here is an example: the Facebook social graph, which is quite large, is stored in memory.
Not on the memory of a single server, but in the memory of a data center.
Finally, Moore's Law applies to Solid State, as well, which are quickly replacing hard disc drives for durable storage in theData Center, as least for all forms of structured data so you're relational database is no longer stored on rotating drives, but instead on much, much more efficient
Solid State drives - again, because of Moore's Law.
So now let's look at the second axiom - the notion that volume drives sustainable innovation.
Now, one thing to keep in mind is that, even though data centers are extremely large today, they remain a niche, in the sense that there are much fewer computers in data centers, that we actually have computers in our pockets, or carrying with us on our desks.
So, to be effective, modern Data Centers are pragmatically built using commodity components that were designed with other applications in mind, and so we're leveraging the volume of other applications in order to effectively build data centers.
Let me give an example - X86 chips were designed for desktops.
This was true and is still true today.
The desktop business at Intel drives the main innovation in X86 chips and the server line, which Intel calls "Xeon," basically leverages and fine-tunes that innovation for server application, not the other way around.
Desktops is driving innovation in servers.
So if you follow the logic, why not use cell phone chips in data centers?
After all, they're much more power-efficient than desktop chips, and there's actually precisely an active line of research and development along that path.
Now, although this may sound obvious, keep in mind that this is only representative of modern cloud-scale data centers.
Your typical data center has a very different mix.
Rather than using desktop or cell phone chips, enterprise data centers still have a lot of legacy servers and legacy technologies.
In 2011, the X86 server market was only 50% of the dollar value of the total server market.
The other 50% went to legacy technologies, such as mainframe and RIS computers.
Now, similarly, enterprise data centers use a combination of networking technologies, things like Fibre Channel, whereas cloud-scale data centers only use the same Ethernet standard that is ubiquitous in campus network.
So here is another example where we are leveraging volume of technologies deployed for a broader application, in order to build a cost-effective data center.
This actually has a significant implication, and that is because the 
Law of Sustainable Innovation applies .
Because cloud data centers are built using commodity parts, the price performance gap, with the classic solution increases every year.
Now let's shift to the third axiom that can explain the rise in cloud-scale data centers.
And it is that they're actually engineered as a system that can deliver some utility at scale.
So consider the picture on the left, of the hydroelectric dam.
The picture is actually from the Grande Dixence in Valais.
One of the largest dam of its type in the world, and the pride of Swiss civil engineering, and it delivers a utility, which is hydroelectric electricity at scale.
Now, the picture on the right is a new Facebook data center, somewhere in Oregon.
It is also engineered to deliver a utility, the social network, at a very large scale.
Similarly, Amazon's data centers are also designed to deliver a utility, infrastructure as a service, or cloud computing, or virtual machines, at a massive scale.
And put together these three underlying technologies and economic principles have serious implications.
We have Moore's Law, we have the notion that volume drives sustainable innovation.
We have the notion of economies of scale and delivery.
Now, let's think about what the implications are this time, from the perspective of the domain of computer systems.
And the implication is that there is a massive architectural disruption that is going on.
In particular, when comparing a traditional enterprise infrastructure with a cloud infrastructure.
So on the left, the enterprise architectural model was built around the notion that individual computer systems had to be: resilient, available and serviceable.
In contrast, data center systems just let the various components fail, unrepaired, in lights-out data centers.
If a server in a Google machine room fails, it is simply going to be left in place, powered off.
We've build 50 years of transactional database systems based around the notion of ACID transactions, and the notion that important data was on disc.
In the cloud, important data is in memory and this data is often replicated on a weekly consistent basis.
The enterprise infrastructure model has a lot of legacy technologies that were designed at the time when data was on disc.
The cloud is shifting toward a model where everything is in memory.
Finally, the classic model consists of individually provisioned components that act as independent systems with, typically, three completely indistinct operational teams responsible for their management.
The application team, which manages the individual servers; the networking team, which manages the network; and the storage team, which actually manages the data separately from the applications.
Now in the cloud, there is no such concept, since everything is automated.
In other words, the configuration of the data center itself is entirely done in software.
Now in the next modules, we will discuss the implication of this disruption and we'll specifically talk about some of the design principles associated with building these cloud-scale applications.
But to summarize this module on infrastructure, there's a lot of hype about a cloud but below the cloud, realize that there is a new data center.
And that new data center is built on a certain set of principles.
It is designed to operate at scale and it is designed to operate as a system, and to deliver a utility at a massive scale.
If you're interested in going further into those topics,
I can recommend to you two books.
One by Barrozo and HÃ¶tzle, two of the leading engineers at Google, who've written a book on warehouse-scale data center systems that summarizes their experience at Google.
And the second book by Nicholas Carr, called "The Big Switch," which makes the analogy between the emergence of the mega data centers and the emergence of the electricity grid, about a century ago.
