1
00:00:04,086 --> 00:00:06,565
In this module, we're going
to go back about a decade

2
00:00:06,565 --> 00:00:11,063
and study the architecture of the first
generation internet-scale services.

3
00:00:11,063 --> 00:00:14,531
Now, these services are tiny
by today's standards,

4
00:00:14,531 --> 00:00:16,469
but they were quite large at the time.

5
00:00:16,469 --> 00:00:20,142
They were even described
as "giant-scale services".

6
00:00:20,461 --> 00:00:24,166
And these were services that were designed
around the year 2000,

7
00:00:24,166 --> 00:00:26,866
just as the first internet bubble
was bursting.

8
00:00:27,332 --> 00:00:29,866
And they had a killer application

9
00:00:29,866 --> 00:00:31,767
and that killer application
was "search".

10
00:00:31,992 --> 00:00:34,596
So, today we associate search
with Google or Bing,

11
00:00:34,596 --> 00:00:35,962
but at the time,

12
00:00:35,962 --> 00:00:39,899
the leading search applications
were Altavista and Inktomi.

13
00:00:40,898 --> 00:00:43,800
And Altavista was designed
by engineers at DEC,

14
00:00:43,800 --> 00:00:46,000
who had access to,
what was at the time,

15
00:00:46,000 --> 00:00:49,268
one of the largest 64-bit
machines available,

16
00:00:49,268 --> 00:00:51,400
the ALPHA TurboLaser.

17
00:00:51,400 --> 00:00:54,531
And they realized that they could
actually fit the web in memory.

18
00:00:55,161 --> 00:00:57,662
And so Altavista,
which was the leading search engine

19
00:00:57,662 --> 00:00:59,935
before the turn of the century,

20
00:00:59,935 --> 00:01:03,437
could actually hold the entire index
of the web in memory

21
00:01:03,437 --> 00:01:06,188
and serve the entire web,

22
00:01:06,188 --> 00:01:09,100
or search application
to all the web's client

23
00:01:09,100 --> 00:01:11,666
from the memory of that one machine.

24
00:01:12,800 --> 00:01:15,524
Now unfortunetly this did not scale.

25
00:01:15,789 --> 00:01:19,301
Around the same time Dave Patterson
and Eric Brewer at Berkeley

26
00:01:19,301 --> 00:01:23,100
started the Berkeley NOW project:
Network of Workstations,

27
00:01:23,100 --> 00:01:26,567
studying how you can actually
put a lot of cheap computers together

28
00:01:26,567 --> 00:01:28,899
to build equivalent applications.

29
00:01:29,264 --> 00:01:31,334
Out of the NOW project came Inktomi.

30
00:01:31,334 --> 00:01:35,651
Inktomi was a commercial search service,
which was competitive with Altavista.

31
00:01:35,651 --> 00:01:37,365
Today both of them are dead,

32
00:01:37,635 --> 00:01:40,399
but in 2001 Eric Brewer wrote some
of the experiences

33
00:01:40,399 --> 00:01:43,528
associated with the deployment of Inktomi
in a paper

34
00:01:43,528 --> 00:01:47,558
called "Lessons from Giant Scale Services"

35
00:01:47,558 --> 00:01:50,434
and this is actually what we
will be covering in this module.

36
00:01:50,936 --> 00:01:53,304
So, this is what the basic model
looks like.

37
00:01:53,304 --> 00:01:55,830
Now, although some of the elements
seem obvious today,

38
00:01:55,830 --> 00:01:57,367
this was not obvious
at the time.

39
00:01:58,308 --> 00:02:00,233
Now, the first comment about this figure

40
00:02:00,233 --> 00:02:02,723
is the notion of a single-site server.

41
00:02:03,028 --> 00:02:04,733
A collection of servers

42
00:02:04,733 --> 00:02:08,033
actually serves a single function
and appears as a single server.

43
00:02:08,263 --> 00:02:10,437
And the load manager in front of it

44
00:02:10,437 --> 00:02:13,962
effectively hides the internal details
of that single-site server

45
00:02:13,962 --> 00:02:15,625
from the outside world,

46
00:02:15,625 --> 00:02:18,300
allowing for horizontal
scale out.

47
00:02:19,235 --> 00:02:21,305
Now, the second comment about this figure

48
00:02:21,305 --> 00:02:23,836
is that scale out applies
equally to servers

49
00:02:23,836 --> 00:02:25,265
and to storage.

50
00:02:25,265 --> 00:02:26,700
This means the elimination

51
00:02:26,700 --> 00:02:28,500
of any notion of centralized storage.

52
00:02:28,864 --> 00:02:32,599
This also a departure
from the Standard Enterprise model.

53
00:02:32,832 --> 00:02:35,400
Now, let's study load balancing
in further detail.

54
00:02:35,400 --> 00:02:39,174
Now, the purpose of the load manager
is to hide the internals

55
00:02:39,174 --> 00:02:40,712
of the single-site server.

56
00:02:40,712 --> 00:02:43,967
And this is realized by combining
different mechanisms.

57
00:02:44,800 --> 00:02:48,144
The first mechanism leverages the level
of indirection in DNS

58
00:02:48,144 --> 00:02:51,233
by returning different IP addresses
for the same DNS name,

59
00:02:51,233 --> 00:02:54,600
based on the context of the request
or the load

60
00:02:54,600 --> 00:02:57,352
or the availability
of the independent servers.

61
00:02:58,642 --> 00:03:01,711
Now, this is, however, only a very
coarse-grain mechanism,

62
00:03:01,711 --> 00:03:03,767
because of DNS caching,

63
00:03:03,767 --> 00:03:07,766
so DNS load balancing is typically used
to identify another middle box,

64
00:03:07,766 --> 00:03:10,467
another layer that hides the end servers,

65
00:03:10,467 --> 00:03:13,700
which is called a layer 4 or 7
load balancer.

66
00:03:14,033 --> 00:03:17,289
Now, layer 4 and layer 7 load balancers
are actually quite different

67
00:03:17,289 --> 00:03:19,475
in how they are architected
and constructed.

68
00:03:19,670 --> 00:03:23,321
A layer 4 load balancer maps TCP flows
onto different end servers,

69
00:03:23,321 --> 00:03:25,168
could either terminate the connection,

70
00:03:25,168 --> 00:03:27,833
or simply perform
network address translation.

71
00:03:28,335 --> 00:03:33,599
A more sophisticated layer 7 load balancer
actually terminates the HTTP requests,

72
00:03:33,599 --> 00:03:36,666
identifies the session cookie
of that HTTP request,

73
00:03:36,666 --> 00:03:39,698
and uses that cookie
to determine the end server.

74
00:03:40,164 --> 00:03:44,103
With that, you can actually guarantee
that different HTTP connections

75
00:03:44,103 --> 00:03:47,638
correspond to the same session,
will be handled by the same end server,

76
00:03:47,638 --> 00:03:50,929
which helps in the design
of some of the applications.

77
00:03:51,534 --> 00:03:54,733
It's actually interesting to study
the evolution of the basic model

78
00:03:54,733 --> 00:03:58,232
of 2001 with the solutions
that we see today.

79
00:03:59,165 --> 00:04:02,321
My first observation is many
of the components are still relevant,

80
00:04:02,321 --> 00:04:05,129
if anything, they're as relevant as ever.

81
00:04:05,525 --> 00:04:08,000
Load managers, for example,
play a central role

82
00:04:08,000 --> 00:04:12,734
in offloading the traffic
from the mega data centers today,

83
00:04:12,734 --> 00:04:16,458
by caching some of the critical data
in edge locations.

84
00:04:17,054 --> 00:04:19,630
Beyond that, there have been some changes
in the model.

85
00:04:19,630 --> 00:04:22,329
Significant changes in the assumption
that an application

86
00:04:22,329 --> 00:04:24,533
will be located in a single site,

87
00:04:24,533 --> 00:04:28,333
that has been, in a pretty fundamental way
today replaced by the notion

88
00:04:28,333 --> 00:04:32,300
that any large application is spread out
across multiple locations.

89
00:04:32,633 --> 00:04:35,999
This has actually
significant implications,

90
00:04:35,999 --> 00:04:38,698
in particular, around the notion
of the CAP theorem,

91
00:04:38,698 --> 00:04:40,900
which we will study
in the next module.

92
00:04:41,767 --> 00:04:44,333
But beyond that, there have been
other, subtle changes.

93
00:04:44,333 --> 00:04:47,333
For example,
although we still have internal storage,

94
00:04:47,333 --> 00:04:51,533
the notion that we would actually use RAID
as the building block

95
00:04:51,533 --> 00:04:54,733
to ensure the durability of the data,
has been replaced

96
00:04:54,733 --> 00:04:59,701
by the notion that we would partition
and replicate data three ways,

97
00:04:59,701 --> 00:05:02,633
within a site,
across multiple machines.

98
00:05:02,989 --> 00:05:08,400
And that's simply a result or consequence
of the increase in capacity in disk drives

99
00:05:08,400 --> 00:05:12,669
and the increase MTTRs associate
with the repair of a disk drive

100
00:05:12,669 --> 00:05:14,057
in a RAID group.

101
00:05:14,057 --> 00:05:17,134
You can actually get much faster behavior
by replicating the data

102
00:05:17,134 --> 00:05:22,635
across all of the machines of the cluster,
rather than within a single computer.

103
00:05:24,268 --> 00:05:26,767
And finally,
in the original diagram,

104
00:05:26,767 --> 00:05:30,140
we had a concept
of an optional blackplane,

105
00:05:30,140 --> 00:05:35,365
which was used as a high-bandwidth
exchange mechanism between computers.

106
00:05:35,410 --> 00:05:38,332
For example, as a storage fabric
or a compute fabric.

107
00:05:38,468 --> 00:05:41,201
The idea was to provide scalable
bi-section bandwidth

108
00:05:41,201 --> 00:05:44,411
to allow servers to communicate
with each other efficiently.

109
00:05:44,544 --> 00:05:49,028
Now, the requirement has not changed,
and it's actually more present than ever.

110
00:05:49,028 --> 00:05:53,134
However, the implementation has evolved
into IP based fabrics,

111
00:05:53,134 --> 00:05:57,766
which can be efficiently realized
using commodity components and factories

112
00:05:58,152 --> 00:06:01,067
And what you see on screen
is the new Tor spine architecture,

113
00:06:01,067 --> 00:06:03,767
that is the standard
in today's mega data centers.

114
00:06:04,465 --> 00:06:08,601
Now, to be fair, none of these changes
invalidate the original model,

115
00:06:08,601 --> 00:06:10,866
if anything they are
incremental evolutions

116
00:06:10,866 --> 00:06:13,464
that were necessary
to keep these servers scaling

117
00:06:13,464 --> 00:06:16,674
with technology trends
and in a cost-effective way.

118
00:06:17,267 --> 00:06:20,833
Now, giant-scale services introduce
actually a new form of metrics.

119
00:06:21,433 --> 00:06:24,701
If you remember our module
on redundancy in fault tolerance,

120
00:06:24,701 --> 00:06:28,200
we introduced a fundamental availability
and downtime metric.

121
00:06:28,536 --> 00:06:31,795
The fact that downtime lowers the ratio
of the MTTR

122
00:06:31,795 --> 00:06:35,735
over the sum of the MTTR and the MTTF.

123
00:06:36,048 --> 00:06:39,932
And that metric makes a lot of sense
if you think about a single server

124
00:06:39,932 --> 00:06:42,400
that is serving a particular application,

125
00:06:42,400 --> 00:06:45,833
but it makes a lot less sense
if you have hundreds of servers

126
00:06:45,833 --> 00:06:48,976
that are independently operating
in a co-operative fashion

127
00:06:48,976 --> 00:06:52,864
in order to provide an answer
to an internet-scale service.

128
00:06:53,063 --> 00:06:56,134
And those new metrics were actually
introduced by Eric Brewer

129
00:06:56,196 --> 00:06:58,355
as yield and harvest.

130
00:06:58,864 --> 00:07:03,568
So first, the yield is the ratio
between the number of queries completed

131
00:07:03,568 --> 00:07:05,484
and the number of queries offered.

132
00:07:05,484 --> 00:07:08,827
And note that the yield metric is not just
a metric of fault tolerance,

133
00:07:08,827 --> 00:07:11,836
in that it is not only impacted
by failure.

134
00:07:12,166 --> 00:07:13,696
Instead, the yield is a metric

135
00:07:13,745 --> 00:07:17,668
that characterizes whether a site
is overloaded or not in its given state.

136
00:07:18,007 --> 00:07:21,472
And the nice thing about that metric
is that it's an end to end metric,

137
00:07:21,472 --> 00:07:24,934
which cloud providers can
and actually do measure.

138
00:07:26,766 --> 00:07:29,226
Now, the second metric
is called the harvest.

139
00:07:29,818 --> 00:07:31,193
And it is a different metric

140
00:07:31,193 --> 00:07:35,534
that makes the observation
that not all data is necessarily available

141
00:07:35,534 --> 00:07:38,322
by the cloud service
at any given point in time.

142
00:07:38,826 --> 00:07:41,134
And it's based on the insight

143
00:07:41,134 --> 00:07:45,435
that many queries can actually return
with a partial dataset.

144
00:07:45,793 --> 00:07:47,142
Take, for example, search.

145
00:07:47,142 --> 00:07:48,927
It's okay to return search results

146
00:07:48,927 --> 00:07:51,299
that only considered
95 percent of the dataset.

147
00:07:51,667 --> 00:07:54,901
And it's actually preferable to return
a good enough answer quickly,

148
00:07:54,901 --> 00:07:56,969
than to delay a response

149
00:07:56,969 --> 00:08:01,163
or have to tell the user
that the site is, overall, not available.

150
00:08:02,000 --> 00:08:05,900
And so the harvest metric measures
what fraction of the complete data

151
00:08:05,967 --> 00:08:10,299
that should be considered,
was actually considered, on average,

152
00:08:10,299 --> 00:08:11,631
for each query.

153
00:08:11,966 --> 00:08:14,234
Now, very much
like the fault tolerance model

154
00:08:14,234 --> 00:08:16,734
led to a specific design process,

155
00:08:16,734 --> 00:08:20,526
the harvest and yield metrics
lead to their own design process

156
00:08:20,630 --> 00:08:22,666
and have design implications.

157
00:08:23,333 --> 00:08:27,724
For example, many web services today
define yield

158
00:08:27,724 --> 00:08:29,899
with a latency bound in mind.

159
00:08:30,267 --> 00:08:33,631
A hundred milliseconds, or one second,
for example, for search.

160
00:08:34,333 --> 00:08:36,854
Now this in turn has an impact
on the harvest,

161
00:08:36,884 --> 00:08:39,332
since the dataset may not be processed
in time.

162
00:08:40,793 --> 00:08:44,467
A great example, where harvest and yield
are taken into consideration

163
00:08:44,467 --> 00:08:48,125
is the design and implementation
of an ad serving service.

164
00:08:48,477 --> 00:08:50,593
Of course, you want to serve ads,

165
00:08:50,593 --> 00:08:52,698
and of course you want
to serve them quickly,

166
00:08:52,698 --> 00:08:57,201
and so you want to maximize your yield
under some SLA constraint.

167
00:08:57,334 --> 00:09:00,025
You also want to personalize the ad,

168
00:09:00,025 --> 00:09:02,065
but it's okay to only personlize it

169
00:09:02,065 --> 00:09:04,890
to the extent that you don't violate
the SLA.

170
00:09:05,161 --> 00:09:09,632
Which means that the harvest metric
may be compromized to maintain the yield.

171
00:09:10,735 --> 00:09:13,832
So, this is a great example
of how both harvest and yield

172
00:09:13,832 --> 00:09:16,033
can help with the design
of a cloud service

173
00:09:16,033 --> 00:09:18,527
that we can effectively
get more effective ads.

174
00:09:18,833 --> 00:09:21,732
Or it's a terrible one,
depending on your point of view.

175
00:09:22,234 --> 00:09:25,063
Now the concept of harvest and yields
are actually linked,

176
00:09:25,063 --> 00:09:28,935
because of a related principle,
called the DQ "Principle".

177
00:09:29,566 --> 00:09:31,899
Now, I'm going to put "principle"
here in quotes,

178
00:09:31,899 --> 00:09:34,767
because this is not really
a principle at all,

179
00:09:34,767 --> 00:09:39,633
but more an observation that has led
to a useful generalization.

180
00:09:40,160 --> 00:09:44,091
And the intuition is that every system
has a bottleneck.

181
00:09:44,091 --> 00:09:48,200
And it's typically the network link
for frontend servers,

182
00:09:48,200 --> 00:09:51,234
and it's typically the disk
for backend servers.

183
00:09:52,602 --> 00:09:57,364
If every system, or every component
in the datacenter system has a bottleneck,

184
00:09:57,364 --> 00:10:00,031
the implication is that the capacity
of each system

185
00:10:00,031 --> 00:10:02,897
is actually very stable
for any given service.

186
00:10:04,232 --> 00:10:06,472
And that it is the product
of the amount of data

187
00:10:06,472 --> 00:10:09,934
consumed by each query
with the query rate.

188
00:10:10,333 --> 00:10:15,732
So the DQ "Principle" states that D,
which is the amount of data per query

189
00:10:15,732 --> 00:10:19,699
times Q which is the query rate,
is a constant.

190
00:10:20,734 --> 00:10:25,100
So, think about the DQ value as a metric
for the capacity of a site.

191
00:10:25,533 --> 00:10:29,102
If you add servers,
you increase your DQ capacity.

192
00:10:29,423 --> 00:10:33,399
If you lose servers, you correspondingly
decrease DQ capacity.

193
00:10:33,626 --> 00:10:36,114
If you have to take servers offline,
for any reason,

194
00:10:36,114 --> 00:10:38,333
your DQ capacity also goes down.

195
00:10:38,999 --> 00:10:44,234
Of course, the DQ value can have
a direct impact on either the harvest

196
00:10:44,234 --> 00:10:46,667
or the yield of the service.

197
00:10:47,093 --> 00:10:48,928
And here are some examples.

198
00:10:48,928 --> 00:10:51,200
If you lose one server out of <i>n</i>,

199
00:10:51,200 --> 00:10:53,054
the load would have to be redirected

200
00:10:53,054 --> 00:10:54,405
to <i>n</i> - 1 servers.

201
00:10:54,405 --> 00:10:56,993
And this can potentially overload
the remaining servers

202
00:10:56,993 --> 00:10:59,431
by the ratio of <i>n</i> over <i>n</i> - 1.

203
00:10:59,732 --> 00:11:02,466
Same observation holds
if you lose <i>k</i> servers.

204
00:11:02,466 --> 00:11:04,333
And at some point the load

205
00:11:04,333 --> 00:11:06,634
will exceed the DQ capacity
that is remaining

206
00:11:06,634 --> 00:11:09,436
and this will in turn reduce your yield.

207
00:11:10,232 --> 00:11:12,465
Now the similar argument could also hold

208
00:11:12,465 --> 00:11:15,667
if you assume that the data is
spread out across the servers

209
00:11:15,667 --> 00:11:17,602
and the data is not replicated.

210
00:11:17,706 --> 00:11:19,728
Your harvest is then reduced
proportionally

211
00:11:19,728 --> 00:11:22,567
based on the number of available servers.

212
00:11:22,964 --> 00:11:25,030
Now, this might sound trivial,

213
00:11:25,030 --> 00:11:28,402
and in some ways it is somewhat 
obvious in retrospect,

214
00:11:28,402 --> 00:11:32,258
the main value of the DQ principle
and the harvest and yield metrics,

215
00:11:32,258 --> 00:11:34,335
is that they allow you to reason

216
00:11:34,335 --> 00:11:36,401
about the implication
of certain operations.

217
00:11:36,632 --> 00:11:39,600
Consider here, the case
where a site needs to be upgraded

218
00:11:39,600 --> 00:11:42,298
and all the servers need to be rebooted.

219
00:11:42,834 --> 00:11:45,498
Now, the picture shows
three different scenarios.

220
00:11:46,066 --> 00:11:48,133
The first one is a fast reboot.

221
00:11:48,666 --> 00:11:51,400
Now, during the reboot,
no queries can be served.

222
00:11:51,400 --> 00:11:53,323
Of course, this impacts your yield.

223
00:11:53,734 --> 00:11:56,799
The second scenario
is the rolling upgrade,

224
00:11:56,799 --> 00:12:00,962
where servers are upgraded
one at a time.

225
00:12:01,105 --> 00:12:02,631
Now, during a rolling upgrade,

226
00:12:02,631 --> 00:12:06,608
a fraction of the data is unavailable,
unless you're replicating it,

227
00:12:06,608 --> 00:12:09,127
and this would obviously
impact your harvest.

228
00:12:09,595 --> 00:12:12,891
And the final model is what Eric Brewer
called the "Big Flip".

229
00:12:13,269 --> 00:12:16,333
The idea here is to steer
all the new traffic

230
00:12:16,333 --> 00:12:18,599
through one half of the nodes,

231
00:12:18,599 --> 00:12:22,600
and then upgrade the nodes
while they are not serving traffic.

232
00:12:23,433 --> 00:12:27,933
Your DQ value goes down by 50 percent
during the entire operation,

233
00:12:27,933 --> 00:12:32,998
but if you replicate the entire dataset
on the half of the remaining servers,

234
00:12:32,998 --> 00:12:35,822
it will not impact your harvest.

235
00:12:36,327 --> 00:12:40,400
And as long as your DQ capacity
remains sufficiently large,

236
00:12:40,400 --> 00:12:43,402
it will also not impact your yield.

237
00:12:43,850 --> 00:12:47,439
Now, of course, this is the best case,
if you're not replicating the data,

238
00:12:47,439 --> 00:12:49,867
your harvest will go down
by up to 50 percent.

239
00:12:49,867 --> 00:12:51,632
And if you're overloading it,

240
00:12:51,632 --> 00:12:54,433
your yield will go down
by up to 50 percent,

241
00:12:54,433 --> 00:12:56,195
if you're actually overloaded,

242
00:12:56,195 --> 00:12:58,627
and the Big Flip is the technique
they used actually

243
00:12:58,627 --> 00:12:59,833
to upgrade the servers,

244
00:12:59,833 --> 00:13:02,866
and in one case to move from
one location to another location

245
00:13:02,866 --> 00:13:06,661
as they were operating the Inktomi
search service in production.

246
00:13:07,489 --> 00:13:10,567
Time to wrap up this module
on internet scale services.

247
00:13:10,567 --> 00:13:13,300
We saw the basic model of 2001.

248
00:13:13,300 --> 00:13:16,601
Since then, if anything,
things have gotten larger

249
00:13:16,601 --> 00:13:18,834
but also simpler, internally.

250
00:13:19,334 --> 00:13:21,300
We also saw some new metrics,

251
00:13:21,300 --> 00:13:24,401
the concept of yield, harvest
and the DQ "Principle",

252
00:13:24,734 --> 00:13:27,299
as more applicable
to internet-scale services

253
00:13:27,299 --> 00:13:29,967
than the classic metrics
of fault tolerance.

254
00:13:30,662 --> 00:13:34,167
What we have not talked about
is any notion of consistency

255
00:13:34,167 --> 00:13:36,333
of the data that is being served.

256
00:13:36,333 --> 00:13:39,794
That's the topic of the next module
on the CAP theorem.
