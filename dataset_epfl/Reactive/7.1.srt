1
00:00:01,070 --> 00:00:04,610
In this last week of the reactive
programming course, we

2
00:00:04,610 --> 00:00:08,930
will focus first on something which was
implicitly there already.

3
00:00:08,930 --> 00:00:16,050
Namely, that actors as independent agents
of computation are by default distributed.

4
00:00:16,050 --> 00:00:19,880
Normally you run them just on different
CPU's in the same system.

5
00:00:19,880 --> 00:00:22,640
But, there is nothing stopping you from
running them on

6
00:00:22,640 --> 00:00:25,300
different network hosts, and we will do
this in practice.

7
00:00:25,300 --> 00:00:27,470
This means,

8
00:00:27,470 --> 00:00:32,950
if you picture for example, people who are
living on different continents that

9
00:00:32,950 --> 00:00:38,520
it takes some effort for them to agree on
a common truth or a common decision.

10
00:00:38,520 --> 00:00:42,890
The same thing is true for actors and we
call this eventual consistency.

11
00:00:44,840 --> 00:00:49,390
After that, we will talk about scalability
and responsiveness

12
00:00:49,390 --> 00:00:53,600
and how you achieve these in an
actor-based system.

13
00:00:53,600 --> 00:00:57,090
Rounding up all the four tenets of
reactive

14
00:00:57,090 --> 00:01:03,524
programming, which were event-driven
Scalable, resilient, and responsive.

15
00:01:03,524 --> 00:01:07,440
>> Actors are designed to be
distributed, and in this

16
00:01:07,440 --> 00:01:10,610
lecture, we will see an example of that
using the Acacluster.

17
00:01:10,610 --> 00:01:14,930
In order to fully appreciate how far

18
00:01:14,930 --> 00:01:17,780
actors take us, when it comes to
distribution,

19
00:01:18,880 --> 00:01:20,540
let us look at.

20
00:01:20,540 --> 00:01:24,600
What the impact of network communication
is to a distributive program.

21
00:01:26,010 --> 00:01:30,890
compared to communication within the same
process, by calling methods for example.

22
00:01:30,890 --> 00:01:37,090
We will experience several differences
when doing that over the network.

23
00:01:38,670 --> 00:01:41,930
First of all, the memory is not shared
anymore.

24
00:01:43,450 --> 00:01:43,920
Data can

25
00:01:43,920 --> 00:01:47,490
only be shared by value because a copy
will be made.

26
00:01:47,490 --> 00:01:50,330
The object will be serialized, sent over
the network

27
00:01:50,330 --> 00:01:56,500
connection, de-serialized, therefore, the
object will not be the same.

28
00:01:56,500 --> 00:02:02,900
We have seen that a stateful object is one
whose behavior depends on its history.

29
00:02:04,210 --> 00:02:09,650
But the history of the copy of the other
side of the network will not be the same

30
00:02:09,650 --> 00:02:13,380
after a while than the history of the
object on the

31
00:02:13,380 --> 00:02:17,940
originating node.
This has the consequence

32
00:02:17,940 --> 00:02:21,550
that only immutable objects can be
sensibily shared.

33
00:02:22,880 --> 00:02:28,630
Other differences include that the
bandwidth available for communication is

34
00:02:28,630 --> 00:02:35,120
a lot lower when you traverse the network.
And the latency for all communication is

35
00:02:35,120 --> 00:02:36,550
quite a bit higher.

36
00:02:36,550 --> 00:02:40,030
You cannot transfer a network packet in a
nanosecond

37
00:02:40,030 --> 00:02:43,160
but you can call a method in a nanosecond.

38
00:02:43,160 --> 00:02:46,230
Together with a need to serialize and
de-serialize

39
00:02:46,230 --> 00:02:49,410
objects instead of just passing along
their reference.

40
00:02:50,770 --> 00:02:54,550
This makes the difference even more
severe.

41
00:02:55,650 --> 00:02:58,410
But in addition to these quantitative
effects,

42
00:02:58,410 --> 00:03:01,280
there are also new things which can
happen.

43
00:03:01,280 --> 00:03:04,470
When you send a message over the network
It might just not

44
00:03:04,470 --> 00:03:09,600
arrive, or it might arrive, and the reply
might not come back.

45
00:03:09,600 --> 00:03:13,590
Thus, there is the possibility of partial
failure,

46
00:03:13,590 --> 00:03:15,590
or part of your messages might not arrive,

47
00:03:15,590 --> 00:03:19,190
for example, and you cannot know which
ones

48
00:03:19,190 --> 00:03:21,500
that were until you get the reply back.

49
00:03:23,280 --> 00:03:27,740
Another kind of failure is that data
corruption can also happen.

50
00:03:27,740 --> 00:03:30,500
I know from personal experience that when
you

51
00:03:30,500 --> 00:03:34,330
send data down a TCP connection for
example.

52
00:03:34,330 --> 00:03:38,240
You will see about one corruption event
per terabyte data sent.

53
00:03:39,290 --> 00:03:42,200
Running multiple processes on the same
computer

54
00:03:42,200 --> 00:03:46,060
and having them communicate for example
Over the

55
00:03:46,060 --> 00:03:48,850
localhost interface, makes some of these
differences

56
00:03:48,850 --> 00:03:53,010
less severe, in number, but the
qualitative issues

57
00:03:53,010 --> 00:03:54,660
stay the same.

58
00:03:54,660 --> 00:03:59,575
The overarching description of the problem
is that distributed computing

59
00:03:59,575 --> 00:04:05,010
breaks the core assumptions made by a
synchronous programming model.

60
00:04:06,830 --> 00:04:10,900
We have talked about actors for quite
awhile now, so you know that

61
00:04:10,900 --> 00:04:16,260
all of their communication is asynchronous
one way, and not guaranteed to arrive.

62
00:04:17,570 --> 00:04:18,210
Hence,

63
00:04:18,210 --> 00:04:21,980
they model precisely what the network
gives us.

64
00:04:21,980 --> 00:04:25,510
You could say that, instead of taking the
local model and

65
00:04:25,510 --> 00:04:30,950
trying to extend it to the network, actors
took the inverse approach.

66
00:04:30,950 --> 00:04:35,600
Looking at the network model and using
that on the local machine.

67
00:04:37,030 --> 00:04:39,470
The next feature of actors is that they

68
00:04:39,470 --> 00:04:43,450
run so fully isolated from each other that
from

69
00:04:43,450 --> 00:04:46,790
the outside they all look the same.
They all just, ActorRefs.

70
00:04:47,820 --> 00:04:51,450
Regardless where they live it will be the
same sending a message to them.

71
00:04:52,780 --> 00:04:55,370
This is what we call location
transparency.

72
00:04:57,070 --> 00:05:02,210
The current semantics in feature set
offered by [UNKNOWN] actors have

73
00:05:02,210 --> 00:05:07,670
been reached by rigorously treating
everything as if it were remote.

74
00:05:07,670 --> 00:05:08,660
And all features

75
00:05:08,660 --> 00:05:12,220
which cannot be modeled in this world were
removed.

76
00:05:14,030 --> 00:05:16,560
As a result, the effort of writing

77
00:05:16,560 --> 00:05:20,830
a distributed program Using actors is
basically

78
00:05:20,830 --> 00:05:25,810
the same as the effort of writing a local
variant of the same program.

79
00:05:26,890 --> 00:05:30,180
The code itself will not look much
different, as we will soon see.

80
00:05:32,000 --> 00:05:34,180
Before we can witness actors

81
00:05:34,180 --> 00:05:36,720
talking to each other over the network, we
need

82
00:05:36,720 --> 00:05:39,310
to look at how that is done under the
hood.

83
00:05:40,970 --> 00:05:44,530
We know that actors are hierarchical and
every actor

84
00:05:44,530 --> 00:05:47,730
has a name in the name space of its
parent.

85
00:05:47,730 --> 00:05:51,545
They form a tree just like a file system.

86
00:05:51,545 --> 00:05:54,858
Let's say we have an actor here, named
user.

87
00:05:54,858 --> 00:06:01,540
This is [UNKNOWN]

88
00:06:01,540 --> 00:06:06,460
Guardian actor for all actors you create.
With system actor of.

89
00:06:06,460 --> 00:06:13,050
And this one creates a greeter.

90
00:06:13,050 --> 00:06:17,210
This is the same as

91
00:06:17,210 --> 00:06:21,840
a file system having a folder called user
and in it a folder called greeter.

92
00:06:24,020 --> 00:06:27,440
All actors can have children so they are
all like folders and not like files.

93
00:06:28,980 --> 00:06:33,100
When you run this program and print the
path

94
00:06:33,100 --> 00:06:36,170
of this actorOf that you will show you
this.

95
00:06:38,200 --> 00:06:42,410
All paths conform to the URI format.
The

96
00:06:42,410 --> 00:06:47,390
uniform resource identifier.
This one has an, a

97
00:06:47,390 --> 00:06:52,866
so-called authority

98
00:06:52,866 --> 00:06:58,922
part, [SOUND]
and a path.

99
00:07:02,210 --> 00:07:08,200
In Akka, the authority part is used to
display the address of the actor system.

100
00:07:09,240 --> 00:07:14,680
In this case, we created an actor system
called HelloWorld in the Akka scheme,

101
00:07:14,680 --> 00:07:20,290
so the Akka protocol designates that this
is a local Actor path.

102
00:07:21,940 --> 00:07:25,880
Then with slashes, we separate the path
element.

103
00:07:25,880 --> 00:07:27,340
So user is this actor

104
00:07:27,340 --> 00:07:32,590
greeter is this actor.
We want to talk about actors

105
00:07:32,590 --> 00:07:37,560
which use the network, so we should look
at a remote address example.

106
00:07:38,850 --> 00:07:42,760
Here we have an akka system using the TCP
protocol.

107
00:07:43,970 --> 00:07:47,990
Also named hello world at some IP address
or

108
00:07:47,990 --> 00:07:52,350
hostname, colon and then the port at which
it listens.

109
00:07:54,520 --> 00:07:57,580
This description is enough for any other
actor system

110
00:07:57,580 --> 00:08:01,790
to send a message to any actor within this
one.

111
00:08:01,790 --> 00:08:07,400
It will just open a TCP connection to this
IP address, and port

112
00:08:07,400 --> 00:08:12,980
and try to connect to this actor system
should it have responded to that.

113
00:08:14,240 --> 00:08:19,200
We have seen here two examples, a local
address for the greeter,

114
00:08:19,200 --> 00:08:20,500
and a remote address.

115
00:08:22,160 --> 00:08:26,630
Which for the greeter, would look like
this, this

116
00:08:26,630 --> 00:08:30,770
means, that every actor is identified by
at least one

117
00:08:30,770 --> 00:08:34,980
URI, it could have multiple, if for
example, the actor

118
00:08:34,980 --> 00:08:39,950
system is reachable by multiple protocols
or multiple IP addresses.

119
00:08:41,850 --> 00:08:44,030
Up to know we have nasty work with
ActorRef.

120
00:08:45,070 --> 00:08:48,620
And the newly found ActorPath has some
relationship

121
00:08:48,620 --> 00:08:52,610
to it ActorRef, which needs to be properly
explained.

122
00:08:53,770 --> 00:08:59,060
Actor names are unique within a parent,
but once the terminated message has

123
00:08:59,060 --> 00:09:03,820
been delivered for a child, the parent
knows that the name can be reused.

124
00:09:03,820 --> 00:09:06,870
It could create an actor which has exactly
the

125
00:09:06,870 --> 00:09:10,560
same name but it will not be the same
actor.

126
00:09:10,560 --> 00:09:12,750
The ActorRef will be a different one.

127
00:09:13,880 --> 00:09:18,860
Therefore, keep in mind that ActorPath is
just the full name of

128
00:09:18,860 --> 00:09:23,650
the actor and the ActorPath exists whether
the actor exists or not.

129
00:09:24,730 --> 00:09:29,490
An ActorRef on the other hand points
exactly to one actor.

130
00:09:29,490 --> 00:09:31,170
Which was started at some point.

131
00:09:32,240 --> 00:09:35,700
It points to one incarnation we say, of
the

132
00:09:35,700 --> 00:09:36,470
actor path.

133
00:09:37,870 --> 00:09:40,210
An actor path also has a tell method, we

134
00:09:40,210 --> 00:09:44,020
will see that later, so you can send it
messages.

135
00:09:44,020 --> 00:09:46,990
The difference is, since you don't know
that behind

136
00:09:46,990 --> 00:09:51,590
this ActorPath actually an actor exists,
it cannot be watched.

137
00:09:53,180 --> 00:09:57,010
You need an ActorRef if you want to use
life cycle monitoring.

138
00:09:58,360 --> 00:10:01,110
In the end the difference between an
ActorRef and

139
00:10:01,110 --> 00:10:05,660
an ActorPath is just one number.
We call it the UID of the

140
00:10:05,660 --> 00:10:10,510
actor incarnation.
If you print an ActorRef, it looks just

141
00:10:10,510 --> 00:10:17,800
like the actor path, but with a fragment
part added, which is this UID.

142
00:10:19,950 --> 00:10:25,790
When communicating with remote systems, it
is necessary to talk to actors

143
00:10:25,790 --> 00:10:32,110
Which you have not created and for which
you have no means to acquire an ActorRef.

144
00:10:32,110 --> 00:10:35,390
You just know at which host and port the
Actor lives.

145
00:10:35,390 --> 00:10:40,080
What the system's name is and where the
Actor is in the hierarchy over there.

146
00:10:41,330 --> 00:10:45,120
To support this, the Actor context has

147
00:10:45,120 --> 00:10:48,120
another method.
Called actor selection.

148
00:10:49,440 --> 00:10:55,590
It can pass in any actor path, and it will
construct something which you can send to.

149
00:10:55,590 --> 00:10:57,930
It's the only operation it supports.

150
00:10:58,960 --> 00:11:04,600
And, there is one message that every actor
automatically handles.

151
00:11:04,600 --> 00:11:09,936
And that is akka.actor.identify, imported
here,

152
00:11:09,936 --> 00:11:16,084
which takes one piece of data, a
correlation identifier, you

153
00:11:16,084 --> 00:11:22,190
can use anything you want.
This little actor here,

154
00:11:22,190 --> 00:11:28,730
the resolver will perform the job of
resolving an actor path, if possible.

155
00:11:28,730 --> 00:11:32,520
To the actor ref of the actor, which is
currently living under that name.

156
00:11:34,270 --> 00:11:35,130
So sending

157
00:11:35,130 --> 00:11:39,860
the Identify message with the pair of path
and current

158
00:11:39,860 --> 00:11:44,990
sender will result in us getting back an
ActorIdentity.

159
00:11:46,140 --> 00:11:52,570
In the positive case, where the actor is
currently alive, we get this one.

160
00:11:52,570 --> 00:11:54,790
With some reference.

161
00:11:54,790 --> 00:12:00,240
So we can tell our client that we resolved
the path, and this is the actor

162
00:12:00,240 --> 00:12:02,250
ref of the actor currently.

163
00:12:02,250 --> 00:12:05,350
Otherwise, the actor identity message will
contain none.

164
00:12:07,000 --> 00:12:11,170
This procedure reflects that communication
is necessary.

165
00:12:11,170 --> 00:12:15,460
In order to determine whether an actor
exists or does not exist.

166
00:12:16,640 --> 00:12:21,760
This is why no actor refs can just be
fabricated from thin air.

167
00:12:21,760 --> 00:12:25,440
They must be obtained by using identify
from

168
00:12:25,440 --> 00:12:28,530
paths, if there is no other way to obtain
them.

169
00:12:30,650 --> 00:12:33,330
Actor Paths do not have to be absolute.

170
00:12:33,330 --> 00:12:37,770
They do not need to contain all the Actor
systems address information.

171
00:12:37,770 --> 00:12:41,330
They can also be relative to the current
Actor.

172
00:12:41,330 --> 00:12:45,380
When we see context.actorSelection, and
give

173
00:12:45,380 --> 00:12:49,520
a relative URI without a leading slash.

174
00:12:49,520 --> 00:12:55,200
Then it will be interpreted to the current
actors position in the hierarchy.

175
00:12:55,200 --> 00:12:56,470
In this case,

176
00:12:56,470 --> 00:13:00,680
if the current actor has a child actor
named child.

177
00:13:00,680 --> 00:13:04,380
That one had a child actor named
grandchild.

178
00:13:04,380 --> 00:13:06,410
And this actor selection will select it.

179
00:13:08,350 --> 00:13:12,120
Just as in file system, you can use dot
dot

180
00:13:12,120 --> 00:13:17,020
to signify the parent at any point in the
selection.

181
00:13:17,020 --> 00:13:22,130
If the selection starts with a slash, its
anchor

182
00:13:22,130 --> 00:13:25,020
will be the system's root guardian.

183
00:13:25,020 --> 00:13:29,490
So you can resolve actors which live
somewhere near the top Of

184
00:13:29,490 --> 00:13:33,690
the hierarchy, and where you know the
exact name of all parents.

185
00:13:35,270 --> 00:13:40,150
Finally, actor selection also supports
wild cards.

186
00:13:40,150 --> 00:13:45,415
So, you can, for example, send a message
to this actor selection.

187
00:13:45,415 --> 00:13:52,820
/users/controllers/* to send to all
children of the controllers actor.

188
00:13:55,590 --> 00:14:00,490
The ability to send to a name instead of
to an actor ref is

189
00:14:00,490 --> 00:14:05,760
exploited by the akka cluster module which
we will look at next.

190
00:14:05,760 --> 00:14:11,230
First, what is a cluster?
A cluster is foremost

191
00:14:11,230 --> 00:14:16,150
a set of nodes, of actor systems in this
case, and

192
00:14:16,150 --> 00:14:21,070
this set is defined such that all members
of

193
00:14:21,070 --> 00:14:24,520
this set, so all nodes of the same
cluster, have the

194
00:14:24,520 --> 00:14:28,280
same idea about who is in the cluster and
who is not.

195
00:14:30,180 --> 00:14:33,280
It is like a group of people where
everybody

196
00:14:33,280 --> 00:14:38,280
knows everybody else and foreigners are
known to be foreign.

197
00:14:38,280 --> 00:14:42,040
This group of people can then collaborate
on a common task.

198
00:14:42,040 --> 00:14:44,490
And the same can be done between actor
systems.

199
00:14:45,930 --> 00:14:47,110
Let's say,

200
00:14:47,110 --> 00:14:52,328
we have some.
Actor systems, some nodes, A, B,

201
00:14:52,328 --> 00:14:57,270
C and D.
And these four,

202
00:15:00,090 --> 00:15:04,190
all have the same idea that they four form
a cluster.

203
00:15:05,340 --> 00:15:07,120
Now if we have another node here.

204
00:15:09,220 --> 00:15:14,070
Called e and this node thinks it forms a
cluster

205
00:15:16,490 --> 00:15:22,050
with the other four.
Then the set of a, b, c, d, and e is not a

206
00:15:22,050 --> 00:15:28,050
cluster because not everyone in this set
agrees upon who is in the cluster.

207
00:15:29,240 --> 00:15:33,440
Cluster membership can change over time,
of course.

208
00:15:33,440 --> 00:15:36,500
And A, B, C, and D might learn of

209
00:15:36,500 --> 00:15:41,370
Es existence, and finally accept E into
the same cluster.

210
00:15:41,370 --> 00:15:47,220
And then the cluster would have size five.
We'll see how that works in a minute.

211
00:15:49,530 --> 00:15:54,930
Clusters are formed in a form of inductive
reasoning.

212
00:15:54,930 --> 00:15:59,620
It starts with one node which basically
joins itself, it declares

213
00:15:59,620 --> 00:16:04,400
itself to be a cluster of size one, and

214
00:16:04,400 --> 00:16:09,530
then any given node can join a given
cluster.

215
00:16:09,530 --> 00:16:14,810
To enlarge that cluster by 1.
This is achieved by

216
00:16:14,810 --> 00:16:18,640
sending a request to any current member of
the cluster and

217
00:16:18,640 --> 00:16:22,131
then the information is spread within the
cluster than this new

218
00:16:22,131 --> 00:16:26,146
one wants to join and once all the current
members have

219
00:16:26,146 --> 00:16:30,295
learned of this, they agree okay it is a
good idea.

220
00:16:30,295 --> 00:16:31,690
Let's accept it in.

221
00:16:34,090 --> 00:16:39,610
One important property of the akka cluster
is that it does not contain any

222
00:16:39,610 --> 00:16:45,550
central leader or coordinator which would
be a single point of failure.

223
00:16:47,530 --> 00:16:51,740
Information is disseminated in an epidemic
fashion.

224
00:16:51,740 --> 00:16:55,902
This gossip protocol is resilient to
failure because...

225
00:16:55,902 --> 00:16:59,110
Every node will gossip to a few of

226
00:16:59,110 --> 00:17:03,480
its peers, every second, regardless of
whether that was successful or

227
00:17:03,480 --> 00:17:08,670
not, so eventually, all information will
be spread throughout the cluster.

228
00:17:09,790 --> 00:17:11,640
Let's see how this works in practice.

229
00:17:12,770 --> 00:17:17,740
For that, we need one more dependency,
which is the akka cluster module.

230
00:17:19,660 --> 00:17:24,470
And in the configuration we need to enable
the cluster module by saying

231
00:17:24,470 --> 00:17:32,750
that the akka.actor.provider must be
akka.cluster.ClusterActorRefProvider.

232
00:17:32,750 --> 00:17:38,730
This configures the actor system to use a
different mechanism when creating actors

233
00:17:38,730 --> 00:17:43,920
All calls to context actor of are in the
end handled by the actor ref provider,

234
00:17:46,140 --> 00:17:49,530
and the cluster one supports a few
operations

235
00:17:49,530 --> 00:17:51,790
that the local one cannot, as we will see.

236
00:17:53,840 --> 00:17:56,380
There are several ways to apply this
configuration,

237
00:17:56,380 --> 00:18:01,010
for example, you could add a file named
application.conf.

238
00:18:01,010 --> 00:18:04,660
To the class path which contains this
snippet.

239
00:18:04,660 --> 00:18:09,770
Or you can use Java system properties like
that on the command line.

240
00:18:11,940 --> 00:18:14,540
Next, we need to write a new main program.

241
00:18:15,910 --> 00:18:21,820
This actor, when it starts up, obtains the
cluster extension of the system.

242
00:18:23,980 --> 00:18:29,890
Subscribes to some events.
This in, in the end works the

243
00:18:29,890 --> 00:18:35,990
same way as the event stream.
And finally, it joins its own address.

244
00:18:37,240 --> 00:18:43,560
If remember, the first step in forming a
cluster is that one node must

245
00:18:43,560 --> 00:18:49,130
start it off, it must join itself.
Then, in the behavior of this

246
00:18:49,130 --> 00:18:53,890
actor, we wait for MemberUp events, for
which we registered.

247
00:18:55,060 --> 00:18:59,660
And if the member which just joined is not
the current one,

248
00:19:00,910 --> 00:19:04,660
then someone else joined and then we can
do some interesting stuff.

249
00:19:04,660 --> 00:19:10,040
The default configuration of the remote
communication module is

250
00:19:10,040 --> 00:19:15,890
that it will start listening on port 2552
using TCP.

251
00:19:15,890 --> 00:19:20,450
To make this exercise interesting, we need
of course a second node.

252
00:19:20,450 --> 00:19:25,860
Since all actor systems will need to
listen on a TCP port But only 1 of

253
00:19:25,860 --> 00:19:31,680
them can have the port 2552.
We need to configure a different port

254
00:19:31,680 --> 00:19:36,840
for the ClusterWorker.
Setting the port is done like

255
00:19:36,840 --> 00:19:41,890
this with the property
akka.remote.netty.tcp.port.

256
00:19:41,890 --> 00:19:44,650
And setting it to 0 means to pick a random
one.

257
00:19:46,110 --> 00:19:48,430
We don't need to know at which port the
cluster

258
00:19:48,430 --> 00:19:52,100
worker lives because it will just join the
main one.

259
00:19:53,200 --> 00:20:01,860
The address of the cluster main can be
derived from this worker's self address by

260
00:20:01,860 --> 00:20:07,680
replacing the port with the number 2552.
And then, the worker joins

261
00:20:07,680 --> 00:20:13,430
the main In this case, we registered for
receiving

262
00:20:13,430 --> 00:20:17,910
not member up, but member removed events.
And

263
00:20:17,910 --> 00:20:22,790
whenever the address of the removed member
is the main one.

264
00:20:22,790 --> 00:20:28,260
So when the main program shuts down, this

265
00:20:28,260 --> 00:20:33,150
also stops.
With these 2 main programs, we can

266
00:20:33,150 --> 00:20:36,840
if we run them observe that they will
join.

267
00:20:36,840 --> 00:20:39,110
But nothing much will happen.

268
00:20:39,110 --> 00:20:42,510
We need to define some actor which makes
use of the cluster.

269
00:20:43,770 --> 00:20:48,760
For this, we are going to modify the well
known receptionist.

270
00:20:48,760 --> 00:20:52,890
To spawn the controller actors not
locally.

271
00:20:52,890 --> 00:20:56,050
Using context actor of simply as local
children.

272
00:20:56,050 --> 00:20:58,350
But to run them instead on

273
00:20:58,350 --> 00:21:01,170
cluster nodes which are not the current
node.

274
00:21:02,570 --> 00:21:04,030
How does that work?

275
00:21:04,030 --> 00:21:10,740
First of all, the receptionist needs to
know who is in the cluster and who is not.

276
00:21:10,740 --> 00:21:14,970
Therefore, it subscribes to member up and
member removed events.

277
00:21:16,060 --> 00:21:25,030
And when it stops, it unsubscribes itself.
In response to cluster.subscribe.

278
00:21:25,030 --> 00:21:29,110
The actor will always receive first the
current cluster state.

279
00:21:30,370 --> 00:21:35,010
And this current cluster state has a
members list.

280
00:21:35,010 --> 00:21:40,000
I convert these members to a linear
sequence from a set and

281
00:21:40,000 --> 00:21:45,050
a map _.address over it to extract all the
addresses of

282
00:21:45,050 --> 00:21:50,550
the cluster nodes.
Then from these addresses I filter out

283
00:21:50,550 --> 00:21:54,759
the self address.
So whatever remains is not myself.

284
00:21:55,930 --> 00:21:59,660
And if there is another node, so not
myself

285
00:21:59,660 --> 00:22:03,650
is not empty, then I change to the active
node.

286
00:22:05,310 --> 00:22:09,030
But typically, when the receptionist is
started, that happens quite

287
00:22:09,030 --> 00:22:12,770
early in the program, the cluster has not
yet been formed.

288
00:22:12,770 --> 00:22:15,920
So typically, the current cluster state
will contain

289
00:22:15,920 --> 00:22:17,590
no members.

290
00:22:17,590 --> 00:22:22,750
Therefore there is the second case that
upon member up.

291
00:22:22,750 --> 00:22:25,530
If that members address is not my own
address.

292
00:22:27,508 --> 00:22:29,570
I change to the active state with the

293
00:22:29,570 --> 00:22:31,970
vector of size one getting just that
member.

294
00:22:33,300 --> 00:22:36,390
While of 18 members we have no computing

295
00:22:36,390 --> 00:22:41,420
resources In order to perform the task of
getting

296
00:22:41,420 --> 00:22:42,598
url for example.

297
00:22:42,598 --> 00:22:45,406
So during that time we reply through the
sender

298
00:22:45,406 --> 00:22:48,550
that it failed because we have no nodes
available.

299
00:22:49,980 --> 00:22:53,880
The active behavior will also have to
monitor the cluster.

300
00:22:53,880 --> 00:22:58,120
Because after all, members can be added or
removed at any point in time.

301
00:23:00,400 --> 00:23:05,410
When more members are added, again which
is not the self address, then we just

302
00:23:07,030 --> 00:23:11,560
change to the active state with the
addition of the newly known address.

303
00:23:13,160 --> 00:23:19,620
And when members are removed from this
set, we filter it out from the addresses.

304
00:23:19,620 --> 00:23:24,500
And, if that was the last one, then we go
back to the awaiting members state.

305
00:23:24,500 --> 00:23:25,560
Otherwise we

306
00:23:25,560 --> 00:23:27,300
continue with the reduced list.

307
00:23:29,830 --> 00:23:34,460
Now we get closer to the interesting part,
using the information we just obtained.

308
00:23:36,010 --> 00:23:39,210
In the active state, when a get request
comes in.

309
00:23:40,240 --> 00:23:45,575
We look whether the currently running
requests that is context.

310
00:23:45,575 --> 00:23:50,450
children.size is less than the addresses
we know about.

311
00:23:51,850 --> 00:23:55,120
Otherwise, we have one request running per
cluster node.

312
00:23:55,120 --> 00:23:58,570
Lets say that is the limit and then we
reject it.

313
00:23:58,570 --> 00:24:03,020
But If it is the first request which comes
in, that will always work.

314
00:24:04,920 --> 00:24:10,750
So we copy the client, that's the sender
of this get request.

315
00:24:10,750 --> 00:24:17,420
Then we pick an address randomly from the
list and extract it here.

316
00:24:17,420 --> 00:24:20,370
And then we create a new actor A

317
00:24:20,370 --> 00:24:23,210
customer which I'll show the in the next

318
00:24:23,210 --> 00:24:26,190
slide, which gets the client, URL, which
is

319
00:24:26,190 --> 00:24:30,650
supposed to be retrieved, and a cluster
address,

320
00:24:30,650 --> 00:24:32,840
where the work is supposed to be
performed.

321
00:24:34,630 --> 00:24:40,200
Please note that these two lines are
necessary because

322
00:24:40,200 --> 00:24:44,700
The actor creation described in here runs
asynchronously.

323
00:24:45,790 --> 00:24:48,880
Props describe how to create an actor
later.

324
00:24:50,670 --> 00:24:53,360
Therefore, it is subject to the, the

325
00:24:53,360 --> 00:24:58,410
same restrictions as with future
call-backs within actors.

326
00:24:58,410 --> 00:25:02,470
Or scheduled actions within actors as we
have seen last week.

327
00:25:04,900 --> 00:25:10,720
Finally, the customer actor.
This one is reponsible

328
00:25:10,720 --> 00:25:16,000
for making sure that the given URL is
retrieved but the work

329
00:25:16,000 --> 00:25:20,580
is supposed to be performed at a remote
node whose address is given here.

330
00:25:21,770 --> 00:25:25,800
For that we need to create a controller.
We have props, controller here.

331
00:25:28,060 --> 00:25:33,580
Props, as I said, is a recipe for how to
create an actor, and what

332
00:25:33,580 --> 00:25:39,220
we have used so far is the default
behavior of creating an actor locally.

333
00:25:40,410 --> 00:25:42,740
But it also supports giving other
arguments.

334
00:25:44,080 --> 00:25:47,340
Here we say, with deploy and then a deploy

335
00:25:47,340 --> 00:25:50,210
that is a description of how to deploy the
actor.

336
00:25:51,640 --> 00:25:53,580
You can modify several things.

337
00:25:53,580 --> 00:25:58,440
One of them is the scope in which it shall
be deployed.

338
00:25:58,440 --> 00:26:00,630
In this case it is a Remote Scope.

339
00:26:00,630 --> 00:26:03,470
We do not want to create the actor on the
local node,

340
00:26:03,470 --> 00:26:07,435
we want to create it on the given node,
whose address is here.

341
00:26:11,090 --> 00:26:18,720
This is the only change which is necessary
To perform the work on a remote node.

342
00:26:18,720 --> 00:26:24,390
The call to context actorOf then looks
just the same, gives back an actorOf.

343
00:26:24,390 --> 00:26:27,550
Sending to this actorOf looks just the
same,

344
00:26:27,550 --> 00:26:30,110
but this will go over the network now.

345
00:26:30,110 --> 00:26:36,330
Context.watch controller will watch this
actor

346
00:26:36,330 --> 00:26:37,750
also over the network.

347
00:26:38,820 --> 00:26:43,330
But the call we make here looks exactly
the same as in the local case.

348
00:26:45,380 --> 00:26:50,230
There is one more detail in this actor
which needs special explanation.

349
00:26:50,230 --> 00:26:51,450
It's this line here.

350
00:26:53,320 --> 00:26:57,140
Whenever a message is sent for example,
here

351
00:26:57,140 --> 00:27:00,050
to another actor we know that the self

352
00:27:00,050 --> 00:27:03,630
reference of the enclosing actor is
automatically picked

353
00:27:03,630 --> 00:27:07,800
up and passed along as the sender
reference.

354
00:27:07,800 --> 00:27:11,300
This customer actor here is an [UNKNOWN]

355
00:27:11,300 --> 00:27:15,100
one, it is one which is not supposed to be
seen from the outside.

356
00:27:16,230 --> 00:27:20,600
What this line does, is it changes the
meaning of

357
00:27:20,600 --> 00:27:25,110
who the sender of messages of this actor
shall be.

358
00:27:25,110 --> 00:27:28,570
This implicit value is also of type actor
ref.

359
00:27:28,570 --> 00:27:31,396
And it is available in the current scope,
which

360
00:27:31,396 --> 00:27:35,800
takes precedence over implicit values
found in the actor trait.

361
00:27:36,880 --> 00:27:40,730
Therefore, all messages sent by this
customer, will

362
00:27:40,730 --> 00:27:43,900
appear to have been sent by its parent
instead.

363
00:27:43,900 --> 00:27:44,400
Since

364
00:27:46,490 --> 00:27:52,540
the consequences of this remote deployment
are not visible in code.

365
00:27:52,540 --> 00:27:54,940
Let me draw a diagram to show what
happens.

366
00:28:02,310 --> 00:28:04,940
The whole program starts, because we

367
00:28:04,940 --> 00:28:09,530
instantiate the cluster main application
actor.

368
00:28:09,530 --> 00:28:13,600
We need to mark out in which actor system
that happens.

369
00:28:14,850 --> 00:28:15,350
This is.

370
00:28:18,930 --> 00:28:22,120
Cluster main.
And it's guardian actor.

371
00:28:23,840 --> 00:28:29,340
The app then goes on to create the
receptionist,

372
00:28:30,710 --> 00:28:37,200
And when a request comes in this one will
create a Customer.

373
00:28:38,630 --> 00:28:42,060
Up to this point everything has been
local.

374
00:28:42,060 --> 00:28:43,950
But the customer deploys the

375
00:28:43,950 --> 00:28:49,680
controller into the cluster workers
system, so we need to draw that as well.

376
00:28:52,820 --> 00:29:00,580
This system also has a, user guardian.
And application.

377
00:29:03,390 --> 00:29:05,220
But this application does not do anything

378
00:29:05,220 --> 00:29:07,910
besides waiting for the termination of the
program.

379
00:29:09,890 --> 00:29:12,220
This system has another thing which is
interesting here.

380
00:29:13,890 --> 00:29:19,800
It has a remote guardian, and when the
customer deploys the controller.

381
00:29:19,800 --> 00:29:24,480
What it really does is it sends a message

382
00:29:24,480 --> 00:29:26,960
to this one to create the controller for
it.

383
00:29:31,580 --> 00:29:35,790
This one will first create a folder so

384
00:29:35,790 --> 00:29:40,480
that it can keep actors deployed from
different other systems.

385
00:29:40,480 --> 00:29:48,100
And within it, it will create a marker
for, that it was

386
00:29:49,320 --> 00:29:55,600
user, app, receptionist, customer, and so
forth.

387
00:29:56,910 --> 00:29:59,800
And then it will finally create

388
00:30:03,680 --> 00:30:04,810
The controller actor.

389
00:30:07,020 --> 00:30:07,700
These here are

390
00:30:10,420 --> 00:30:11,970
not really actors.

391
00:30:11,970 --> 00:30:15,830
They are just names inserted such that,

392
00:30:15,830 --> 00:30:18,610
the controller can be found for remote
communication.

393
00:30:21,060 --> 00:30:27,220
But the controller will be the child actor
of the customer

394
00:30:27,220 --> 00:30:30,945
logically.
So when in the controller we say,

395
00:30:30,945 --> 00:30:35,910
contexts.parent, the message will go here.
Then as in

396
00:30:35,910 --> 00:30:40,902
this link checker example, it will spawn

397
00:30:40,902 --> 00:30:46,270
getters as needed during the retrieval of
the URL.

398
00:30:48,630 --> 00:30:52,190
Now that we have successfully remote
deployed

399
00:30:52,190 --> 00:30:55,430
the controller, we just need to supervise
it.

400
00:30:56,490 --> 00:31:02,860
As usual, there was the receive timeout,
in which case We unwatch the controller

401
00:31:02,860 --> 00:31:07,120
and say, the retrieval has failed, because
we didn't get anything in time.

402
00:31:08,470 --> 00:31:13,680
Note that deathwatch works exactly the
same even though the controller

403
00:31:13,680 --> 00:31:15,110
is remote deployed.

404
00:31:15,110 --> 00:31:18,790
So in that case, we also give a failure
message.

405
00:31:18,790 --> 00:31:21,640
If we get back a successful result.

406
00:31:21,640 --> 00:31:25,026
Again we unwatch and send back the
successful result.

407
00:31:25,026 --> 00:31:33,620
And then whatever happens after any of
these three has occurred, we need to stop.

408
00:31:33,620 --> 00:31:38,700
And this stop is recursive, so once this
parent actor is stopped the

409
00:31:38,700 --> 00:31:40,030
controller is stopped with it.

410
00:31:41,240 --> 00:31:44,700
We need to explicitly annotate the type of
this

411
00:31:44,700 --> 00:31:48,940
partial function literal here because
galas type inference would

412
00:31:48,940 --> 00:31:51,830
not be able to figure that out without
help

413
00:31:51,830 --> 00:31:54,810
a when it comes to the and then
commonator.

414
00:31:56,990 --> 00:31:59,940
I have loaded these actors as well into
the

415
00:31:59,940 --> 00:32:03,330
Eclipse project so that we can try it out.

416
00:32:03,330 --> 00:32:09,210
First here you see the cluster worker with
the code that you already know.

417
00:32:09,210 --> 00:32:13,330
Note that the web client will now be
called in the cluster worker so

418
00:32:13,330 --> 00:32:18,550
this is where we need to clean up the
resources of the async web client.

419
00:32:19,820 --> 00:32:22,230
The ClusterMain is like the

420
00:32:22,230 --> 00:32:26,210
original main program, but it has been
adapted as shown to

421
00:32:26,210 --> 00:32:30,700
watch for cluster events and to join
itself in the beginning.

422
00:32:32,940 --> 00:32:38,520
And in the behavior, when we're, when we
receive the MemberUp event and

423
00:32:38,520 --> 00:32:44,250
the member's address is not our own Then
we know that there is a cluster worker.

424
00:32:44,250 --> 00:32:50,740
So I have written a small function get
later to use this system scheduler

425
00:32:50,740 --> 00:32:57,310
to send a get request to the receptionist
after a certain delay which is given here.

426
00:32:57,310 --> 00:32:58,180
So after 1

427
00:32:58,180 --> 00:33:01,400
second we retrieve google.com.

428
00:33:01,400 --> 00:33:04,740
After 2 seconds, we try two parallel
queries.

429
00:33:04,740 --> 00:33:08,430
One of them should fail because there's
just one cluster worker.

430
00:33:08,430 --> 00:33:10,930
And then we try two more.

431
00:33:13,590 --> 00:33:16,990
In the run configuration for the cluster
main, I have

432
00:33:16,990 --> 00:33:21,440
set the necessary option to switch on the
clustering support.

433
00:33:23,110 --> 00:33:27,070
And here is another option, which can be
set We say

434
00:33:27,070 --> 00:33:31,300
the cluster shall not start unless there
are at least two members.

435
00:33:33,650 --> 00:33:38,430
For the cluster worker, as mentioned, we
need to configure the remote port

436
00:33:38,430 --> 00:33:43,260
to be automatic.
And then there is one more option which

437
00:33:43,260 --> 00:33:49,730
I've set, which is called auto down.
Which I will explain in the following.

438
00:33:51,240 --> 00:33:56,268
So, we run the cluster main.
It starts up.

439
00:33:56,268 --> 00:34:06,070
Informs us that the node akka tcp main on
port two, five, five, two.

440
00:34:06,070 --> 00:34:09,545
Is in the joining phase but nothing has
happened yet.

441
00:34:09,545 --> 00:34:16,430
When we start up, the cluster worker, the
program will start to run, and

442
00:34:19,730 --> 00:34:22,110
now it has finished, so we can look at the
logs.

443
00:34:29,410 --> 00:34:32,820
This was the line which we had already
seen.

444
00:34:32,820 --> 00:34:39,070
Then, the cluster worker running on the
[UNKNOWN] port is also seen as joining.

445
00:34:40,610 --> 00:34:44,170
Then That, this here, it says the leader
is moving

446
00:34:44,170 --> 00:34:48,830
node 2552 to up and the other one also to
up.

447
00:34:50,030 --> 00:34:54,480
I told you there is no single point of
bottleneck in akka cluster.

448
00:34:54,480 --> 00:34:56,040
Although it does have a leader.

449
00:34:57,250 --> 00:35:00,380
Now this seems to be contradictory but it
is not so.

450
00:35:00,380 --> 00:35:03,660
The leader is not elected.
It is one special node.

451
00:35:03,660 --> 00:35:09,450
Any node can be the leader.
And it is just statically determined

452
00:35:09,450 --> 00:35:14,720
by this set of node addresses.
They are sorted in a certain format

453
00:35:14,720 --> 00:35:19,610
and then always the first address which is
in the membership list will be

454
00:35:19,610 --> 00:35:20,590
the leader.

455
00:35:20,590 --> 00:35:25,740
And since everybody agrees on who is in
the list and on the sort order.

456
00:35:25,740 --> 00:35:30,160
Everybody will see the same as the leader
without the need for communication.

457
00:35:33,250 --> 00:35:38,185
Once the nodes are up, the program goes on
to retrieve the page from

458
00:35:38,185 --> 00:35:44,950
Google.com.
Then we launched two queries in parallel.

459
00:35:44,950 --> 00:35:47,570
One of them gets rejected by too many
queries.

460
00:35:49,500 --> 00:35:50,730
The other are successful.

461
00:35:55,200 --> 00:35:59,780
Finally, we had modified the main program
such that it

462
00:35:59,780 --> 00:36:03,970
will shut down after the retrievals have
all been performed.

463
00:36:03,970 --> 00:36:09,780
So the leader is moving itself first to
leaving and then to exiting states.

464
00:36:09,780 --> 00:36:12,170
And then the whole program shuts down.

465
00:36:12,170 --> 00:36:16,650
There is some communication afterwards
between nodes in the end.

466
00:36:16,650 --> 00:36:20,740
First this one shuts down and the other
one has

467
00:36:20,740 --> 00:36:22,520
also automatically shut down.

468
00:36:24,060 --> 00:36:29,630
This one has gone by a different route
which I will explain in the following.

