Recall that our whole premise is that we will use a distribution of a statistical variable at the level of a random sample distribution, in order to understand the distribution of this variable at the level of the population.
If we have a model for the population distribution, we can calculate everything we need to know about a population, provided that we know the value of the parameter it depends on.
So, a natural question is, how can we use the sample distribution in order to infer the unknown parameter of the population model?
This problem is known as the problem of point estimation.
So let's say we have a model,
F x Theta, for our population, which has some explicit formula that we omit for the moment.
Though we know the formula of the model, we don't know the value of the true parameter Theta.
For example, we know that the model is exponential with rate Theta, but we don't know the precise value of the rate.
We have, however, at our disposal a sample of X1, Xn from the population.
In order to estimate the unknown perimeter Theta, we will construct an estimator.
A function Theta hat of the sample values X1 to Xn, who's value will provide our estimate of the unknown parameter.
If only 2 questions naturally arise, one, how do we construct estimators?
In other words, what is the right formula for Theta hat?
And two, how de we evaluate the precision of estimators?
We will first start with question 1, and then eventually move on to question 2.
What ever form of estimator we end up using, the motivating principal always remains the same.
We wish to calibrate the population distribution to the sample distribution.
For example, on the right you can see a sample histogram in red, and different possible exponential distributions models in blue.
Each one corresponds to a different choice of the rate parameter.
Which one should we choose
to arrive at the best possible agreement between the sample and the population?
This calibration is always with respect on criteria.
For instance, we might ask that the parameter of interest should be such that the mean of the population model, and the mean of the sample distribution coincide.
This leads to what we call the Method of Moments Estimator.
Another approach is to ask for a finer calibration.
We might want to ask what is the parameter value that makes the sample as representative as possible for the corresponding model?
This leads to what we call
The Maximum Likelihood Estimator.
If the population model depends on the parameter Theta, then the mean of the population will also be a function of that parameter
Let's call it M of Theta.
Now, if we have a random sample
X1 of  Xn, we can also calculate the sample mean.
Let's call it X bar.
What the method of moments suggests is to find the value of Theta, that yields a population means
M of Theta, that matches the sample mean
X bar as best as possible.
Equivalently, we are asked to solve the equation M of Theta is equal to 1 over N, the sum of X1 up to Xn, with respect to Theta.
The resulting solution, is called, The Method of Moments Estimator of Theta.
As an example, let's consider a population that we choose to model using the exponential distribution.
The diagram on the right, shows a histogram in red.
This corresponds to a sample of size 40 that we've drawn from the population.
As we said, we model this population by the exponential distribution.
This depends on one unknown perimeter Theta, which is called the rate perimeter.
In blue you can see overlaid several curves, that corresponds to different exponential distributions with different choices of Theta.
Which one should we choose to best match the sample?
The method of moment says, that we should first of all look at the sample mean.
This is located where this vertical line is and is actually equal to 0,938.
The method of moment then suggests that of all these blue curves, we should select the one that has population means equal to 0.938.
This specific curve is given in grey.
So what is the precise value of Theta that corresponds to the grey curve?
Well, the mean of the exponential distribution is the inverse of it's perimeter.
It follows that the method of moments estimator should solve the equation that says that the population mean should be equal to the sample mean.
Solving this, gives us an estimate for Theta which we call Theta Hat, and equals 1 over X bar.
The precise value is 1 divided by which gives is 1.066.
The method of moments chooses a parameter to match the sample population location as expressed by the mean.
Another approach would be to choose a parameter to match the sample and population distribution at a finer level.
A choice such that the sample is as representative as possible of the population.
Equivalently, this requires the proportions calculated from the sample histogram match proportions calculated from the population curve as best as possible.
It turns out that there is a natural mathematical way to choose the best such Theta.
And this is called, The Method of Maximum Likelihood.
It says that we should choose the Theta that maximizes the function called
The Likelihood Function.
L of Theta.
This is equal to the product of model function each evaluated at one of the sample values.
It actually has an intuitive interpretation.
It is the parameter choice yielding the population model under which our random sample would've been most probable to obtain.
It often happens that the
Method of Moments yields the same estimator as the Method of Maximum Likelihood.
This will be the case, for example, in the normal exponential and plus on models.
But it can also happen that the two estimation methods product different estimators in the same model, for the same sample.
So, one might naturally ask which one should we use?
Well, the method of moments is typically simpler and easier to apply. so that can be an advantage in complicated situations.
On the other hand,
Maximum Likelihood is usually preferred, because it makes more efficient use data at hand.
Many cases in fact it makes the most efficient use of the data.
Another advantage of
Maximum Likelihood is that we have some rather general formulas to describe it's accuracy.
Given an estimator of a population parameter, it is important to have an understanding of it's precision.
While the value of an estimator, which we call The Estimate, is constant for a given sample, it will vary over different samples.
So, for the first sample we might get an estimate at a short of the truth.
For the second sample, we might get an estimate that is even shorter of the truth.
While for the third sample, we might slightly overestimate the truth.
The moral of the story is, every time we collect a new sample we get a new estimate.
To understand the accuracy of our estimator we must thus understand the fluctuations of our estimate around the value of the true parameter as the sample varies over all possible choices over the population.
If we consider all possible samples of size N then we can arrange an axis all the resulting estimates of the corresponding parameter.
We can then ask, what proportion of these estimates fall within some range of the true perimeter value?
For example we might ask, what proportions of the estimates are going to fall within this interval containing the true parameter value?
We can more generally ask about any possible interval.
In effect, the estimator has it's own distribution.
And understanding its accuracy is equivalent to understanding how concentrated this distribution is around the true parameter value.
One might dare ask, can we know the formula of this distribution?
Unfortunately, the sample distribution of an estimator is not always the same.
It depends on the model used, the parameter that is being estimated, as well as the method of estimation that we have chosen.
Fortunately, though, if the sample size is large enough, and of we use Maximum Likelihood as out method of estimation, then we have a pretty good approximation.
Strikingly, this approximation is actually a normal distribution.
And this result is true, regardless of the population model and regardless of the specific parameter we're interested in.
This is due to deep mathematical result called the Central Limit Theory.
The mean of this normal distribution is actually the true parameter.
Which means that on average we're getting the right answer.
The variance, on the other hand, is given by an explicit formula that depends on the likelihood function.
We are not going to discuss the precise nature of this formula, but suffices to say that this is a quantity that we can calculate for any possible population model.
What we do remark, though, is that this variance is inversely proportional to N
The larger the sample size, the smaller the variance.
In other words, the large sample sizes, the
Maximum Likelihood Estimator becomes more and more concentrated around the true parameter value.
The large majority of cases,
Maximum Likelihood is a very good approach to an estimation.
