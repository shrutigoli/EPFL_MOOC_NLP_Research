1
00:00:08,576 --> 00:00:11,626
A confidence interval gives us
a plausible range of values for

2
00:00:11,835 --> 00:00:14,448
the unknown parameter of a
population distribution

3
00:00:14,655 --> 00:00:16,842
with a guaranteed
level of confidence.

4
00:00:17,166 --> 00:00:19,728
But in some cases we may have
a theory or a conjecture

5
00:00:19,930 --> 00:00:23,054
that the parameters should be
contained in some range of values

6
00:00:23,246 --> 00:00:26,021
that we ourselves have
specified a priori;

7
00:00:26,260 --> 00:00:28,798
that is before having
collected any data.

8
00:00:29,015 --> 00:00:31,815
For example we might believe
that a certain coin 

9
00:00:32,012 --> 00:00:35,699
or a roulette wheel is fair
or we might conjecture

10
00:00:35,926 --> 00:00:38,802
that the average lifetime of a
certain brand of car battery

11
00:00:38,997 --> 00:00:41,534
is at least 4 years.
In such cases,

12
00:00:41,865 --> 00:00:44,865
rather than construct a plausible
range from the sample

13
00:00:45,104 --> 00:00:47,905
we would like to see
whether our conjectured range

14
00:00:48,102 --> 00:00:50,727
is plausible or not
based on the sample.

15
00:00:51,027 --> 00:00:53,640
In other words we would
like to use the data we collect

16
00:00:53,847 --> 00:00:58,334
in the form of a sample x1, ..., xn
to put our theory to the test.

17
00:00:58,628 --> 00:01:01,666
We wish to contrast the theory
to the evidence.

18
00:01:01,934 --> 00:01:05,122
Is the observed data coherent
with our hypothesized range?

19
00:01:06,207 --> 00:01:09,257
If the behaviour of the sample
deviates from what it should be

20
00:01:09,457 --> 00:01:14,033
under the hypothesis then we have
evidence against the hypotheses.

21
00:01:14,418 --> 00:01:17,593
Hypothesis tests are a
statistical tool that allows us to

22
00:01:17,797 --> 00:01:20,898
quantify whether the degree of
observed deviation

23
00:01:21,169 --> 00:01:24,169
is large enough to consist
a true departure from the theory

24
00:01:24,443 --> 00:01:28,505
or small enough that it is simply a
manifestation of sampling variability.

25
00:01:29,189 --> 00:01:31,951
So hypotheses assert that the
true parameters within

26
00:01:32,175 --> 00:01:36,000
some range of values. In statistical
jargon these are known as

27
00:01:36,216 --> 00:01:39,704
null hypotheses and they
are denoted by the symbol H0.

28
00:01:40,213 --> 00:01:43,501
Null hypotheses typically come
in 2 forms.

29
00:01:44,700 --> 00:01:50,649
Firstly hypotheses of the form
ø = ø0 are called simple hypothesis.

30
00:01:50,957 --> 00:01:54,569
These assert that the true
parameter value is equal to

31
00:01:54,761 --> 00:01:57,324
some predetermined
constant ø0.

32
00:01:57,812 --> 00:02:00,563
For example if we hypothesis
that a coin is fair

33
00:02:01,003 --> 00:02:03,428
this means that the
parameter corresponding to

34
00:02:03,637 --> 00:02:08,149
the probability of 'heads' is
exactly equal to 50%.

35
00:02:09,071 --> 00:02:12,596
A second form is that of
one-sided hypothesis.

36
00:02:12,805 --> 00:02:18,767
That is hypotheses of the form
Ø  ≥  Ø0 or Ø  ≤  Ø0

37
00:02:18,996 --> 00:02:21,746
for some predetermined
constant Ø0.

38
00:02:21,917 --> 00:02:25,154
These formulate upper or
lower bounds for the parameter.

39
00:02:25,437 --> 00:02:29,224
For example hypothesizing that
the mean lifetime of a battery

40
00:02:29,416 --> 00:02:33,541
is at least 4 years corresponds
to a left-sided hypothesis.

41
00:02:34,530 --> 00:02:38,130
Now the outcome of any
hypothesis test is a decision.

42
00:02:38,703 --> 00:02:41,915
Based on sample x1, ..., xn
of values from the population

43
00:02:42,164 --> 00:02:45,414
we decide whether or not
to reject the hypothesis.

44
00:02:46,120 --> 00:02:49,370
A key element in hypothesis
testing is to construct

45
00:02:49,595 --> 00:02:53,670
good rules for rejecting or not
rejecting a hypothesis.

46
00:02:53,979 --> 00:02:57,079
What is good?
Well, a good rule is one

47
00:02:57,280 --> 00:03:00,468
that limits the proportion of times
that we would produce

48
00:03:00,691 --> 00:03:04,279
a false rejection,
also known as a false positive.

49
00:03:04,670 --> 00:03:08,557
In other words, when the null
hypothesis is truly valid

50
00:03:08,746 --> 00:03:11,870
at the level of population,
our rule should produce

51
00:03:12,103 --> 00:03:16,415
a rejection decision for no more
than a small fraction

52
00:03:16,657 --> 00:03:20,357
of all possible samples.
Let's say 5%.

53
00:03:21,197 --> 00:03:23,885
This proportion is called
the significance level

54
00:03:24,070 --> 00:03:26,245
usually denoted by the
Greek letter alpha (a)

55
00:03:26,458 --> 00:03:29,270
and is most often
taken to be 5%.

56
00:03:30,128 --> 00:03:33,628
A simple but effective strategy
is to operate as follows.

57
00:03:34,147 --> 00:03:38,184
On the basis of our sample we can
construct an estimate of the true θ.

58
00:03:38,390 --> 00:03:41,739
Let's call it θ^ (x1, ..., xn)

59
00:03:42,212 --> 00:03:46,062
We can then look to see
whether the estimate θ^

60
00:03:46,355 --> 00:03:50,193
actually satisfies the
hypothesis or not.

61
00:03:50,493 --> 00:03:54,617
If it does satisfy the hypothesis,
there is no reason to reject

62
00:03:55,241 --> 00:03:59,241
but if it does not satisfy the
hypothesis we should not

63
00:03:59,435 --> 00:04:04,372
necessarily immediately reject.
The point is that the estimate

64
00:04:04,592 --> 00:04:09,717
will never be precisely right.
We must only reject if the estimate

65
00:04:09,934 --> 00:04:13,396
deviates substantially
from the null hypothesis.

66
00:04:14,194 --> 00:04:17,331
What deviations are
substantial enough though?

67
00:04:17,584 --> 00:04:20,359
One idea is to consider a deviation
to be large enough

68
00:04:20,583 --> 00:04:24,582
and thus to reject
if a 95% confidence interval

69
00:04:24,794 --> 00:04:28,481
for the parameter does not
overlap with the hypothesis.

70
00:04:28,725 --> 00:04:32,687
The rational is simple.
We know that the confidence interval

71
00:04:32,901 --> 00:04:36,775
contains the true parameter
for 95% of all samples.

72
00:04:37,165 --> 00:04:40,665
So if the null hypothesis
is indeed true,

73
00:04:40,933 --> 00:04:45,583
it will overlap with a confidence
interval for 95% of all samples.

74
00:04:46,128 --> 00:04:50,228
Equivalently if the null hypothesis
is true, we will falsely reject it

75
00:04:50,476 --> 00:04:54,475
only for 5% of all
possible samples.

76
00:04:55,001 --> 00:04:57,352
But which interval
should we use,

77
00:04:57,569 --> 00:05:00,582
a two-sided or
a one-sided interval?

78
00:05:00,874 --> 00:05:04,086
Well as you might expect
we use one-sided intervals

79
00:05:04,300 --> 00:05:08,187
for one-sided hypothesis and
we use two-sided intervals

80
00:05:08,420 --> 00:05:13,119
for simple hypothesis.
Specifically for one-sided hypotheses

81
00:05:13,480 --> 00:05:16,018
if the hypothesis asserts an
upper bound

82
00:05:16,245 --> 00:05:18,469
then we use a
lower confidence bound,

83
00:05:18,676 --> 00:05:21,726
in other words, a
left-sided confidence interval.

84
00:05:22,174 --> 00:05:25,761
On the other hand, if the hypothesis
asserts a lower bound

85
00:05:26,013 --> 00:05:28,888
then we use an
upper confidence bound,

86
00:05:29,091 --> 00:05:31,966
in other words,
a right-sided interval.

87
00:05:32,235 --> 00:05:34,910
If all this sounds complicated,
we can translate it

88
00:05:35,136 --> 00:05:37,787
to a simple list
of conditions.

89
00:05:38,120 --> 00:05:41,957
Remember the basic setup:
we reject if and only if

90
00:05:42,178 --> 00:05:46,540
our confidence interval does not
overlap with the hypothesis.

91
00:05:46,794 --> 00:05:50,543
This translates to the following
simple list of conditions:

92
00:05:50,833 --> 00:05:54,046
For simple hypothesis
we reject if and only if

93
00:05:54,272 --> 00:06:01,223
the distance of θ^ from θ0
exceeds 2σ (θ^)/√n

94
00:06:02,105 --> 00:06:05,530
For one-sided hypotheses
we reject if and only if

95
00:06:05,731 --> 00:06:11,980
θ^ falls at least
1.6 x σ (θ^)/√n on the

96
00:06:12,201 --> 00:06:16,863
opposite side of θ0 then that's
stipulated by the null hypothesis.

97
00:06:17,508 --> 00:06:20,183
In each of those cases
we are guaranteed that the level

98
00:06:20,396 --> 00:06:22,721
of significance
will be 5%.

99
00:06:23,024 --> 00:06:26,312
In other words, if the null
hypothesis is true

100
00:06:26,522 --> 00:06:32,759
we will falsely have rejected it
only for 5% of all possible samples.

101
00:06:33,790 --> 00:06:37,565
Let's go back to the example involving 
the high school seniors weight.

102
00:06:37,763 --> 00:06:41,013
We would like to test the hypothesis
that the true population mean

103
00:06:41,211 --> 00:06:46,098
is at least 150 pounds.
We have calculated the sample mean

104
00:06:46,311 --> 00:06:51,848
to be 145 pounds. Thus our
estimator deviates from what

105
00:06:52,060 --> 00:06:57,222
the hypotheses asserts. Does it
deviate substantially enough

106
00:06:57,457 --> 00:07:00,457
that we should reject
the hypothesis though?

107
00:07:01,029 --> 00:07:05,091
According to our rejection rule
since this is a one-sided hypothesis

108
00:07:05,333 --> 00:07:10,832
we should reject if the sample mean
falls short of 150 pounds

109
00:07:11,055 --> 00:07:20,267
by at least 1.6 x σ(x̅)/√n.
This is indeed the case.

110
00:07:20,466 --> 00:07:28,628
The sample mean of 145 is smaller
than the critical value of 146

111
00:07:28,868 --> 00:07:34,456
and so we must reject the null
hypothesis at the 5% significance level.

112
00:07:35,339 --> 00:07:38,152
So what is the right
significance level to choose

113
00:07:38,444 --> 00:07:45,344
in order to test a hypothesis?
5%? 2.5%? Maybe 1%.

114
00:07:45,587 --> 00:07:48,549
A lower significance level means
that we will be less likely

115
00:07:48,778 --> 00:07:53,115
to falsely reject a null hypothesis
but changing the significance level

116
00:07:53,376 --> 00:07:58,050
will change the factors 2 and 1.6
used in the formulas earlier.

117
00:07:58,609 --> 00:08:03,684
Indeed, picking a lower significance
level will inflate these factors.

118
00:08:04,084 --> 00:08:08,072
That means that a lower significance
level corresponds to being more

119
00:08:08,319 --> 00:08:11,457
conservative.
We will only reject the hypothesis

120
00:08:11,609 --> 00:08:15,583
for very large deviations because
we want a low risk

121
00:08:15,796 --> 00:08:19,995
of a false rejection.
A higher significance level

122
00:08:20,207 --> 00:08:23,607
corresponds to being more
aggressive. We are willing

123
00:08:23,830 --> 00:08:27,893
to reject the hypothesis
even for moderate deviations.

124
00:08:28,148 --> 00:08:31,036
In other words we are willing to
tolerate a higher risk

125
00:08:31,263 --> 00:08:35,463
of a false rejection.
So at the end of the day we can ask,

126
00:08:35,677 --> 00:08:38,376
how conservative
or aggressive should we be?

127
00:08:38,769 --> 00:08:42,544
Well a possible idea is to
not make that choice at all.

128
00:08:43,065 --> 00:08:47,803
For a given sample, find the
smallest possible significance level

129
00:08:48,018 --> 00:08:50,630
for which you would
reject the hypothesis

130
00:08:50,856 --> 00:08:55,681
and record it. This recorded value
is called the 'p' value.

131
00:08:55,910 --> 00:08:58,922
It tells us what is the least risk
we can commit to

132
00:08:59,155 --> 00:09:03,579
and still reject the hypothesis
on the basis of a given sample.

133
00:09:03,918 --> 00:09:07,668
A very small value for the p-value
means that even when being

134
00:09:07,892 --> 00:09:12,379
very conservative we would still
reject H0 on the basis of the data.

135
00:09:12,651 --> 00:09:15,727
This is strong evidence
against the null hypothesis.

136
00:09:16,137 --> 00:09:19,874
A larger p-value suggest that
we can reject H0

137
00:09:20,092 --> 00:09:22,868
only if we tolerate
a high level of risk.

138
00:09:23,158 --> 00:09:27,471
This does not furnish
evidence against H0.

139
00:09:27,733 --> 00:09:31,058
In general, the lower
the p-value, the stronger

140
00:09:31,260 --> 00:09:34,873
the evidence in the data
against H0.

141
00:09:35,238 --> 00:09:38,588
A very important warning has
to be issued here.

142
00:09:38,856 --> 00:09:44,768
The p-value is not the probability
of the hypothesis being true.

143
00:09:44,997 --> 00:09:47,747
The correct interpretation
is that the p-value

144
00:09:47,951 --> 00:09:51,664
is the probability of observing
a deviation at least as large

145
00:09:52,123 --> 00:09:56,435
as what we observed if the
null hypothesis were true.

146
00:09:56,721 --> 00:09:59,608
So a small p-value suggests
that we have observed something

147
00:09:59,811 --> 00:10:04,011
that would be very unlikely
under the hypothesis H0

148
00:10:04,476 --> 00:10:07,639
therefore we have evidence
against the hypothesis.

149
00:10:08,624 --> 00:10:12,099
Now we not discuss precisely how to
calculate a p-value

150
00:10:12,328 --> 00:10:16,153
but this can easily be done in all
of the settings we have treated.

151
00:10:16,738 --> 00:10:21,613
Luckily a simple rule is:
rejecting if and only if

152
00:10:21,836 --> 00:10:25,686
the p-value is smaller than a,
guarantees a level

153
00:10:25,882 --> 00:10:28,882
of significance equal
precisely to a.

154
00:10:29,213 --> 00:10:33,001
In other words, if you know you want
a level of significance of 5%,

155
00:10:33,201 --> 00:10:36,976
you can always compare the p-value
to that value of 5%

156
00:10:37,208 --> 00:10:41,821
in order to decide whether or not
you will reject the null hypothesis.

157
00:10:42,120 --> 00:10:44,407
Going back to the high school
student example,

158
00:10:44,601 --> 00:10:47,363
let's use the p-value approach
to test the null hypothesis

159
00:10:47,545 --> 00:10:52,108
that the population mean
is at least 150 pounds.

160
00:10:52,530 --> 00:10:55,455
We notice that
the sample mean is 145

161
00:10:55,668 --> 00:11:00,855
which deviates from our hypothesis.
Indeed, the sample mean is

162
00:11:01,089 --> 00:11:06,626
5 pounds less than the
hypothesized lower bound of 150

163
00:11:06,846 --> 00:11:14,433
which corresponds to just over
2σ (x̅)/√n under the bound.

164
00:11:15,237 --> 00:11:17,762
From the sampling distribution
of our estimator

165
00:11:17,971 --> 00:11:22,171
we know that if the null
hypothesis is truly correct,

166
00:11:22,445 --> 00:11:27,183
then a left-sided deviation of
this or larger magnitude

167
00:11:27,453 --> 00:11:34,140
can only arise for 2.15%
of all possible samples.

168
00:11:34,548 --> 00:11:39,686
This 2,15% is exactly
our p-value.

169
00:11:39,966 --> 00:11:44,803
It tells us that the observed
deviation would be highly unlikely

170
00:11:45,042 --> 00:11:49,017
if the null hypothesis H0
were true.

171
00:11:49,702 --> 00:11:53,927
This gives strong evidence
against H0.

172
00:11:54,133 --> 00:11:57,308
In particular, it means that we
should reject the null hypothesis

173
00:11:57,516 --> 00:12:01,316
at a 5% significance level
but also for any other

174
00:12:01,547 --> 00:12:06,585
significance level
that is higher than 2.15%.

