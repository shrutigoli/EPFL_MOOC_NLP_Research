1
00:00:08,353 --> 00:00:11,154
Recall that our whole premise
is that we will use

2
00:00:11,355 --> 00:00:13,217
a distribution of a statistical
variable

3
00:00:13,427 --> 00:00:15,802
at the level of a random sample
distribution,

4
00:00:16,010 --> 00:00:18,210
in order to understand
the distribution of this variable

5
00:00:18,423 --> 00:00:20,036
at the level of the population.

6
00:00:20,942 --> 00:00:23,367
If we have a model for
the population distribution,

7
00:00:23,633 --> 00:00:25,521
we can calculate everything
we need to know

8
00:00:25,721 --> 00:00:26,933
about a population,

9
00:00:27,190 --> 00:00:30,190
provided that we know the value
of the parameter it depends on.

10
00:00:31,242 --> 00:00:32,730
So, a natural question is,

11
00:00:32,949 --> 00:00:35,361
how can we use the sample
distribution

12
00:00:35,611 --> 00:00:39,249
in order to infer the unknown
parameter of the population model?

13
00:00:39,973 --> 00:00:43,423
This problem is known as
the problem of point estimation.

14
00:00:43,764 --> 00:00:45,201
So let's say we have a model,

15
00:00:45,418 --> 00:00:48,368
F x Theta, for our population,

16
00:00:48,604 --> 00:00:51,829
which has some explicit formula
that we omit for the moment.

17
00:00:52,531 --> 00:00:54,368
Though we know the formula
of the model,

18
00:00:54,739 --> 00:00:57,052
we don't know the value
of the true parameter Theta.

19
00:00:57,413 --> 00:01:01,264
For example, we know that the model
is exponential with rate Theta,

20
00:01:01,544 --> 00:01:03,719
but we don't know
the precise value of the rate.

21
00:01:04,563 --> 00:01:05,839
We have, however,
at our disposal

22
00:01:06,055 --> 00:01:08,930
a sample of X1, Xn
from the population.

23
00:01:09,855 --> 00:01:12,168
In order to estimate the unknown
perimeter Theta,

24
00:01:12,446 --> 00:01:14,096
we will construct an estimator.

25
00:01:14,555 --> 00:01:18,080
A function Theta hat of the sample
values X1 to Xn,

26
00:01:18,877 --> 00:01:21,915
who's value will provide our
estimate of the unknown parameter.

27
00:01:22,713 --> 00:01:24,688
If only 2 questions naturally arise,

28
00:01:25,187 --> 00:01:27,600
one, how do we construct
estimators?

29
00:01:27,968 --> 00:01:30,593
In other words, what is the right
formula for Theta hat?

30
00:01:31,528 --> 00:01:34,790
And two, how de we evaluate
the precision of estimators?

31
00:01:35,713 --> 00:01:37,463
We will first start with question 1,

32
00:01:37,702 --> 00:01:39,827
and then eventually
move on to question 2.

33
00:01:40,604 --> 00:01:43,204
What ever form of estimator
we end up using,

34
00:01:43,518 --> 00:01:46,318
the motivating principal
always remains the same.

35
00:01:46,564 --> 00:01:49,476
We wish to calibrate the
population distribution

36
00:01:49,745 --> 00:01:51,320
to the sample distribution.

37
00:01:51,546 --> 00:01:52,284
For example,

38
00:01:52,523 --> 00:01:55,273
on the right you can see
a sample histogram in red,

39
00:01:55,532 --> 00:01:58,682
and different possible exponential
distributions models in blue.

40
00:01:59,127 --> 00:02:02,539
Each one corresponds to a different
choice of the rate parameter.

41
00:02:03,114 --> 00:02:04,464
Which one should we choose

42
00:02:04,709 --> 00:02:06,797
 to arrive at the best possible
agreement

43
00:02:07,043 --> 00:02:09,468
between the sample and
the population?

44
00:02:10,117 --> 00:02:12,967
This calibration is always
with respect on criteria.

45
00:02:13,216 --> 00:02:16,179
For instance, we might ask
that the parameter of interest

46
00:02:16,463 --> 00:02:19,150
should be such that the mean
of the population model,

47
00:02:19,384 --> 00:02:22,171
and the mean of the
sample distribution coincide.

48
00:02:22,750 --> 00:02:26,000
This leads to what we call
the Method of Moments Estimator.

49
00:02:26,566 --> 00:02:29,266
Another approach is to ask
for a finer calibration.

50
00:02:29,681 --> 00:02:31,756
We might want to ask
what is the parameter value

51
00:02:31,972 --> 00:02:34,547
that makes the sample as
representative as possible

52
00:02:34,961 --> 00:02:36,598
for the corresponding model?

53
00:02:37,182 --> 00:02:40,182
This leads to what we call
The Maximum Likelihood Estimator.

54
00:02:40,863 --> 00:02:43,625
If the population model depends
on the parameter Theta,

55
00:02:44,395 --> 00:02:47,570
then the mean of the population will
also be a function of that parameter

56
00:02:47,965 --> 00:02:49,752
Let's call it M of Theta.

57
00:02:50,253 --> 00:02:53,253
Now, if we have a random sample
X1 of  Xn,

58
00:02:53,974 --> 00:02:56,049
we can also calculate
the sample mean.

59
00:02:56,354 --> 00:02:57,804
Let's call it X bar.

60
00:02:58,262 --> 00:03:00,012
What the method
of moments suggests

61
00:03:00,306 --> 00:03:02,269
is to find the value of Theta,

62
00:03:02,657 --> 00:03:05,433
that yields a population means
M of Theta,

63
00:03:05,963 --> 00:03:09,587
that matches the sample mean
X bar as best as possible.

64
00:03:10,325 --> 00:03:13,463
Equivalently, we are asked to solve
the equation M of Theta

65
00:03:13,739 --> 00:03:17,751
is equal to 1 over N, the sum of X1
up to Xn,

66
00:03:18,039 --> 00:03:19,427
with respect to Theta.

67
00:03:19,774 --> 00:03:20,999
The resulting solution,

68
00:03:21,274 --> 00:03:24,274
is called, The Method
of Moments Estimator of Theta.

69
00:03:25,213 --> 00:03:27,463
As an example, let's consider
a population

70
00:03:27,677 --> 00:03:30,677
that we choose to model
using the exponential distribution.

71
00:03:31,086 --> 00:03:33,949
The diagram on the right,
shows a histogram in red.

72
00:03:34,399 --> 00:03:37,061
This corresponds to a sample
of size 40

73
00:03:37,302 --> 00:03:38,839
that we've drawn from
the population.

74
00:03:39,666 --> 00:03:42,154
As we said, we model
this population

75
00:03:42,370 --> 00:03:44,257
by the exponential distribution.

76
00:03:44,717 --> 00:03:47,380
This depends on one
unknown perimeter Theta,

77
00:03:47,693 --> 00:03:49,293
which is called the rate perimeter.

78
00:03:50,211 --> 00:03:53,424
In blue you can see overlaid
several curves,

79
00:03:53,744 --> 00:03:56,181
that corresponds to different
exponential distributions

80
00:03:56,391 --> 00:03:58,154
with different choices of Theta.

81
00:03:58,967 --> 00:04:02,417
Which one should we choose
to best match the sample?

82
00:04:03,259 --> 00:04:04,659
The method of moment says,

83
00:04:04,912 --> 00:04:07,800
that we should first of all look
at the sample mean.

84
00:04:08,380 --> 00:04:10,980
This is located where this
vertical line is

85
00:04:11,268 --> 00:04:15,243
and is actually equal to 0,938.

86
00:04:17,067 --> 00:04:20,242
The method of moment then suggests
that of all these blue curves,

87
00:04:20,491 --> 00:04:23,366
we should select the one
that has population means

88
00:04:23,702 --> 00:04:25,789
equal to 0.938.

89
00:04:26,545 --> 00:04:30,970
This specific curve is given
in grey.

90
00:04:31,641 --> 00:04:34,904
So what is the precise value of Theta
that corresponds to the grey curve?

91
00:04:35,914 --> 00:04:38,214
Well, the mean of the exponential
distribution

92
00:04:38,501 --> 00:04:39,976
is the inverse of it's perimeter.

93
00:04:41,753 --> 00:04:44,140
It follows that the method
of moments estimator

94
00:04:44,400 --> 00:04:47,387
should solve the equation that says
that the population mean

95
00:04:47,608 --> 00:04:49,133
should be equal to the
sample mean.

96
00:04:49,727 --> 00:04:52,590
Solving this, gives us
an estimate for Theta

97
00:04:52,847 --> 00:04:57,009
which we call Theta Hat,
and equals 1 over X bar.

98
00:04:57,417 --> 00:05:00,942
The precise value is 1 divided by
0.938

99
00:05:01,188 --> 00:05:04,325
which gives is 1.066.

100
00:05:05,721 --> 00:05:07,658
The method of moments
chooses a parameter

101
00:05:07,913 --> 00:05:11,550
to match the sample population
location as expressed by the mean.

102
00:05:12,192 --> 00:05:14,155
Another approach would be
to choose a parameter

103
00:05:14,366 --> 00:05:17,378
to match the sample and population
distribution at a finer level.

104
00:05:18,304 --> 00:05:21,229
A choice such that the sample
is as representative

105
00:05:21,466 --> 00:05:23,379
as possible of the population.

106
00:05:24,279 --> 00:05:27,216
Equivalently, this requires
the proportions calculated

107
00:05:27,469 --> 00:05:29,969
from the sample histogram
match proportions

108
00:05:30,178 --> 00:05:33,178
calculated from the population
curve as best as possible.

109
00:05:34,242 --> 00:05:36,542
It turns out that there is a
natural mathematical way

110
00:05:36,799 --> 00:05:38,749
to choose the best such Theta.

111
00:05:39,018 --> 00:05:41,106
And this is called, The Method
of Maximum Likelihood.

112
00:05:41,528 --> 00:05:45,190
It says that we should choose
the Theta that maximizes

113
00:05:45,423 --> 00:05:47,161
the function called
The Likelihood Function.

114
00:05:47,408 --> 00:05:48,496
L of Theta.

115
00:05:48,952 --> 00:05:51,602
This is equal to the product
of model function

116
00:05:52,147 --> 00:05:54,484
each evaluated at one
of the sample values.

117
00:05:54,714 --> 00:05:57,476
It actually has an intuitive
interpretation.

118
00:05:57,938 --> 00:06:00,901
It is the parameter choice yielding
the population model

119
00:06:01,149 --> 00:06:05,474
under which our random sample
would've been most probable to obtain.

120
00:06:06,641 --> 00:06:08,641
It often happens that the
Method of Moments

121
00:06:08,858 --> 00:06:11,758
yields the same estimator as
the Method of Maximum Likelihood.

122
00:06:12,593 --> 00:06:15,593
This will be the case, for example,
in the normal exponential

123
00:06:15,822 --> 00:06:17,122
and plus on models.

124
00:06:17,918 --> 00:06:20,656
But it can also happen that the
two estimation methods

125
00:06:20,865 --> 00:06:22,878
product different estimators

126
00:06:23,081 --> 00:06:25,118
in the same model,
for the same sample.

127
00:06:26,161 --> 00:06:27,786
So, one might naturally ask

128
00:06:28,212 --> 00:06:29,525
which one should we use?

129
00:06:30,091 --> 00:06:32,304
Well, the method of moments
is typically simpler

130
00:06:32,732 --> 00:06:33,895
and easier to apply.

131
00:06:34,190 --> 00:06:36,865
so that can be an advantage
in complicated situations.

132
00:06:37,516 --> 00:06:38,891
On the other hand,

133
00:06:39,194 --> 00:06:40,981
Maximum Likelihood is usually
preferred,

134
00:06:41,250 --> 00:06:44,513
because it makes more
efficient use data at hand.

135
00:06:44,750 --> 00:06:47,975
Many cases in fact it makes
the most efficient use of the data.

136
00:06:48,837 --> 00:06:51,012
Another advantage of
Maximum Likelihood

137
00:06:51,535 --> 00:06:54,822
is that we have some rather general
formulas to describe it's accuracy.

138
00:06:55,807 --> 00:06:58,083
Given an estimator of a
population parameter,

139
00:06:58,308 --> 00:07:00,983
it is important to have
an understanding of it's precision.

140
00:07:01,741 --> 00:07:05,266
While the value of an estimator,
which we call The Estimate,

141
00:07:05,496 --> 00:07:07,109
is constant for a given sample,

142
00:07:07,607 --> 00:07:09,582
it will vary over different samples.

143
00:07:09,965 --> 00:07:12,477
So, for the first sample
we might get an estimate

144
00:07:12,730 --> 00:07:14,117
at a short of the truth.

145
00:07:14,806 --> 00:07:15,968
For the second sample,

146
00:07:16,231 --> 00:07:18,831
we might get an estimate that is
even shorter of the truth.

147
00:07:19,128 --> 00:07:23,241
While for the third sample, we
might slightly overestimate the truth.

148
00:07:23,936 --> 00:07:25,736
The moral of the story is,

149
00:07:25,987 --> 00:07:29,537
every time we collect a new sample
we get a new estimate.

150
00:07:30,340 --> 00:07:32,540
To understand the accuracy
of our estimator

151
00:07:32,789 --> 00:07:35,227
we must thus understand
the fluctuations of our estimate

152
00:07:35,465 --> 00:07:37,515
around the value of the
true parameter

153
00:07:37,815 --> 00:07:42,003
as the sample varies over all
possible choices over the population.

154
00:07:43,104 --> 00:07:46,929
If we consider all possible
samples of size N

155
00:07:47,853 --> 00:07:51,328
then we can arrange an axis
all the resulting estimates

156
00:07:51,617 --> 00:07:53,192
of the corresponding parameter.

157
00:07:53,716 --> 00:07:55,053
We can then ask,

158
00:07:55,461 --> 00:07:58,373
what proportion of these
estimates fall within some range

159
00:07:58,571 --> 00:07:59,809
of the true perimeter value?

160
00:08:00,251 --> 00:08:01,813
For example we might ask,

161
00:08:02,044 --> 00:08:05,231
what proportions of the estimates
are going to fall within this interval

162
00:08:05,514 --> 00:08:07,151
containing the true parameter value?

163
00:08:08,410 --> 00:08:10,885
We can more generally ask about
any possible interval.

164
00:08:12,055 --> 00:08:16,205
In effect, the estimator has
it's own distribution.

165
00:08:17,365 --> 00:08:21,103
And understanding its accuracy
is equivalent to understanding

166
00:08:21,394 --> 00:08:25,032
how concentrated this distribution
is around the true parameter value.

167
00:08:25,611 --> 00:08:26,999
One might dare ask,

168
00:08:27,740 --> 00:08:30,503
can we know the formula
of this distribution?

169
00:08:31,580 --> 00:08:34,417
Unfortunately, the sample
distribution of an estimator

170
00:08:34,658 --> 00:08:35,983
is not always the same.

171
00:08:36,496 --> 00:08:37,971
It depends on the model used,

172
00:08:38,223 --> 00:08:39,760
the parameter that is
being estimated,

173
00:08:39,967 --> 00:08:42,392
as well as the method of estimation
that we have chosen.

174
00:08:42,932 --> 00:08:44,120
Fortunately, though,

175
00:08:44,467 --> 00:08:46,204
if the sample size is large enough,

176
00:08:46,422 --> 00:08:49,448
and of we use Maximum Likelihood
as out method of estimation,

177
00:08:49,998 --> 00:08:52,036
then we have a pretty good
approximation.

178
00:08:52,798 --> 00:08:56,423
Strikingly, this approximation
is actually a normal distribution.

179
00:08:57,122 --> 00:09:00,385
And this result is true, regardless
of the population model

180
00:09:00,598 --> 00:09:03,311
and regardless of the specific
parameter we're interested in.

181
00:09:03,901 --> 00:09:07,926
This is due to deep mathematical
result called the Central Limit Theory.

182
00:09:08,692 --> 00:09:12,217
The mean of this normal distribution
is actually the true parameter.

183
00:09:12,595 --> 00:09:15,182
Which means that on average
we're getting the right answer.

184
00:09:16,125 --> 00:09:19,125
The variance, on the other hand,
is given by an explicit formula

185
00:09:19,378 --> 00:09:21,178
that depends on
the likelihood function.

186
00:09:21,521 --> 00:09:24,184
We are not going to discuss
the precise nature of this formula,

187
00:09:24,406 --> 00:09:27,268
but suffices to say that this is
a quantity that we can calculate

188
00:09:27,501 --> 00:09:29,764
for any possible population model.

189
00:09:30,832 --> 00:09:32,120
What we do remark, though,

190
00:09:32,584 --> 00:09:35,259
is that this variance is inversely
proportional to N

191
00:09:35,826 --> 00:09:38,564
The larger the sample size,
the smaller the variance.

192
00:09:39,132 --> 00:09:40,169
In other words,

193
00:09:40,480 --> 00:09:43,343
the large sample sizes, the
Maximum Likelihood Estimator

194
00:09:43,592 --> 00:09:46,954
becomes more and more concentrated
around the true parameter value.

195
00:09:47,497 --> 00:09:50,334
The large majority of cases,
Maximum Likelihood

196
00:09:50,558 --> 00:09:52,971
is a very good approach
to an estimation.

