Welcome to the third and last video of Week
5 of this introductory course on synchrotrons and x-ray free-electron lasers!
In this video, I will introduce you to aspects of x-ray detector science and technology.
What do we want an x-ray detector to do? It needs to be able to detect x-rays and convert this captured signal into a form which can be subsequently read out. The first such detector was the photographic plate. More recently, conversion of x-rays to visible light via scintillator or phosphor screens meant that the signal could be detected by scientific devices used also for optical spectroscopy in the visible and near visible regime. Most recently, CCDs and hybrid pixel detectors allow direct detection of x-rays via the production of electrons in the x-ray absorption process.
An ideal detector would be able to resolve the spatial distribution of the x-ray signal, have a high quantum (or capture) efficiency, deliver signal strengths directly proportional to the x-ray intensity, be able to record both weak and strong signal simultaneously
(that is, have a high dynamic range), record data rapidly, and be devoid of internal artefacts.
Before describing different detector types and technology in detail, we first cover the topic of noise. Any stochastic or random process composed of a series of discrete events, such as the arrival of x-ray photons on to a detector element, is subject to statistical, or â€˜Poissonâ€™ noise. If the average number of events in a measurement (say, the number of photons recorded in a detector pixel after a fixed length of exposure time) is N, then the scatter in the data (or, more precisely, the standard deviation) is +/- N^1/2. So, for example, if the average intensity of a Bragg peak maximum on average after many identical measurements is 10000 counts, approximately 68 % of the results will lie within sigma = +/- 100 counts of this value. However, for a static (time independent) experiment, the signal strength itself increases linearly with exposure time.
A consequence of this is that in order to achieve a factor X improvement in the signal-to-noise ratio âˆ†N/N, one must record X^2 times as long. The careful experimenter must therefore decide how best to invest his or her precious beamtime â€“ does improving the signal-to-noise ratio of a given signal by, say, a factor of two, justify the necessary fourfold increase in acquisition times, or would this time be better invested in recording other data points?
Consider the simulated images shown here.
The numbers correspond to the average number of photons/pixel across each image, which have been scaled linearly from black to white.
The insets are blowups of the most prominent crater in the image, close to the edge of the day/night shadow. Note that the image quality improves significantly between the first and fourth image (corresponding to an increase in exposure time by a factor of 100), while subsequent increases in exposure time by first another factor of 25, and then a factor of 16 (that is, a factor of 400 with respect to the fourth image) reap only dubious benefits.
What we have described so far is called shot noise - fluctuations associated with the signal itself. Most detectors also have another source of noise called dark noise, which occurs even when there is no incident signal. For example, photosensitive diodes such as CCD pixel elements exhibit dark noise, caused by crystallographic defects in the depletion region of the diode device. The level of dark noise can thus vary from pixel to pixel, depending on the defect concentration in each element. A dark-field profile can thus be generated by recording in the dark sufficiently long to produce a statistically reliable response array. It is noted that some modern area detectors have effectively zero dark noise on account of the fact that they are not based on diode technologies.
Readout noise is a source of spurious signal in CCD detectors produced in the on- chip amplifier used to convert the collected charge produced by absorption of x-rays to a voltage
(via a capacitor) in the readout process after the exposure. This becomes particularly critical when reading out very weak signals.
In general, a measured intensity I will be composed of the signal of interest, S, plus a background signal, B, which may be dark noise, but might also be a real background associated with the experiment. The noise associated with I, S, and B, are all the square-root of those values. If, then, the noise associated with the total signal I is larger than that associated with the signal of interest, i.e., the square-root of S, then the signal will be lost in the noise. Two approaches to resolving this are (a) reduction of B if at all possible
- perhaps it is caused by fluorescence of diffuse scattering of the incident beam from a chamber wall - in which case measures can be taken to remove, or at least, reduce this.
If, however, the background is either intrinsic or impossible to remove, the exposure needs to be increased until the noise becomes smaller than S.
Another important property of detectors which can spatially resolve the signal is that of the point-spread function, or PSF. If a detector element has a spatial response function which is not infinitely narrow, then the spatial distribution of any signal, as recorded on the detector, will be a convolution of the original signal with the form of the detector-element response function. So if the element were used to detect a signal of zero width (or for practical purposes, a width known to be much smaller than the element size), then the recorded signal is the detectorâ€™s point-spread function - the name is self-explanatory - a point is spread to a finite-sized function.
The minimum point=spread function of a 2D pixel array is the pixel size, as within a given pixel, any spatial variation is lost.
Before discussing more modern technologies, some older solutions are mentioned here, as they are still used extensively, if nowadays more for diagnostic purposes than for actual experiments.
Photographic plates were used to spatially resolve x-ray signal for many decades until the advent of image plates, and then CCDs.
They have many drawbacks, including difficulty in obtaining quantitative relative intensities due primarily to a nonlinear change in transparency of the photographic plate with signal intensity; a poor dynamic range; poor spatial resolution; and the very long readout times.
Scintillators, such as Ce:YAG, convert x-rays to visible light by absorbing them, and, via a dopant rapid relaxation through non radiative processes to low-lying energy levels, which only then relax by emission of a visible or near-visible photon. Scintillators can then be installed in front of a photomultiplier tube, which detects the visible light and produces a signal through an avalanche process in a cascade of dynodes.
Such a detector has no spatial resolution, other than that provided by slitting down the response area. This is a so-called point detector.
In the central image, both a point detector and a 1-dimensional are shown . In traditional x-ray powder diffraction experiments, the
0-D point detector is scanned over an angular range, which may only cover a few degrees, but may also be several tens of degree, and require scanning times of several minutes to several hours. The 1-D detector shown here, called Mythen, consists of an array of 30â€™720 strip detector elements, each covering an angle of 0.004 degrees. Acquisition is in parallel, and takes several orders of magnitude less time to record data. This has revolutionised powder-diffraction experiments at synchrotrons, allowing time-resolved studies that were utterly unthinkable before its conception.
Two-dimensional photon detectors are commonplace at synchrotrons, not least at macromolecular crystallography beamlines, such as shown here in the right hand image.
Even larger angular ranges still are now accessible, such as shown here for the 12-million-pixel
Pilatus detector developed and custom built for low-energy protein crystallography experiments at the I23 beamline at the Diamond Light Source.
Two different approaches to recording x-ray intensities are summarised in this slide, those of photon counting and integrating detectors.
Imagine a train of photons incident on a detector.
There are two different photon energies, due, for example, elastic scattering of the incident beam, and lower-energy fluorescence signal at characteristic energies. These two different photon energies are shown here as spikes of two different heights.
The philosophy behind photon-counting detectors is shown on the left. Each detected photon produces a short-lived voltage spike in a solid-state amplifier, the spike height being proportional to the photon energy. Internal electronics allow one to distinguish between these photon energies by setting a threshold voltage and comparing this to that produced by the absorbed photon. If the comparator registers that the photon spike is larger than the threshold, a digital one is generated , and sent to a counter. If the photon produces a spike smaller than the threshold, no signal is registered. This feature is greatly advantageous in minimising fluorescence background in some diffraction experiments.
CCDs and other integrating devices operate by accumulating charge each time a photon is detected. So the photons are not read off one by one, but instead, the total generated charge in each pixel at the end of the exposure is converted to a digital signal using an analog-to-digital converter (ADC). The amount of added charge for each incident photon is proportional to the photon energy.
Letâ€™s look at CCDs a little more closely, as they remain the most common modern area detector type. They have proved to be so important and ubiquitous in both science and society at large (there is one in every modern mobile phone) that their inventors Willard Boyle and George Smith would win the Nobel Prize in physics in 2009.
One can think of CCDs as arrays of buckets, buckets that collect electrical charge. As already mentioned, the amount of accumulated charge due to absorption of photons during the exposure time is proportional to each photonâ€™s energy and how many have been detected.
After the end of the exposure, this total â€œintegratedâ€� accumulated charge is read off.
Shown here is a cartoon of a 1-D array of
CCD â€œbucketsâ€�. After the exposure , the charge in each bucket along the row is read off one after the other by shifting the charge one bucket at a time towards the read-out
ADC. A CCD is therefore a â€œshift registerâ€� in the jargon of semiconductor electronics.
Associated with this shifting of charge and reading out is a readout noise, which may be significant.
The analogy of a CCD pixel being like a bucket extends to the fact that, just as a bucket can carry only a finite volume of water, a
CCD pixel can accommodate only a limited electrical charge. If, during the exposure, a CCD pixel is exposed to too many photons, the excess charge spills over into adjacent buckets - this undesirable artefact is called â€œbloomingâ€�, which is particularly observed in the readout direction, resulting in streaks of saturated signal. Blooming is especially troublesome in CCDs for experiments where weak (but important) signal lies close to much stronger signal.
Photon-Counting devices have their own limitations.
The spikes generated in the amplifier decay again in a finite time of the order of 100 ns *****. If two photons arrive within this time, the second of these will not be registered.
But synchrotron pulses from the direct but monochromatized direct beam contain approximately
3 x 10^4 photons over 30 ps. The average temporal separation between photons is therefore only about 1 fs, some 10^8 times shorter than the recovery, or â€œdeadâ€�, time of the spikes.
So, for example, if the entire direct beam is incident on just one detector pixel, it is necessary to attenuate this using filters by a factor of approximately 10^8 in order to record this with any accuracy.
The situation is in fact still more serious, as the arrival rate of photons is not perfectly constant, but in fact stochastic. So although the average rate of arrival may be N_0, this will fluctuate strongly. One can correct for the observed intensity, or number of photon counts, N_obs, for this statistical fluctuation using the equation shown here and plotted on the right-hand side. For average incident rates equal to the reciprocal of the dead time, tau, the curve of N_obs has a gradient of zero. Hence, in order to ensure one remains to the left of this, the real arrival rate
N_0 should, as a rule of thumb, not exceed
1/(2 tau).
The relative performance of photon-counting and integrating CCD detectors is shown here.
First, because photon counters count photons, they have an intrinsic dead time. Integrating detectors do not suffer from this limitation, an important aspect we discuss in just a moment.
In most other aspects, including the dynamic range, readout times, noise, photon-energy discrimination, and the point-spread function, photon counters perform better than integrators.
However, CCDs can presently provide smaller pixel sizes by an order of magnitude.
But let us return to the issue of the dead time of photon counters in the context of
XFELs. The instantaneous arrival rate of photons within XFEL pulses is 11 orders of magnitude higher still than at synchrotrons. This would therefore require attenuation by 19 orders of magnitude of the XFEL direct beam in order for it to be detectable using photon-counting technology, which would completely obviate one of the primary features of XFEL radiation.
Photon counters cannot be used at XFELs, and integrating devices must instead be employed, ones with high sensitivity and a large dynamic range.
With this in mind, the Jungfrau detector has been recently developed. Although it is an integrating device, it is based on the same pixel technology of the photon counting Eiger detector. The required high dynamic range and sensitivity is achieved by the Jungfrau automatically changing its sensitivity according to whether the signal is weak or strong.
This begs the question, how does this work?
No electronics is capable of reacting within the few tens of fs of XFEL pulses. The answer is that a buffer layer on top of the Jungfrau pixels first absorbs the photons , after which the generated electrons require of the order of 20 ns to drift across to the pixels themselves, plenty enough time for modern electronics to respond.
How this is done exactly is shown here. As the charge accumulates in a first capacitor, the voltage increases. Once this reaches a certain threshold, a switch is closed to include a second capacitor in parallel. The total capacitance of individual capacitors wired in parallel is simply their sum, hence by adding the second capacitor, the capacitance increases, and the voltage drops according to V = Q/C. A third range of charge is accessible by the inclusion of a third capacitor.
Until now, we have discussed (if somewhat implicitly) mainly experiments in which one photon energy is of interest, as is normally the case in scattering and diffraction experiments.
In many spectroscopic experiments, not least in x-ray fluorescence, the spectrum emitted by an irradiated sample is of interest, for which one requires a dispersive setup. One approach, called wavelength dispersive spectrometry
(or WDX) is to scan the energy using a crystal and exploiting Braggâ€™s law - the crystal is rotated and the diffracted intensity measured using a point detector rotating at twice the crystal rotating rate. This approach is slow and with low sensitivity, as only a small angular range of the emitted radiation is captured. Its one advantage is that it has a high resolution.
In energy-dispersive spectrometry (EDX), the photon energies are converted into voltage spikes with amplitudes proportional to the photon energy within doped semiconductor devices.
Different energies can be recorded in parallel, and the detector can capture a large angular cross-section. EDX is thus cheap, fast, and highly sensitive, though it has a resolution two orders of magnitude worse than in WDX.
A comparison between EDX and WDX spectra for a clay mineral shows this dramatic difference.
A nice example of EDX is shown here of a fossil fish excavated from the Green Rive formation of Fossil Lake in Wyoming. The sample was irradiated with focussed 17.2 keV x-rays and the fluorescence signal recorded as the fossil sample was rastered.
Shown here is the result of selecting a narrow section of the EDX spectra at 12.97 keV, corresponding to the thorium L-alpha fluorescence signal.
The high levels of thorium are are caused by its substitution for calcium in the phosphate mineral that composes the bone, fluorapatite.
Lastly, a modern alternative to EDX and WDX detectors combines some of the most attractive features of both. Von Hamos spectrometers operate by collecting a large angular cross-section of fluorescence on a cylindrically bent single crystal. The bend radius, R_b, has its origin on the same axis containing the sample and a 1-D or 2-D detector. This configuration means that von-Hamos detectors have a high sensitivity, while the use of a 1- or 2-D detector allows parallel acquisition of the entire spectrum. These detectors are being used increasingly for high-resolution and time-resolved x-ray emission studies, not least at XFELs.
In summary, this last video of Week 5 has covered some basic but important aspects of different types of x-ray detectors to complete our discussion of beamlines and optics.
In these first five weeks, we have learned how x-rays interact with material, how synchrotron radiation is generated and what its properties are, and in this most recent week, how this is subsequently manipulated for synchrotron-based experiments. In the next three weeks, we will discuss aspects of synchrotron experiments, including scattering and diffraction, x-ray spectroscopies, and x-ray imaging. We begin next week with x-ray diffraction and scattering.
