{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Goal: To convert all the .srt to .txt for all the english files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before even converting the files, I had to run this bash script \n",
    "for f in *\\ *; do mv -- \"$f\" \"${f// /_}\"; done\n",
    "to clean up the spaces in the data and had to manually change the few en.srt files that was not formatted correctly. Note if this project were to be continued, to use the dataset_epfl that I have here\n",
    "\n",
    "Note to self: clean up POCS,Ville,Safewater folder - change to en.srt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am creating a new folder for all the english subtitles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of the directory /Users/shrutigoli/EPFL_MOOC_NLP_Research/updated_dataset_epfl failed\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/shrutigoli/EPFL_MOOC_NLP_Research/updated_dataset_epfl\"\n",
    "try:  \n",
    "    os.mkdir(path)\n",
    "except OSError:  \n",
    "    print (\"Creation of the directory %s failed\" % path)\n",
    "else:  \n",
    "    print (\"Successfully created the directory %s \" % path)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I am parsing through all the directories/subdirectories/files and filtering only the files with english subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fileslist = []\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "    for d in dirs:\n",
    "        fileslist.append(os.path.relpath(os.path.join(root, d), \".\"))\n",
    "    for f in files:\n",
    "        fileslist.append(os.path.relpath(os.path.join(root, f), \".\"))\n",
    "    #print(root)\n",
    "    if root.endswith(\"en.srt\") or root.endswith(\"ENGL.srt\") or root.endswith(\"EN.srt\") :\n",
    "        !python srt_to_text.py $root cp1252"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I attempted to convert the english .srt files to .txt files using the srt_to_text.py converter I found online (details attached in that .py file). I noticed not all of the files converted, the ones listed below with comments below. I had to go through the folders and convert these english .srt files by running the below commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#astro, safewater, BIOIMG, Brain, Electronique_I/Annexe, Electrotechnique/Semestre01/Final/,\n",
    "#Electrotechnique/Semestre01/Transifex/ENGL/ , same for the semester2 <--\n",
    "#Emergency/First_batch (en_Goodman.srt)\n",
    "#Espace/ENGL/Session1 <-- all Sessions1-9\n",
    "#Fluids/Teaser <-- all folders in here \n",
    "#Innov/WEEK_01/ <-- all folders 1-6\n",
    "#Market/Week01 \n",
    "#MEMs/W01/ <-- all of them 1-6\n",
    "#/OOP_CPP/ENGL/W07/ <-- all of them 1-7\n",
    "#/OOP_JAVA/ENGL/W07/ <-- all of them 1-7\n",
    "#POCS/W11/ <-- All of them\n",
    "#Risk/W07 <-- all of them\n",
    "#Sanitation/W05/ <-- all of them\n",
    "#Space/W07/ <-- all of them\n",
    "#Ville/W12/ <-- all of them\n",
    "#/WHO/W05/ <-- all of them 1-5\n",
    "\n",
    "a = '/Users/shrutigoli/EPFL_MOOC_NLP_Research/dataset_epfl/WHO/W05/'\n",
    "for filename in os.listdir(a):\n",
    "    b = a + filename\n",
    "    if b.endswith(\"en.srt\") or b.endswith(\"ENGL.srt\") or b.endswith(\"EN.srt\") :\n",
    "        !python srt_to_text.py $b cp1252"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have filtered out the .txt file names and paths into a list fileslist2 to identify the files we will be using for the rest of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SameFileError",
     "evalue": "'updated_dataset_epfl/13-Coupon_bonds_and_interest_rate_swaps-en.txt' and '/Users/shrutigoli/EPFL_MOOC_NLP_Research/updated_dataset_epfl/13-Coupon_bonds_and_interest_rate_swaps-en.txt' are the same file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSameFileError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4b5770f7b9be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdest_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/shrutigoli/EPFL_MOOC_NLP_Research/updated_dataset_epfl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileslist2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \"\"\"\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_samefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mSameFileError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{!r} and {!r} are the same file\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSameFileError\u001b[0m: 'updated_dataset_epfl/13-Coupon_bonds_and_interest_rate_swaps-en.txt' and '/Users/shrutigoli/EPFL_MOOC_NLP_Research/updated_dataset_epfl/13-Coupon_bonds_and_interest_rate_swaps-en.txt' are the same file"
     ]
    }
   ],
   "source": [
    "fileslist2 = [k for k in fileslist if '.txt' in k]\n",
    "dest_dir = '/Users/shrutigoli/EPFL_MOOC_NLP_Research/updated_dataset_epfl'\n",
    "for filename in fileslist2:\n",
    "    shutil.copy(filename, dest_dir)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the fileslist that appends every folder/file and saves this into the list, fileslist2 filters out the 3794 files we will be conducting the NLP analysis on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18218\n",
      "3794\n"
     ]
    }
   ],
   "source": [
    "print(len(fileslist))\n",
    "print(len(fileslist2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#directory = '/Users/shrutigoli/Masters_Research/updated_dataset_epfl'\n",
    "#print(os.listdir(directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the files are converted into txt, we have to remove stopwords/lemmatize and clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (fileslist2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk \n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_epfl/Neuron/2.5.2_Understanding_neuronal_morphologies_using_NeuroM_en.txt\n"
     ]
    }
   ],
   "source": [
    "#Creating a dataframe to extract all the important words for the relevant .txt file\n",
    "df = pd.DataFrame.from_dict(fileslist2)\n",
    "df.columns = ['file_path']\n",
    "df.head()\n",
    "#print(df.shape[0]) #3794\n",
    "print(df['file_path'][77])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "df_key = pd.DataFrame(columns=['key_words'])\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [word for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "i = 75\n",
    "count_row = df.shape[0]\n",
    "for i in range(76):\n",
    "    txt_path = df['file_path'][i]\n",
    "    data=pd.read_csv(txt_path, delimiter=\"\\t\")\n",
    "\n",
    "    data.columns = ['body_text']\n",
    "    data['body_text_nostop'] = data['body_text'].apply(lambda x: clean_text(x.lower()))\n",
    "    \n",
    "    #running a for loop to combine all the no stop words into one list \n",
    "    mergedlist = []\n",
    "    for j in range(data.shape[0]): \n",
    "        mergedlist = list(set(data['body_text_nostop'][j] + mergedlist ))\n",
    "\n",
    "        mergedlist = [x.strip('') for x in mergedlist] #supposed to take out spaces but not working \n",
    "\n",
    "    df_key.loc[j, 'key_words'] = mergedlist \n",
    "\n",
    "#df_key.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n"
     ]
    }
   ],
   "source": [
    "count_row = data.shape[0]\n",
    "print(count_row)\n",
    "\n",
    "mergedlist = []\n",
    "for i in range(count_row): \n",
    "    mergedlist = list(set(data['body_text_nostop'][i] + mergedlist ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[brain, measurements, lectures, test, simulate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[brain, measurements, lectures, test, simulate...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   A\n",
       "1  [brain, measurements, lectures, test, simulate...\n",
       "2  [brain, measurements, lectures, test, simulate..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfk2 = pd.DataFrame(columns=['A'])\n",
    "mergedlist2 = mergedlist\n",
    "mergedlist2.append('cool')\n",
    "dfk2.at[1, 'A'] = mergedlist\n",
    "dfk2.at[2, 'A'] = mergedlist2\n",
    "\n",
    "\n",
    "\n",
    "#ml3 = [mergedlist, mergedlist2]\n",
    "#print(ml3)\n",
    "#print(mergedlist)\n",
    "#dfk = pd.DataFrame(ml3)\n",
    "dfk2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
