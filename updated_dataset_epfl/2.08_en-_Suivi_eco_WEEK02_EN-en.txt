A confidence interval gives us a plausible range of values for the unknown parameter of a population distribution with a guaranteed level of confidence.
But in some cases we may have a theory or a conjecture that the parameters should be contained in some range of values that we ourselves have specified a priori; that is before having collected any data.
For example we might believe that a certain coin or a roulette wheel is fair or we might conjecture that the average lifetime of a certain brand of car battery is at least 4 years.
In such cases, rather than construct a plausible range from the sample we would like to see whether our conjectured range is plausible or not based on the sample.
In other words we would like to use the data we collect in the form of a sample x1, ..., xn to put our theory to the test.
We wish to contrast the theory to the evidence.
Is the observed data coherent with our hypothesized range?
If the behaviour of the sample deviates from what it should be under the hypothesis then we have evidence against the hypotheses.
Hypothesis tests are a statistical tool that allows us to quantify whether the degree of observed deviation is large enough to consist a true departure from the theory or small enough that it is simply a manifestation of sampling variability.
So hypotheses assert that the true parameters within some range of values. In statistical jargon these are known as null hypotheses and they are denoted by the symbol H0.
Null hypotheses typically come in 2 forms.
Firstly hypotheses of the form
Ã¸ = Ã¸0 are called simple hypothesis.
These assert that the true parameter value is equal to some predetermined constant Ã¸0.
For example if we hypothesis that a coin is fair this means that the parameter corresponding to the probability of 'heads' is exactly equal to 50%.
A second form is that of one-sided hypothesis.
That is hypotheses of the form
Ã˜  â‰¥  Ã˜0 or Ã˜  â‰¤  Ã˜0 for some predetermined constant Ã˜0.
These formulate upper or lower bounds for the parameter.
For example hypothesizing that the mean lifetime of a battery is at least 4 years corresponds to a left-sided hypothesis.
Now the outcome of any hypothesis test is a decision.
Based on sample x1, ..., xn of values from the population we decide whether or not to reject the hypothesis.
A key element in hypothesis testing is to construct good rules for rejecting or not rejecting a hypothesis.
What is good?
Well, a good rule is one that limits the proportion of times that we would produce a false rejection, also known as a false positive.
In other words, when the null hypothesis is truly valid at the level of population, our rule should produce a rejection decision for no more than a small fraction of all possible samples.
Let's say 5%.
This proportion is called the significance level usually denoted by the
Greek letter alpha (a) and is most often taken to be 5%.
A simple but effective strategy is to operate as follows.
On the basis of our sample we can construct an estimate of the true Î¸.
Let's call it Î¸^ (x1, ..., xn)
We can then look to see whether the estimate Î¸^ actually satisfies the hypothesis or not.
If it does satisfy the hypothesis, there is no reason to reject but if it does not satisfy the hypothesis we should not necessarily immediately reject.
The point is that the estimate will never be precisely right.
We must only reject if the estimate deviates substantially from the null hypothesis.
What deviations are substantial enough though?
One idea is to consider a deviation to be large enough and thus to reject if a 95% confidence interval for the parameter does not overlap with the hypothesis.
The rational is simple.
We know that the confidence interval contains the true parameter for 95% of all samples.
So if the null hypothesis is indeed true, it will overlap with a confidence interval for 95% of all samples.
Equivalently if the null hypothesis is true, we will falsely reject it only for 5% of all possible samples.
But which interval should we use, a two-sided or a one-sided interval?
Well as you might expect we use one-sided intervals for one-sided hypothesis and we use two-sided intervals for simple hypothesis.
Specifically for one-sided hypotheses if the hypothesis asserts an upper bound then we use a lower confidence bound, in other words, a left-sided confidence interval.
On the other hand, if the hypothesis asserts a lower bound then we use an upper confidence bound, in other words, a right-sided interval.
If all this sounds complicated, we can translate it to a simple list of conditions.
Remember the basic setup: we reject if and only if our confidence interval does not overlap with the hypothesis.
This translates to the following simple list of conditions:
For simple hypothesis we reject if and only if the distance of Î¸^ from Î¸0 exceeds 2Ïƒ (Î¸^)/âˆšn
For one-sided hypotheses we reject if and only if
Î¸^ falls at least
1.6 x Ïƒ (Î¸^)/âˆšn on the opposite side of Î¸0 then that's stipulated by the null hypothesis.
In each of those cases we are guaranteed that the level of significance will be 5%.
In other words, if the null hypothesis is true we will falsely have rejected it only for 5% of all possible samples.
Let's go back to the example involving the high school seniors weight.
We would like to test the hypothesis that the true population mean is at least 150 pounds.
We have calculated the sample mean to be 145 pounds. Thus our estimator deviates from what the hypotheses asserts. Does it deviate substantially enough that we should reject the hypothesis though?
According to our rejection rule since this is a one-sided hypothesis we should reject if the sample mean falls short of 150 pounds by at least 1.6 x Ïƒ(xÌ…)/âˆšn.
This is indeed the case.
The sample mean of 145 is smaller than the critical value of 146 and so we must reject the null hypothesis at the 5% significance level.
So what is the right significance level to choose in order to test a hypothesis?
5%? 2.5%? Maybe 1%.
A lower significance level means that we will be less likely to falsely reject a null hypothesis but changing the significance level will change the factors 2 and 1.6 used in the formulas earlier.
Indeed, picking a lower significance level will inflate these factors.
That means that a lower significance level corresponds to being more conservative.
We will only reject the hypothesis for very large deviations because we want a low risk of a false rejection.
A higher significance level corresponds to being more aggressive. We are willing to reject the hypothesis even for moderate deviations.
In other words we are willing to tolerate a higher risk of a false rejection.
So at the end of the day we can ask, how conservative or aggressive should we be?
Well a possible idea is to not make that choice at all.
For a given sample, find the smallest possible significance level for which you would reject the hypothesis and record it. This recorded value is called the 'p' value.
It tells us what is the least risk we can commit to and still reject the hypothesis on the basis of a given sample.
A very small value for the p-value means that even when being very conservative we would still reject H0 on the basis of the data.
This is strong evidence against the null hypothesis.
A larger p-value suggest that we can reject H0 only if we tolerate a high level of risk.
This does not furnish evidence against H0.
In general, the lower the p-value, the stronger the evidence in the data against H0.
A very important warning has to be issued here.
The p-value is not the probability of the hypothesis being true.
The correct interpretation is that the p-value is the probability of observing a deviation at least as large as what we observed if the null hypothesis were true.
So a small p-value suggests that we have observed something that would be very unlikely under the hypothesis H0 therefore we have evidence against the hypothesis.
Now we not discuss precisely how to calculate a p-value but this can easily be done in all of the settings we have treated.
Luckily a simple rule is: rejecting if and only if the p-value is smaller than a, guarantees a level of significance equal precisely to a.
In other words, if you know you want a level of significance of 5%, you can always compare the p-value to that value of 5% in order to decide whether or not you will reject the null hypothesis.
Going back to the high school student example, let's use the p-value approach to test the null hypothesis that the population mean is at least 150 pounds.
We notice that the sample mean is 145 which deviates from our hypothesis.
Indeed, the sample mean is
5 pounds less than the hypothesized lower bound of 150 which corresponds to just over
2Ïƒ (xÌ…)/âˆšn under the bound.
From the sampling distribution of our estimator we know that if the null hypothesis is truly correct, then a left-sided deviation of this or larger magnitude can only arise for 2.15% of all possible samples.
This 2,15% is exactly our p-value.
It tells us that the observed deviation would be highly unlikely if the null hypothesis H0 were true.
This gives strong evidence against H0.
In particular, it means that we should reject the null hypothesis at a 5% significance level but also for any other significance level that is higher than 2.15%.
