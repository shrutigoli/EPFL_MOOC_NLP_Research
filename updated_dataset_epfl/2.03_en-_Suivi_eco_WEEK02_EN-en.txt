We now have a collection of n values corresponding
to the measured values of the variables on the sample elements and would like to use these to probe the characteristics of the sample distribution.
We can carry out this exploration by means of graphical as well as numerical summaries.
Let's first focus on numerical summaries.
In the case where the variable of interest is continuous there are several numerical summaries that can be constructed, as we will soon see. 
But in the case where the variable of interest is qualitative all you can essentially do is record and present the percentage of sample values falling in each category.
Consider for instance, a sample of size n equals 25, where the variable of interest is the blood group of each individual.
All we can do in this case, is to classify and organize the values in a so-called frequency table.
A frequency table lists all possible categories of the qualitative variable in it's first column.
It then lists the corresponding number of appearances at the level of the sample which are called the absolute frequencies. 
Finally, it lists what proportion of the same members has any given value in the last column.
This is called the relative frequency.
The frequency table summarizes and organizes all the information that the sample carries about the qualitative variable.
When the variables are quantitative, on the other hand, one can use more sophisticated numerical summaries.
In order to do this. a very useful pre-processing step is to order the sample values in increasing order, the so-called order statistics. 
In fact, it is this very ordering that makes quantitative variables interesting.
The fact that they can be arranged and visualized on an axis.
Getting the order statistics involves taking the original sample values, x1 up to xn, and rearranging them as x(1 in parenthesis) up to x(n in parenthesis).
These new labels in parenthesis highlight what the rank of each observation is in the sample rather than it's arbitrary position in the initial list.
For example, consider a sample of size n equals 2 with sample values of 5, 1, and 4.
The order statistics relabel the original observations according to rank, even though the value 1 corresponds to the second member of the sample it has the smallest rank, being the minimal value.
And thus, becomes the first order statistics.
When this ordering is finalized we can then probe features of the distribution.
Is it concentrated around a certain location?
Is it dispersed?
Does it possess some symmetries?
All these characteristics give us information about the distribution of the sample.
We will consider numerical summaries for the following sample characteristics of quantitative variables. 
First, measures of location, indicating the center of the sample distribution, such as the mean and the median.
Second, measures of dispersion, indicating how spread out the sample values are, and whether they locally concentrate and are otherwise dispersed.
These include the variance, quantiles and quartiles and finally the range, and interquartile range.
Let's start with the mean and the median. 
So suppose we have a sample of n values denoted by x1 up to xn.
The sample mean is defined as the sum of these observations divided by the sample size.
On the other hand, the sample median is a bit more complicated to define.
It is defined as the middle observation when the sample size n is odd.
Or the average of the two middle observations when the sample size is even.
But what do these definitions mean?
Let's take a closer look.
The physical interpretation of the mean is that it is the center of mass of the data.
Let's say we have
 a sample of size 5.
And let's order this sample on an axis as shown right here.
Suppose that we place an equal amount of mass on the location of each one of these observations.
We can then ask the question where on the axis should we place this pen in order for the axis to balance?
If we do so too far to the left then the axis is going to fall down to the right.
If we do so too far to the right then the axis is going to fall down to the left.
We seek a perfect point where the axis is going to balance.
Such a unique location actually exists.
It is precisely the sample mean.
In this case, it is somewhere here.
The median, on the other hand is motivated by a symmetry consideration.
Suppose we ask for a point on the axis that splits the data in half.
That means that to the left of the point we should have as many observations as we have to the right of that point.
Now, if the number of observations is odd, such as 5. in this example, this is easy, we can simply pick the middle observation.
In this case, the middle observation is the third largest observation.
Notice that there are exactly two observations to the left of this and two observations to it's right.
So we pronounce this observation to be the median.
If the number of observations is even, on the other hand then any point between two middle observations can serve as a median.
For instance, in this case, we have the two middle observations, x(2) and x(3).
Notice that any point we might choose in the interval between these two observations would split the data set into two equal halves.
The first half containing the first two observations the second half containing the last two observations.
Which point should we pick?
Well, by symmetric considerations it could make sense to simply pick the midpoint, the point right at the middle of the interval.
This midpoint turns out to be exactly the average
1 over 2 times x (2) plus x (3) in this case.
Otherwise said, the average between the two middle observations.
Now, notice that the mean uses the precise numerical value of all the observations in the sample.
The median, on the other hand, relies mostly on the order of the observations.
So generally the mean is a more efficient means of using the data and will typically be a  better descriptor of the center of the sample.
Then why bother with the median?
The reason is that the mean can be very sensitive to the presence of extreme observations.
Consider what can happen if a single observation stands out far from the rest.
For instance, going back to our example of 5 observations, suppose we take the largest observation and we shift it to the edge of the axis.
Now the original location of the mean provided the location where this axis is going to balance if we've placed an equal amount of weight on each of these observations.
If we shift the largest observation to the right then the weight will be shifted to the right.
So at this point now, the axis is going to fall down.
This means that we need to correspondingly shift the mean in order to re-balance the axis.
In fact, the further away the fifth observation is moved the further the mean is going to be shifted.
The morale is that a single extreme data point could completely undermine the quality of the mean as a descriptor of the location of the data set.
In contrast, let's see what happens to the median.
Let's again take the largest observation in our sample and push it to the right.
Now, the original median was the third observation.
After all, this is exactly in the middle of the sample.
There are 2 observations to it's left and 2 observations to it's right.
Having shifted the fifth observation to the right, this doesn't really change.
There are still 2 observations to the left and then 1, 2 observations to the right.
The median stays put. 
In fact, no matter how much we push the largest observation to the right, the median will not move.
It remains the middle observation no matter how much the extremes are moved around.
In summary, the mean is generally a more accurate descriptor of location and should be used when we are not liable to having extreme observations.
The median, on the other hand, maybe less efficient in it's use of it's data, but could come in useful when we expect that extreme observations are likely to arise. 
Now, once we have a notion of center for our sample distribution, we may want to know how dispersed or concentrated the sample values are around that center.
The variance is a natural descriptor of how dispersed the sample values are around their mean, in particular.
It is defined as the sum of square differences of observations minus their mean, suitably renormalized.
It represents the average square distance between a sample value and the sample mean.
How far is an observation from the center of the data set, on average?
Now, for technical reasons, one often uses a scaling factor 1 over n minus 1, instead of 1 over n.
But let's not worry about that too much.
Now, a draw back of the variance is that it is not expressed in the same units of measurement as the original variable, because of the squares.
For instance, if the values x1 to xn are measured in meters, their mean is also measured in meters.
But their variance is measured in square meters.
The standard deviation corrects for that by taking the square root of the variance.
The 2 notions are equally informative the only difference is a difference of scale.
To understand variance, we consider
2 different samples, each comprised of 5 observations, x1 to x5, and y1 to y5.
We arrange these 2 samples on 2 different axes from smallest observation to largest observation using the order statistics.
We then calculate their sample means.
Notice that these 2 sample means are exactly the same.
Still, the 2 samples are different.
In the first sample, the sample observations are more concentrated around their sample mean than they are in the second sample. 
This means that the first sample has a smaller variance than the second sample.
The average deviation of the sample observation to it's mean is higher in the second sample than it is in the first.
Now, the variance could be argued to be too coarse a measure for some data sets.
For example, if you look at the 2 data sets on my right, these have the exact same mean and the have the exact same variance.
But, arguably, the distribution of their values on the axis is quite different.
In effect, what's going on is that they have different asymmetry relative to their mean.
To distinguish finer characteristics such as asymmetries, we define the notion of quantiles.
For any percentage point alpha, the alpha quantile is defined to be the smallest observation of the sample that exceeds at least alpha percent of the sample values.
For instance, when the sample size is odd the 50% quantile is equal to the median.
We term the 25%, 50 and 75% quantiles as the first, second and third quartile.
We partition the axis into four regions, each containing roughly one-fourth of the sample, that is one quarter of the sample, hence the name quartile.
Now the quartiles give us more detailed information on the dispersion and concentration properties of the sample.
For instance, they allow us to appreciate whether the distribution of the sample is symmetric or asymmetric around the sample mean.
If the distance between the first and second quartile is smaller than the distance between the second and third quartile then we see that there is a right a symmetry.
The second half of the sample is more spread out than the first half.
This is the case in the first plot on the right.
Conversely, when the first and second quartile are further apart than second and third one we have a left symmetry.
If the distance between consecutive quartiles is roughly the same then we speak of a symmetric distribution.
This is precisely the case with the last plot on the right.
To conclude this section, we briefly mention two more measures of dispersion.
First, the difference between the first and third quartile.
This is called the interquartile range
It can be used to assess the concentration of the sample
It reveals how concentrated or spread apart the middle 50% portion of the sample is.
Finally, the overall range or just range is an even coarser descriptor.
It gives us the length of the smallest possible interval containing the entire sample.
Therefore, it's just a difference between the largest observation and the smallest observation.
