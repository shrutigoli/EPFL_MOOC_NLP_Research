<i>Chapter 3, Implementation Research Outcomes. </i>
Hi, my name is Vivian Go, and I'm an Associate Professor in the Department of Health Behavior at Gillings School of Global Public Health at the University of North Carolina in Chapel Hill.
Today, we're going to talk about how to measure
 implementation research outcomes.
In chapter one, Dr. Alonge discussed the differences between innovation efficacy, innovation effectiveness and implementation effectiveness.
Then in chapter two, Dr. Alonge introduced six examples of field studies conducted in Africa on infectious diseases of poverty, and the implementation research outcomes that each of these studies utilized.
In chapter three, I'll be talking about how to measure implementation research outcomes using these same six examples.
This chapter specifically will to be divided to two sections.
In section one, we will discuss examples of how implementation research outcomes have been defined and operationalized  in implementation research focused on infectious diseases of poverty in the field.
In section two, we will describe data sources.
In other words, from whom we will collect data from and data collection approaches.
In other words, what methods we will use to collect data to support implementation research outcomes.
Let's start by taking a little quiz to gauge your awareness of these issues.
This is just for you to get a sense of what you know and what you don't know but can learn from this chapter.
I'm going to read four statements.
Please take a moment to write one to four on a piece of paper.
After each statement I read, mark if you think the statement is true or false on the paper, next to the statement number.
At the end of chapter three, I'll revisit the statements and go over why each of them is either true or false.
Statement number one, true or false.
Data for measuring implementation research outcomes are often collected across multiple socioecological levels.
By socio-ecological levels, I mean individual levels, community level, organizational level, etc.
Statement number two, true or false.
Data on implementation research outcomes can be collected using both qualitative
 and quantitative methods.
Statement number three, true or false.
Administrative data are useful sources of information for measuring implementation research outcomes such as cost and fidelity.
By administrative data, we mean data that is already being collected by the organization or programme.
Statement number four, true or false.
Measurement of implementation research can often involve scale development.
You can set aside your paper now and we'll go back and revisit these statements and the correct answers at the end of this chapter.
In the meantime, let's go over these concepts in the next two sessions.
Let's go back now to the six studies that Dr. Alonge presented in chapter two of this module.
I'm going to use these six studies as examples of how
implementation research outcomes have been defined for a specific disease in a specific context.
First, let's look at the example of the study entitled
Community directed intervention strategies for the control of onchocerciasis with ivermectin.
As we mentioned in module 2, the goal of the study was to assess if a community directed intervention approach improved treatment coverage in onchocerciasis affected communities.
The implementation research outcomes in the study were coverage and cost.
Coverage was defined as the number of persons treated with ivermectin for total population within a community health worker's kinship zone in one year.
In other words, the total number of people treated among those who should have received it.
Cost was defined as cost of training which included allowances for the trainers, transport and food for trainees and training materials divided by the total number of community health workers and supervisors that were trained in one year.
In the study by Akogun and colleagues in Nigeria, among nomadic Fulani communities to manage malaria, the implementation research outcomes were coverage and acceptability.
As we mentioned in module two, the goal of this study was to improve uptake of insecticide treated nets and prompt treatment of malaria among children who are under 5.
Coverage in the study was defined as the proportion of children under 5 with a fever who received antimalarial treatment as well as the proportion of individuals under insecticide treated nets per two nomadic cycles within a pre-defined nomadic group.
Acceptability in this study was defined as general satisfaction with community directed intervention strategy and the two evidence based interventions consisting of treated nets and malarial treatment, as well as the demand for evidence based interventions in the community.
In the study conducted by Akweongo, Artemisinin-based Combined
Combination Therapy distributed to a community based distributor in rural areas in Ghana, Burkina Faso
Ethiopia, and Malawi was evaluated.
Specifically, community medicine distributors (CMDs) were trained to educate care givers, diagnose and treat malaria cases in under fiveyear-old children.
The implementation outcomes of the study were fidelity, coverage and feasibility.
Fidelity here was defined as services delivered by community medicine distributors as planned, which was measured as the proportion of febrile illnesses that was treated with Artemisinin-based Combination Therapy, promptness of treatment and correct dosage and duration.
Coverage was defined as the proportion of children who are under five presented to the community medicine distributors that were treated within 24 hours of onset in the two weeks prior to the survey in a predefined area.
Feasibility was operationalized as a community medicine distributorâ€™s self-purported ability to recognize and promptly treat malaria cases and care giversâ€™ acceptability of these services provided by the community medicine distributors.
In the study conducted by Okeibunor and colleagues in Nigeria,
Togo and Cameroon, researchers studied the involvement of community directed distributors (CDDs) of ivermectin for onchocerciasis control and other health related activities.
Specifically the researchers addressed or assessed if involvement in additional activities such as EPI, water and sanitation in community development projects detracted from their effectiveness in delivering ivermectin.
Feasibility and coverage were measured as the two primary implementation outcomes.
Feasibility was measured as attitude and performance of CDDs involved in other public health activities.
In this case, performance was measured as coverage rate
 for ivermectin treatment.
Coverage was measured as the proportion of the population who received ivermectin treatment in the communities from which the community directed distributors were selected.
In the study, Early treatment of childhood fevers with pre-packaged antimalarial drugs in the home,
Sirima and colleagues evaluated the impact of using pre-packaged antimalarial drugs by mothers at home and the progression of disease in children from uncomplicated fevers to severe malaria.
Fidelity was the only implementation outcome assessed and it was operationalized specifically by looking at the proportion of children, six years and under who had uncomplicated malaria that was treated as planned. â€œTreated as plannedâ€� here was defined as being promptly treated with pre-packaged antimalarial drugs with correct age-specific dose and duration in the four weeks prior to the assessment survey.
In the last example, Akogun and colleagues assessed strategies for community directed treatment of onchocerciasis with ivermectin in Nigeria.
The study evaluated fidelity, coverage, cost and feasibility as its implementation research outcomes.
Fidelity in this study was measured as appropriate adherence to the treatment regimen, correct dosage and treatment frequency, and the correct management of adverse reactions to medication.
Coverage was measured as the total number of people treated out of the total population after two cycles of treatment.
Feasibility was operationalized as the acceptability of a range of community directed ivermectin approaches and the cost and coverage under different approaches.
And finally, cost was measured as all costs that the village spent in delivering ivermectin.
You'll notice that although there are similarities in diseases such as malaria and onchocerciasis, in these examples, that both selection of implementation outcomes and the way that they were operationalized were tailored to the specific goals and contexts of the interventions and the studies.
So, for example, in the study conducted by Akogun and colleagues in Nigeria among nomadic Fulani communities to manage malaria, the implementation research outcomes were coverage and acceptability.
As you may remember the goal in the study was to improve the uptake of insecticide-treated nets and prompt treatment of malaria among children under five.
In the study conducted by Akweongo, Artemisinin-based
Combination Therapy distributed to a community medicine distributor in rural areas in Ghana, Burkina Faso, Ethiopia, and Malawi was evaluated.
The implementation outcomes of this study were fidelity, coverage and feasibility.
So you can see there were differences in implementation outcomes selected based on the goals of the intervention, and although both studies looked at coverage as an outcome, the operationalization was a little different.
Coverage in the study by Akogun and colleagues was defined as the proportion of children under five with a fever who received antimalarial treatment as well as the proportion of individuals under insecticide treated nets per two nomadic movement cycles within a predefined nomadic group.
Coverage in the Akweongo study was defined as the proportion of children who are under five who presented to the community medicine distributors that were treated within 24 hours of onset in the two weeks prior to the survey in a predefined area.
So you see that there are similarities in the operational definitions of coverage, so both consider appropriate malarial treatment for children under five, but one denominator is all children with the fever whereas in other denominator, it's just among those who presented to a community medicine distributor.
And one also considers insecticide-treated nets.
These are due to the differences in the evidence-based interventions being tested or being scaled up.
In addition,  the time frames are different.
One considers it per nomadic movement cycles while the other considers it within two weeks prior to the survey, reflecting the different populations under study.
That is why we say, implementation research outcomes are heuristically defined.
Now that we've talked about how different implementation outcomes can be operationalized in the field, we're going to talk about how data on implementation research outcomes are collected.
Implementation research outcomes can be collected at different levels and through different data collection approaches.
I want to emphasize that implementation studies typically use mixed quantitative-qualitative designs, identifying factors that impact uptake across multiple levels including patient, provider, clinic, facility, organization and often the broader community and policy environments.
Accordingly, implementation science requires the involvement of trans-disciplinary research teams as we had mentioned in module 1 of this course.
Common quanitative measures include structured surveys that assess for example organizational context, provider attitudes and behaviors, patient receptivity to change or household access to provider services.
Administrative data are often utilized either in focal target populations or at the broader system levels to characterize for example, baseline and change in rates of utilization of particular practices.
Common qualitative data collection methods include semi-structured interviews with patients, providers or other stakeholders, focus groups, direct observation of clinical processes and document review.
One final note, implementation research outcomes are by nature, latent variables.
That is to say that these variables are often not directly observable but rather, are rather inferred from other variables that are observed, and may require scale development.
For example you can not directly observe acceptability but rather can combine a series of questions that together infer acceptability.
Let's look now at each of the nine implementation research outcomes that we have presented in this chapter.
Acceptability can be measured at the individual or community level through quantitative surveys that assess satisfaction.
It can also be measured through qualitative interviews including in-depth interviews or focus groups that explore participantsâ€™ satisfaction with a particular intervention.
And finally, acceptability can be measured through administrative data that collects data on service utilization overtime.
Adoption can be measured at the individual, community or organization level since the intention to try a new intervention can be an individual,  community, and/or organizational level decision.
For example, the decision to integrate mental health services at a primary healthcare system can be made at the organizational level by clinic leadership but the decision to utilize these services may be made at an individual and/or community level.
Adoption can be seen through observation through a survey, or through other qualitative data such as in-depth interviews or focus groups and through administrative data, for example, you can examine clinical records to check if primary care physicians are appropriately screening for and referring to mental health services.
Appropriateness or fit of the evidence based programme or implementation strategy can also be measured through individual, community or organizational level through either surveys or qualitative methods.
And feasibility, the extent to which an intervention can be carried out in a specific setting or organization can be measured at the individual, community or organizational levels through surveys, administrative data and qualitative methods.
Continuing on to the last four implementation outcomes; fidelity, the degree to which an intervention was implemented as planned, can be measured at the individual level through survey, observation, checklists, self-report and administrative data.
For example, if assessing the fidelity of a counseling session as an intervention, an independent observer could sit in on the session and complete a checklist to ensure that the session was delivered as planned.
Implementation cost can be measured by the individual or organization level by assessing cost data that's already being collected by the clinic or organization.
Penetration, the extent to which a population eligible to receive an intervention, actually receives that intervention can be measured at the individual, community or organization level also, through surveys, checklists and, or an audit by an independent observer.
And finally, sustainability, which is defined as the extent to which an intervention is maintained in a given setting or context can be measured at the individual, community or organizational level using surveys, administrative data and qualitative methods including an audit.
Let's now turn back to our six examples of studies of infectious diseases of poverty in the field.
We can see how implementation research outcomes are measured, both in terms of the multiple levels from which we draw our data from and the different methods used to collect these data.
In Uganda and Cameroon, as you may remember,
Katabarwa and colleagues defined coverage as the total number of people treated with ivermectin among those who should have received it.
They used both individual and community level data through community household treatment records that were completed by the community health workers.
In the study, data on cost, which was defined as the cost of training including allowances for the trainers, transport and food for the trainees and the training materials was collected at the organizational level through the administrative system that already tracked these costs.
In Nigeria, among nomadic Fulani communities who are managing malaria, coverage defined as you may remember, as the proportion of children under five who received antimalarial treatment and the proportion of individuals under insecticide treated nets which measured at both the household and community level through household surveys.
Acceptability which was defined in the study as general satisfaction with the community directed intervention strategy and the two evidence based interventions consisting of treated nets and malaria treatment as well as the demand for the evidence based interventions in the community,
was assessed through qualitative in-depth interviews and focus groups at the individual level and feedback meetings at the community level.
These two studies are good examples of how one implementation outcome can be measured at multiple levels.
You can see in the study by Akogun how if we want to get community level perceptions of acceptability, a different data collection approach in the form of community meetings is used compared to the individual level acceptability data collection methods.
In our third example, [in] the study by Akweongo and colleagues as we remember, aimed to assess the impact of community medicine distributors trained to diagnose and treat malaria cases in children under five years of age using ACT.
Fidelity was defined as treatment as planned delivered by community medicine distributors,[and] was measured both at the individual and organization levels using the records completed by the community medicine distributors.
Coverage defined in the study as a proportion of children under five who presented to community medicine distributors that were treated within 24 hours  of onset, was measured at the household and the community level through a household survey completed by clients.
Feasibility defined here as the CMDs ability to recognize and promptly treat malaria cases and the care giver's satisfaction of these services was collected at the individual and community level using qualitative interviews with care givers.
In our fourth example, Okeibunor and colleagues assessed whether community directed distributors' additional activities had a negative or positive effect on the distribution of ivermectin to treat onchocerciasis.
Feasibility operationalized here as attitude and performance of CDDs involved in other public health activities was measured at the individual or CDD level through health worker survey.
At the household level, through household surveys, and at the community level through focused group discussions and in-depth interviews with care givers.
Coverage defined as the proportion of the population who received Ivermectin treatment in the communities from which the community directed distributors were selected was measured at the household level through household surveys.
Moving on to our last two examples.
As you recall, Sirima and colleagues evaluated the impact using pre-packaged antimalarial drugs by mothers at home, on the progression of diseases in children from uncomplicated fever to severe malaria.
Fidelity was the only implementation outcome assessed and it was operationalized by looking at the proportion of children six years and under who had uncomplicated malaria that was treated as planned.
Fidelity was measured at the household level through a household survey.
And finally,
Akogun and colleagues assessed a range of strategies for community directed treatment of onchocerciasis with ivermectin in Nigeria.
Fidelity measured here as the appropriate adherence to the treatment regimen, correct dosage and frequency and the correct management of adverse reactions to medication was measured at the individual and household levels through a household survey.
Coverage defined in the study as the total number of people treated out of the total population after two cycles of treatment was also measured at the individual and household level using a household survey.
Feasibility operationally defined in this study as acceptability of a range of community directed ivermectin approaches and the cost in coverage under different approaches was assessed at both the individual and household levels through the household survey and at the community level using direct observations and in depth interviews with clients.
And finally, cost, operationally defined as all costs that the villages spend in delivering ivermectin was obtained at the community level through a structured form that cataloged all inputs from the village.
We can see from these examples that the levels from which we obtain data, and the methods used for data collection stands from a combination of how we operationalized
the implementation outcomes and what is realistically feasible given time and resource constraints.
So, we've learned in chapter three how we operationalize implementation research outcomes and how we collect data on these outcomes, both in terms of sources and methods using our six examples drawn from our required readings.
Let's now revisit the statements that we used to gauge our knowledge about implementation outcomes at the beginning of this chapter.
The first statement was: data for measuring implementation research outcomes are often collected across multiple socio-ecological levels.
The correct response here is true.
We know the data come from multiple levels including patients, provider, clinic, facility and organizational levels to give us a fuller picture of the implementation effectiveness for multiple perspectives.
The second statement: implementation research outcome data can be collected using both quantitative and qualitative methods.
The correct response is true.
We know that in order to glean a fuller picture of barriers to implementation and implementation effectiveness, it's necessary to use both quantitative and qualitative methods to understand the extent of barriers and effectiveness and the hows and the whys behind those barriers and to triangulate information from both sources and all methods.
Three, administrative data are useful sources of information for measuring implementation research outcomes such as cost and fidelity.
The correct response here is true.
To save resources and to avoid duplication, we can use already existing data that are being collected by clinics, programmes and organizations to assess implementation outcomes such as cost and fidelity.
Four.
Measurement of implementation research outcomes often involves scale development.
The correct response to this statement is true.
We know that often implementation outcomes are by nature, latent variables.
That is to say, that these variables are not directly observable, but can rather be inferred from other variables that are observed.
This may require scale development.
For example, you cannot directly observe acceptability but rather can combine a series of questions that together infer acceptability.
This concludes chapter three and our discussion on how to measure implementation research outcomes.
