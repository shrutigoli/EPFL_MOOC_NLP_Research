So, good morning.
We are here again to continue in the Module 4 of the course on implementation research.
So, today we will be continuing in this same vein and we will be looking at examples of implementation research outcomes.
This chapter has been divided into 3 sections.
The first section will describe common constructs that have been used as implementation research outcomes.
Then, in the second chapter, we will describe the relevance of different implementation research outcomes for different stages of implementation research.
Before we proceed, it's now a practice, we're going to do some quizzes to kind of help us to gauge our knowledge and to also present some of the key messages that we will be presenting in this chapter.
So, I would like you to take your pen and your paper and to answer true or false to the following questions.
The first question:
The following are examples of implementation research outcomes: mortality, morbidity, and disease incidence.
Question 2:
Implementation research outcomes are only single-domain constructs and are heuristically defined.
Question 3:
Implementation fidelity could be assessed by examining adherence to intervention protocol and quality of delivery.
Question 4:
Various implementation research outcomes are relevant for research focused on different temporal stages of implementation.
And the last question:
Fidelity measures are some of the most common implementation research outcomes used in assessing health system readiness.
We'll begin Section 1 by looking at common constructs that are being used in public health studies as implementation research outcomes.
I would like to clarify that these are not the only constructs that could be used or that are examples of implementation research outcomes.
But these are the common ones that you'll find across the literature not only for infectious diseases of poverty, but also across different areas of public health.
So, to the first construct, which is acceptability.
That we defined as the perception among stakeholders that an intervention is agreeable.
So it's this whole idea that people find the way in which an intervention is delivered to be agreeable.
They have some level of comfort or some satisfaction with different aspects of implementation activities around the delivery of an intervention.
That has been called by other terms including comfort, relative advantage and credibility.
So, again to the example that we used earlier on in Chapter 1 we had said that in testing the impact of different strategies on the delivery of an evidence based intervention that we could use acceptability to compare one strategy to another.
We could say to some extent that implementation activities are successful when we have better acceptability of a strategy compared to another.
Adoption has been defined here as the intention or the initial decision to try out an intervention.
So it's this whole idea that you contemplate to try out an intervention and you eventually make the decision to do the intervention and then in the phases where you are beginning to test out the intervention.
So all that is the measure of adoption.
For instance even by virtue of a new strategy people in a particular community are able to adopt some intervention compared to some other community.
That, in a way, is useful for testing whether these sorts of implementation activities have worked or not.
Appropriateness is the perceived fit or the relevance of an intervention even in a particular setting.
So this whole idea that because of the way an intervention is designed, because of the make-up of a particular context in which the intervention is going to be delivered, that you could begin to imagine that that would be a good fit of an intervention into the new setting that you are intending to implement the intervention.
And that, in a way, is also a measure of your implementation activities because you could say that in places where you have high appropriateness you're going to achieve effective implementation and therefore achieve effective results.
Feasibility.
It's perhaps a very common construct that has been used in study of implementation research.
Here it is defined as the extent to which an intervention can be carried out in a particular setting or organization.
So you see here it's different from appropriateness in that appropriateness looks at the perceived fit while here in feasibility we're actually measuring the actual fit of the intervention into the new setting.
That is to say that we can not measure feasibility without actually delivering the intervention and usually this measure is taken at the very early phases of an implementation activity.
So we think about our pilot studies that look about what is the likelihood of success if we do this intervention in a new setting.
The likelihood of success might be determined by looking at the cost or might be determined by looking at the acceptability of the intervention or may be determined by looking at how effective it was for the stakeholders and the main actors in being able to deliver the actual intervention to the population that needs it.
So all of these together are measures of feasibility.
And  you  would see from the elaborations that are provided that there are different domains that can be used to measure feasibility.
For instance I mentioned cost,
I mentioned acceptability,
I mentioned the degree of execution of the intervention.
So all of these domains could be together used to measure feasibility.
That is to say that the definition comes by experience.
It really comes by the reason of doing of the intervention or doing of the activity and therefore you're able to define this construct based on the actual experience of implementation.
Put in another way, these outcomes are heuristically defined.
Let's move on.
Still talking about the common examples of constructs that are being used as implementation research outcomes.
The next common construct that we are going to talk about is fidelity.
So, fidelity is the degree to which an intervention was implemented as it was designed, in an original protocol, plan or policy.
That is to say that for every evidence based intervention there is a way in which the intervention was designed or there is a way in which the intervention was originally delivered when it was being tested for effectiveness.
So fidelity is being able to carry out that intervention in the way that it was originally designed in the original protocol by the designer.
So anything that deviates from the main protocol or that deviates from the way in  which the intervention was originally designed would be a deviation in fidelity.
So there are several other names by which fidelity has been described in the literature including adherence, delivery as intended, integrity, quality of delivery, intensity, dosage, and so on and so forth.
But it's important to also highlight that for fidelity just like feasibility it is a multi-domain construct because you have different domains that contribute to the measure that we call fidelity.
Implementation cost is the incremental cost to deliver an intervention in a new setting.
All the cost that is associated with the implementation activities to carry out an intervention in a new setting will be implementation cost.
This is a single domain or construct because it looks simply at monetary value of a set of activities to do an intervention in a new setting.
Penetration talks about this whole idea of the vertical scale-up of an intervention within a particular system.
For example, let's imagine that an intervention can only be delivered at the health facility level.
Penetration will look at how many of the different health
facilities in a new setting have actually carried out the evidence with practice.
It's going to be a measure or penetration of this vertical scale up of an intervention.
And it's slightly different from coverage which is really looking at the spread of the intervention within the community or within the eligible population that needs the intervention.
That is to say that what proportion of an eligible population that should receive an intervention actually receives it.
Next and last, but not the least is sustainability.
Here we define sustainability as the extent to which an intervention is maintained or institutionalized in a given setting.
That is to say that this whole idea of being able to integrate an evidence based intervention along with these implementation activities within the larger framework or the larger health system or the larger organization setting in which the intervention was originally delivered.
This has been called by several other names including maintenance,  continuation, routinization and institutionalization.
Obviously there is no particular way of taking this measure.
What is important is to be true to the paradigm and to be true to the thinking and to be able to show the linkages between how these implementation research outcomes link your intervention and your evidence based strategy on one hand to the impact that you hope to see at the population level or in the individual health level on the other hand.
So before we conclude Section 1 we are going to look at some specific examples of implementation research outcomes that have been used in TDR supported grants.
It's important for you to have done the required reading at this time because the examples that we're going to be looking at in the rest of this chapter or Chapter 2 and also in Chapter 3 are going to be heavily based on the required readings.
So if you have not done so already
I would like you to take a pause now and go to the required reading and complete the 6 readings before you move on.
If you have done the readings already
I would like to invite you to come along with me as we look at these examples.
So the first example that we're going to look at today is really the example by Katabarwa et al and we are looking at community directed interventions in an ivermectin maintained control programme.
They looked at this study in Uganda and in Cameroun.
The objective of their study was to assess this strategy and to assess its impact on the performance of community health workers.
So community directed intervention is this whole idea of empowering community to find solutions to challenges around implementation of an evidence based intervention within their locale.
The idea is that if communities are empowered they're able to direct appropriate strategy to the area of interest for their community.
And that in effect will encourage or allow successful or effective implementation.
And the idea was that for the community that underwent this strategy, they will be trained in the ability on how to deliver ivermectin successfully to their community members.
The training will require some costs and at the end of the day they will look at coverage of the spread or the distribution of the ivermectin within the community where the strategy is being implemented.
So, in this study coverage is an example of implementation research outcome.
So they looked at the proportion of people who had received the ivermectin under the community directed strategy and they compared it to people who have received ivermectin under the normal status quo where ivermectin had been previously delivered.
They also looked at cost: cost of delivering even the community directed strategy including cost of the Ivermectin control program within their community.
The findings were that mass treatment coverage with Ivermectin increased as a result of the CDI strategy and implementation cost reduced over time.
So, in this second example we are going to be looking at a study by Akogun et al in Nigeria.
They were looking at how
Nomadic communities nomadic Fulani communities treat malaria using some evidence based intervention for malaria control.
The objective of the study was to assess the CDI strategy on how this improved the coverage and the acceptability of the malaria control intervention.
So, specifically they were looking at training, providing training to Fulani herdsmen on how to manage malaria for under 5 children using artemisinin combination therapy and also teaching them on how to properly use bed nets.
In doing this they are going to encourage the community to seek pathways or to seek ways in which they could successfully deliver the 2 interventions within their community.
So it's this whole idea again of community directedness.
That is to say the communities put heads together and decide on how best to deliver an intervention or what is the best strategy to put in place for the delivery of an intervention.
And we are going to be looking at how successful this community directed strategy is with regards to the current approach being used in the health system.
And they measured the test of this community directed intervention or strategy by looking at coverage and also acceptability.
Coverage they looked at to see whether the people who were under the community directed strategy had better access or better coverage of the malaria control program.
And they also looked at whether the people that received treatment from community members found this agreeable or were satisfied with the approach rather than going to the health facility or going to the health care worker to receive the treatment.
And at the end they found that yes, both malaria control programs were acceptable to the nomadic population and there was an improvement in the coverage overall with the community directed strategy.
So, in this third example we are going to be looking at a study by Akweongo et al and here we are looking at feasibility and acceptability of artemisinin combination therapy.
And the study was conducted in Ghana
Burkina Faso, Ethiopia and Malawi.
At this point I would like to emphasize again this idea that was presented in the first module [even] of this course, the idea that implementation research is often done in a multi country setting.
That is to say that for us to establish generalizable evidence for a set of implementation strategies or a set of implementation activities, we have to do it in multiple settings or in fact do it in multiple countries.
And that's what you see being demonstrated  by some of the studies that we are looking at today.
So for the study looking at feasibility and acceptability of ACT the objective was to see how feasible it is to use even community medicine distributors for case management of malaria in urban settings.
So the idea is that yes, community medecine  distributors were being used to manage malaria for under 5 children  in rural settings.
But thinking about the fact that more and more countries in sub-Saharan Africa are becoming urbanized so they try to see whether the same strategy, that is to use community medicine distributors to deliver an evidence based intervention, would work even in an urban setting.
And then to determine whether this strategy is going to work or not or whether this set of implementation activities that they are going to put in place are going to work, they looked at specific measures of implementation research outcomes.
They looked at fidelity, that is, to look at for all the community medecine distributors that were trained to what extent did they deliver
[even] the intervention as they were trained according to the protocol in terms of the dosage, the frequency by which the children should've received the intervention.
They also looked at the coverage.
That is to say the number of children who should receive the malaria treatment, the appropriate malaria treatment.
How many of them did receive the treatment?
And then they looked at feasibility.
This whole idea that it's even possible for us to be able to do this in an urban setting at all.
Is this actually feasible?
And at the end of the day they concluded that yes,
case management of malaria could actually be done successfully by community medecine distributors even in an urban setting based on the changes in fidelity, coverage and feasibility.
So for the fourth example we look at a study by Okeibunor et al in Nigeria and in Togo and Cameroun.
They are looking at the use of community-directed distributors of ivermectin for other public health control programs.
So this whole idea that yes we have these community distributors within villages or rural areas.
That is to say can they be used also for some other public health programming including [areas] like sanitation, immunization activities and so on?
And does this inclusion negatively impact the activities in delivering the ivermectin control programme that they were originally slated for?
And in studying this they looked at specific implementation research outcomes.
First they looked at feasibility.
That is to say is it possible at all to actually incorporate additional tasks for these community distributors of the ivermectin control programme?
For instance, could they be involved
[even] in sanitation, could they be involved in immunization and nutrition programmes in their community?
And without that negatively or adversely affecting their work in ivermectin control?
And they also looked at coverage.
That is to say that does the proportion of people that received ivermectin change with the inclusion of additional tasks for these community distributors in their community?
So, at the end they concluded that yes involvement of these community-directed distributors in other public health activities even in their community did not have a negative impact on the ivermectin control programme.
So far we have looked at
4 different examples of implementation research studies that were done using implementation research outcomes as a measure of effectiveness of implementation activities in different parts of sub-Saharan Africa.
When we come back we will continue with 2 additional examples and then we will look at other usefulness of implementation research outcomes, such as implementation research outcomes being used for the assessment of health system readiness and the relevance of implementation research outcomes for different stages of implementation or implementation activities.
