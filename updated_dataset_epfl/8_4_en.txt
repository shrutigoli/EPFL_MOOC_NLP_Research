Now I state theorem 3.1 from the book.
Let g be a function from R to R, assumed to be C1, that is once differentiable, and let x bar be a fixed point of g, that is g(x bar) = x bar.
Thus it is assumed that there exists a fixed point of this function g, moreover, it is assumed that the absolute value of g'(x bar) is strictly smaller than 1.
The claim is: there exists a positive epsilon such that, if the initial guess x_0 is between x bar minus epsilon and x bar plus epsilon, then the sequence defined by x_n+1 equal to g(x_n) converges to x bar.
We have a C1 continuous function g, x bar a fixed point of g, and g'(x bar) in absolute value smaller than 1.
In this case, there exists a neighborhood of x bar such that by choosing x0 in this neighborhood, insures that the sequence x_n+1 equal to g(x_n) converges to x bar.
There is an extra information: furthermore, the convergence is linear. 
This means there exists a positive C strictly smaller than 1, such that for all n, n the iteration index, x_n+1 equal to g(x_n), for all n, well the error at step n+1 is smaller or equal to C, being strictly smaller than one, times the error at step n.
Hence the error decreases at each step, since the ratio between the error at step n+1 and the error at step n, is strictly smaller than 1.
