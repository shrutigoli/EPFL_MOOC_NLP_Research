In this video we're going to talk about declarativity in more detail.
So, what is a declarative language?
It's a language in which you specify what you want to get as a result rather than how to compute it.
So it describes the desired result, rather than steps towards obtaining it.
Examples would be functional languages in general and logic-based languages.
And to make that clear, because there's sometimes
I would argue, misunderstood, the distinction is between declarative on one hand and imperative language on the other, rather than between declarative and procedural.
So why would we want to use declarative languages?
The reasons are the same as why you would want to use DSLs, and the reasons are even stronger potentially.
We would like to have an abstraction if possible and separation of concerns.
If there's low level implementation details potentially, low level things about how to do things, we don't want to have to worry about them if that can be avoided, right?
That increases, of course, simplicity and programmer productivity and we can more easily avoid making mistakes.
That way we might be able to do a substantial drop in very few lines of code.
And we are able to concisely capture the essence of the system that this language supports, right?
So if we have really designed a language that very well captures what I can do in a system, then this language might also explain the system so to say.
It is very easy to basically, you understand the language and you know what the system does.
And it's very easy to avoid misusing the system.
If it's not possible to program things that are not meant to happen in the system.
So functional languages as said,
I would argue, as being declarative, since we have learned first about functions in mathematics, these are in a sense conceptual items.
It's not necessarily about how to compute these functions, that would be algorithms, right?
But about what they mean.
And so it is known, there are reports of all kinds that programmable activity in functional languages can be substantially higher that in classical imperative languages.
This for certain examples.
Something like two orders of magnitude has been reported.
A particularly good example is compilers.
You can write a compilers particularly easy, usually, to do it in a functional language.
For example there's pattern matching, etc. and constructed very natural and widespread in functional programmer languages which are making for example, writing compiler very easy, in a functional language and it's much more complicated usually in imperative language.
I'm not going to say much more about this because I assume that you know functional programming and there's not much more to say.
Except for maybe this one qualification.
That you can think of functional languages also as logical languages.
And clearly the pioneers of computer science, for example,
Church-Turing and Lambda Calculus thought that way.
And really thought of, for example, lambda calculus as combination of logic and functional programming so to say.
So what kind of logics are we going to study in more detail?
Well first this class of classical logics.
By that I refer to classical semantics.
Semantics are consistent with first order predicate logic according to Frege.
So that includes first order logic but also it's simplification propositional logic and also higher-order logics, where I've got more powerful quantifiers so in which I can quantify and dilations and sets.
And also modal logics.
I've got modes of true, for example, which give me interesting power.
Alternative to that, there's logic programming and it includes datalog and prolog.
We're going to talk more about this in a separate video.
And there's nonmonotonic logics which have semantics that is inconsistent in the sense of classical logic, but they can be very useful.
And there is nonmonotonic generalizations of classical logics, for example, positional and first-order logics but also off logic programming.
