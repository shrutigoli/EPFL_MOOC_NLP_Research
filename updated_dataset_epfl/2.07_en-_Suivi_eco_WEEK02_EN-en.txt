The problem with a point estimator is that no matter how accurate it is, it will almost never be exactly right.
For instance, we know that, had we chosen a different sample, we would have obtained a slightly different value of our point estimator.
A different approach than point estimation, is to give a range of plausible values rather than a single value for the unknown parameter.
This comes in the form of the interval.
If we had many samples at hand, perhaps we'd use the different estimates obtained to find a plausible range, similar to the image on the right... but we only have one.
So, what we need to do, is to take advantage of our knowledge of the sampling distribution of the estimator, in order to produce such an interval.
What are the characteristics that such an interval should possess?
Well, obviously, we would like they be reasonably short.
The shorter, the more informative.
A long interval says that many values are plausible, giving little useful information.
But a short interval narrows down our choice of parameter.
But at the same time, the interval should come with some guarantees.
We'd like to be quite confident that the interval constructed on the basis of the sample actually contains the true value.
For instance, we'd like to have a method for constructing intervals that guarantees that 95% of all possible samples will produce an interval containing the true value.
Such an interval, if we can find it, will be called the 95% confidence interval.
Let's see how we could construct such an interval.
Let's suppose that we have a sample, say x1,...,xn, from a population model according to some model: f (x; Î¸).
We construct the estimator of Î¸ by maximum likelihood estimation.
Let's call this Î¸^(x1,...,xn).
We know that this should be close to the true parameter.
It makes sense to anchor a confidence interval around this estimator.
The interval we seek would thus be centered at
Î¸^(x1,...,xn) Â± Î´.
How exactly should we choose that constant, in order to guarantee 95% confidence?
To see how, we need to revisit the sampling distribution of Î¸^(x1,...xn).
Recall that for large enough samples, the sampling distribution of the maximum likelihood estimator is approximately normal, and this is independent of what population model we have used.
The mean of this normal distribution is given by the true parameter: Î¸.
The variance is given by an explicit formula: Ïƒ^2(Î¸)/n.
Given this, we can use our understanding of the quantiles of the normal distribution to figure our interval out.
We know that for 95% of all possible samples,
Î¸^ would fall within a distance of twice its standard deviation from its mean.
But its mean is the true parameter Î¸, and its standard deviation is known to be Ïƒ(Î¸)/âˆšn
In summary, we know that for 95% of all possible samples of size n,
Î¸^ is going to be at a distance
 of no more than 2[Ïƒ(Î¸)/âˆšn].
Turning this on its head, we can equivalently say that for 95% of all possible samples of size n, the interval with left endpoint
[Î¸^(x1,...,xn) - 2[Ïƒ(Î¸)/âˆšn]] and right endpoint
[Î¸^(x1,...,xn) + 2[Ïƒ(Î¸)/âˆšn]] will contain Î¸.
This is our confidence interval.
Notice that our formula for the confidence interval can be written in the form of Î¸^Â±Î´, which was precisely what we had set out to get.
This Î´ depends on the unknown Î¸, but since we have an estimate Î¸^ of Î¸, we can plug that in instead to get a formula that we can calculate fully, since it no longer depends on any unknowns.
This formula actually reveals interesting structural characteristics.
For instance, we see that the interval is centered at our estimate.
The length of the interval is inversely proportional to the square root of the sample size, and this is something we can control.
For example, it tells us that if we would like to get an interval that is of the same confidence level but is of half the length, we need to collect a sample that is 4 times larger.
Finally, there is this factor of 2.
This factor is related to the confidence level, which was 95% in our case.
Choosing a higher confidence level will give a higher factor value, whereas a lower confidence level will give a lower factor value.
For example, if we would like to build a confidence interval with confidence 68%, the factor actually becomes 1, whereas for a confidence interval of level 99%, the factor becomes 3.
In other words, requiring higher confidence will also increase the length of the interval.
The extreme case of a 100% confidence interval would make the length infinite, and the interval would be uninformative.
In any case, in most practical cases, one typically chooses
95% confidence intervals.
To see a practical example, let's go back to the data set on the weight of 92 high school students measured in pounds.
Let's say we'd like to build a 95% confidence interval for the true mean weight of adolescents of that age.
The population model that we assume is a normal model with mean parameter Î¼, representing the unknown mean weight at the level of population.
In this example, the maximum likelihood estimator of the true mean Î¼ is given by
Î¼^(x1,...,xn)=1/n(x1,...,xn)=x
This is just the sample mean: x.
So, the interval should be of the form: xÂ±2[Ïƒ(x)/âˆšn]
Here, we can calculate the sample mean to be 145,
Ïƒ(x) to be 23.7, and âˆšn to be 9.6.
Consequently, our 95% confidence interval for Î¼ is [140 , 150].
We can draw this on the histogram on the left.
In many cases, we're actually particularly interested in having an upper bound or a lower bound for a parameter of interest.
Of course, a confidence interval automatically yields both an upper and a lower bound.
But, what if we only care about one of them?
The idea is that by letting one of the endpoints go, it will give us a sharper bound on the other endpoint.
We can follow the exact same steps as before, using the sampling distribution of a maximum likelihood estimator.
These steps will give us a one-sided confidence interval, as shown on the right.
Notice that the corresponding upper and lower bounds have the same form in a two-sided confidence interval.
They are obtained by a small perturbation of the estimate to the right, or to the left.
The difference is the scaling factor.
In the one-sided case, this is no longer equal to 2, but has become 1.6.
This tells us that if we only care about a bound, we can typically get more sharp intervals with the same amount of confidence.
Let's now try to find an upper bound for the true mean weight in the high school example.
Using our formulas, we know that the upper 95% confidence interval will have an upper bound of x+1.6[Ïƒ(x)/âˆšn].
Using our calculations from before, we find the result to give a 95% upper bound of 149.
In comparison, the two-sided interval would have given us an upper bound of 150, which is more conservative
In summary, the two-sided interval gives us simultaneous upper and lower bounds, but at the expense that neither of these bounds is as sharp as the corresponding
95% one-sided intervals.
The one-sided interval will give tighter, one-sided bounds, but at the expense that it only gives us information on the one side, and is totally uninformative of the other side.
