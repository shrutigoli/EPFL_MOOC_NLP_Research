Hello, my name is Samuel Kerrien
I'm the manager of the Blue Brain Nexus Project which is currently being developed by the Blue Brain Project.
Today we're going to look at the challenges faced by neuroscience in data integration, we're going to look at various approaches to do data integration.
Then we're going to have a look at the Blue Brain Nexus : a data platform, and in that topic we're going to look at it's architecture at how to do the main design, at the management of ontologies and how to re-use existing and well defined domains.
Finally, we're going to look at MINDS and what it enables in the context of data integration.
So first of all, I'll come back to the issue with data integration which is that in neuroscience there's a really broad variety of data.
Not only have we got multiple scales we want to capture from sub cellular resolution, to cellular, to tissue resolution, and the whole brain, but also within each of these scales we have multiple data modalities.
And so this leads to serious challenges in integrating data as you have to cope with them all in a seamless manner.
So here we're going to discuss various challenges of integrating data in neuroscience.
So like we said, data is extremely varied.
Then we have data ranging from very small data to data set around Petabytes, which is just by the shear size of it difficult to handle.
The data, by it's nature can be distributed in silos whether it is within the same organizations, people having their own packets of data, all the way to data being distributed through different organizations, and connecting them together can be difficult.
There's an obvious need for data provenance and I'm not going to elaborate too much on these aspects right now, we're going to look at it in more detail later; but essentially you want to know where data came from if the data set was generated from another specific one that gives you a lot of useful information to know why this data set is and that gives you a lot of useful information to know why this data set is and assess it's quality. this data set is and assess it's quality.
Then you have the need to discover similar related data so obviously you bring your own data into a platform you are running your own science and it is very useful to be able to tap into what the rest of the community has actually brought into the platform in order to enrich whatever data sets you had at your disposal and improve the science you are currently running.
Finally, neuroscience has very ambitious goals and that does require a state of the art data platform to support it.
So, just to give you a few examples, the automatic reconstructions or the reconstruction of neurons, so here, on the right-hand side we can see a slice of a mouse brain that has been generated by
Wuhan University in China, and they have reconstructed by hand the morphology of neurons which you see highlighted: you have one here, you have another yellow one, you have the red one here, and the green one, so four neurons spanning the whole width of the brain, and this task is extremely time consuming, thus very expensive.
And wielding tools that will allow to automate this reconstruction is extremely valuable.
Now that's a very difficult task as well and thus it is a significant challenge.
And we have a few other examples, such as electro-physiology analysis, whole brain imaging, and finally large scale simulations validation, which is the heat of what Blue Brain does.
So next we're going to have a brief look at the various components of what makes a neuro-informatics platform, and here I'm not going to give you all the details that
Sean has already explained in previous videos, but I want ask to really focus on the Knowledge Graph and the search facilities that the knowledge graph does enable.
So in the next few slides we're really going to focus on that part.
So what is the purpose of the Knowledge Graph?
It is to represent and validate and store scientific knowledge and when I say validate, it means when you bring the information into this knowledge graph
It is able to tell you if you have indeed connected your elements in the right order, if you have provided the right amount of meta data, if mandatory information has been indeed provided.
So I'm listing two things here:
This encompasses the entities of knowledge you're trying to capture with it's meta data, but also with raw data that is attached to these entities.
You can think of raw data as your data sets.
Also there's the concern of provenance that links these entities together as they are derived from one and other.
Next, it allows you to search data based on user defined criteria and that can be either on the entity's meta data whether they are literal values or ontology terms and I'll be giving an example of that later but at the same time, it can also be based on provenance information, so a user should be able to build and formulate scientific questions, submit a query to the Knowledge Graph, that both encompasses meta data and provenance information, that gets an answer to its question.
Finally you want to be able to discover new data through the main specific commonalities, so by leveraging: meta data is shared by a number of these data sets you want to be able to actually find new things that you weren't aware of originally and again, we'll cover more on that later and give very concrete examples.
So I have been mentioning provenance in the past few slides and I think I would like to take the time now to actually tell you again what is provenance.
At a very simple level, it is a way to tell where your entities are coming from, and one way to do that for instance, is to use the concept of activity.
So activity may consume or use a number of entities, an activity would generate a number, one or many other entities.
And this activity is associated with agents and these agents can be people, you can imagine them like people doing an experiment in the lab and generating a specific data set it could be software, it has been used in the process of running this activity to generate these entities so on a very simple level this is what provenance is and it really enables you to track where things came from and how they have been deriving into other things.
Now, why provenance?
It really does enable you to do a lot of things.
Like I said earlier, it enables you to track the original data track how the data is being used and we call that derivation.
Having this information at hand allows you to asses the quality of the data and the trust in the data you might be re-using for your own use cases.
Reproducability of workflows can be ensured as if you capture all the necessary information that allows the generation of the given data sets you could just re-run the same workflow with the same parameters and reproduce these exact same data sets, quite easily.
Another use for provenance is attribution, whether it is about the data you're generating, or algorithms you're creating you can really find out who has done the work and which affiliations they are associated with.
